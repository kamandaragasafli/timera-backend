from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status, permissions
from rest_framework.parsers import MultiPartParser, FormParser
from rest_framework.decorators import api_view, permission_classes
from django.conf import settings
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
from django.utils import timezone
from django.db import transaction
import openai
import logging
import re
import base64
import io
import uuid
import requests
import json
from datetime import datetime, timedelta
from PIL import Image
import sys
from urllib.parse import urlparse, urljoin
import time

logger = logging.getLogger(__name__)

# Safe logging function for Windows console
def safe_log_error(logger_func, message, *args, **kwargs):
    """Safely log error messages that may contain Unicode characters"""
    try:
        logger_func(message, *args, **kwargs)
    except UnicodeEncodeError:
        # Fallback: encode to ASCII with replacement
        safe_message = message.encode('ascii', errors='replace').decode('ascii')
        logger_func(safe_message, *args, **kwargs)


def convert_image_to_supported_format(image_data, content_type=None, filename=None):
    """
    Convert image to a format supported by OpenAI API (PNG, JPEG, GIF, WEBP).
    AVIF and other unsupported formats are converted to PNG.
    
    Args:
        image_data: Binary image data
        content_type: MIME type of the image (optional)
        filename: Original filename (optional)
    
    Returns:
        tuple: (converted_image_data, mime_type)
    """
    try:
        # Try to detect format from content type or filename
        format_hint = None
        if content_type:
            if 'avif' in content_type.lower():
                format_hint = 'AVIF'
            elif 'png' in content_type.lower():
                format_hint = 'PNG'
            elif 'jpeg' in content_type.lower() or 'jpg' in content_type.lower():
                format_hint = 'JPEG'
            elif 'gif' in content_type.lower():
                format_hint = 'GIF'
            elif 'webp' in content_type.lower():
                format_hint = 'WEBP'
        
        if not format_hint and filename:
            ext = filename.split('.')[-1].lower()
            if ext == 'avif':
                format_hint = 'AVIF'
            elif ext in ['png', 'jpg', 'jpeg', 'gif', 'webp']:
                format_hint = ext.upper()
        
        # Open image with PIL
        image = Image.open(io.BytesIO(image_data))
        
        # Check if format is supported by OpenAI
        supported_formats = ['PNG', 'JPEG', 'GIF', 'WEBP']
        current_format = format_hint or image.format
        
        if current_format and current_format.upper() not in supported_formats:
            # Convert unsupported format to PNG
            logger.info(f"Converting unsupported format {current_format} to PNG")
            
            # Convert to RGB if necessary (for JPEG compatibility)
            if image.mode in ('RGBA', 'LA', 'P'):
                # Create white background for transparency
                background = Image.new('RGB', image.size, (255, 255, 255))
                if image.mode == 'P':
                    image = image.convert('RGBA')
                background.paste(image, mask=image.split()[-1] if image.mode in ('RGBA', 'LA') else None)
                image = background
            elif image.mode not in ('RGB', 'L'):
                image = image.convert('RGB')
            
            # Save as PNG
            output = io.BytesIO()
            image.save(output, format='PNG', optimize=True)
            converted_data = output.getvalue()
            output.close()
            
            return converted_data, 'image/png'
        
        # Format is supported, return as-is or convert to ensure compatibility
        output = io.BytesIO()
        if image.format == 'PNG':
            image.save(output, format='PNG', optimize=True)
            mime_type = 'image/png'
        elif image.format in ('JPEG', 'JPG'):
            # Ensure RGB mode for JPEG
            if image.mode != 'RGB':
                rgb_image = Image.new('RGB', image.size, (255, 255, 255))
                if image.mode == 'RGBA':
                    rgb_image.paste(image, mask=image.split()[3])
                else:
                    rgb_image.paste(image)
                image = rgb_image
            image.save(output, format='JPEG', quality=95, optimize=True)
            mime_type = 'image/jpeg'
        elif image.format == 'GIF':
            # For GIF, save as PNG (OpenAI supports GIF but PNG is more reliable)
            if image.mode != 'RGB':
                rgb_image = Image.new('RGB', image.size, (255, 255, 255))
                if image.mode == 'P':
                    image = image.convert('RGBA')
                if image.mode in ('RGBA', 'LA'):
                    rgb_image.paste(image, mask=image.split()[-1])
                else:
                    rgb_image.paste(image)
                image = rgb_image
            image.save(output, format='PNG', optimize=True)
            mime_type = 'image/png'
        elif image.format == 'WEBP':
            # Convert WEBP to PNG for better compatibility
            if image.mode != 'RGB':
                rgb_image = Image.new('RGB', image.size, (255, 255, 255))
                if image.mode in ('RGBA', 'LA'):
                    rgb_image.paste(image, mask=image.split()[-1])
                else:
                    rgb_image.paste(image)
                image = rgb_image
            image.save(output, format='PNG', optimize=True)
            mime_type = 'image/png'
        else:
            # Default to PNG
            if image.mode != 'RGB':
                rgb_image = Image.new('RGB', image.size, (255, 255, 255))
                if image.mode in ('RGBA', 'LA', 'P'):
                    if image.mode == 'P':
                        image = image.convert('RGBA')
                    rgb_image.paste(image, mask=image.split()[-1] if image.mode in ('RGBA', 'LA') else None)
                else:
                    rgb_image.paste(image)
                image = rgb_image
            image.save(output, format='PNG', optimize=True)
            mime_type = 'image/png'
        
        converted_data = output.getvalue()
        output.close()
        return converted_data, mime_type
        
    except Exception as e:
        logger.error(f"Error converting image format: {e}", exc_info=True)
        # Return original data and let API handle it
        return image_data, content_type or 'image/jpeg'

# Optional: Fal.ai service for video generation
try:
    from .fal_ai_service import FalAIService
    # Try to import FAL_CLIENT_AVAILABLE if it exists
    try:
        from .fal_ai_service import FAL_CLIENT_AVAILABLE
    except ImportError:
        FAL_CLIENT_AVAILABLE = True  # Assume available if we can import FalAIService
    FAL_AI_AVAILABLE = True
except ImportError as e:
    FAL_AI_AVAILABLE = False
    FalAIService = None
    FAL_CLIENT_AVAILABLE = False
    safe_log_error(logger.warning, f"  FalAIService import edil…ô bilm…ôdi: {e}")

# Optional: BeautifulSoup for web scraping
try:
    from bs4 import BeautifulSoup
    BS4_AVAILABLE = True
except ImportError:
    BS4_AVAILABLE = False
    BeautifulSoup = None


def detect_language(text):
    """Detect if text is in Azerbaijani or English"""
    if not text:
        return 'az'  # Default to Azerbaijani
    
    # Azerbaijani specific characters
    az_chars = ['…ô', 'ƒ±', '√∂', '√º', 'ƒü', '≈ü', '√ß', '∆è', 'ƒ∞', '√ñ', '√ú', 'ƒû', '≈û', '√á']
    
    # Common Azerbaijani words
    az_words = ['v…ô', '√º√ß√ºn', 'il…ô', 'olan', 'm…ôs', '≈üirk…ôt', 'biznes', '≈üirk…ôtiniz']
    
    # Check for Azerbaijani characters
    has_az_chars = any(char in text for char in az_chars)
    
    # Check for Azerbaijani words
    has_az_words = any(word in text.lower() for word in az_words)
    
    if has_az_chars or has_az_words:
        return 'az'
    
    return 'en'


class GenerateContentView(APIView):
    """Generate AI content using OpenAI for various purposes"""
    
    permission_classes = [permissions.IsAuthenticated]
    
    def post(self, request):
        try:
            prompt = request.data.get('prompt')
            content_type = request.data.get('content_type', 'general')
            
            if not prompt:
                return Response({
                    'error': 'Prompt is required'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # Detect language from the prompt
            language = detect_language(prompt)
            
            # Initialize OpenAI client
            client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
            
            logger.info(f"ü§ñ Generating AI content for user: {request.user.email}, type: {content_type}, language: {language}")
            
            # Set system message based on language and content type
            if language == 'az':
                system_message = """Siz pe≈ü…ôkar biznes m…ôsl…ôh…ôt√ßisi v…ô marketinq ekspertisiniz. 
Az…ôrbaycan dilind…ô d…ôqiq, professional v…ô faydalƒ± cavablar verirsiniz.

Qaydalar:
1. Yalnƒ±z t…ôl…ôb olunan m…ôzmunu verin, …ôlav…ô izahat v…ô ya giri≈ü s√∂zl…ôri yazmayƒ±n
2. T…ôbii v…ô professional ton istifad…ô edin
3. Konkret v…ô d…ôqiq olun
4. Cavabƒ± t…ômiz formatda verin (lazƒ±m olduqda verg√ºll…ô ayrƒ±lmƒ±≈ü)

Xarakter limitl…ôri:
- Qƒ±sa m…ôtnl…ôr (keywords, topics): 100-200 simvol
- Orta m…ôtnl…ôr (descriptions): 200-500 simvol
- Uzun m…ôtnl…ôr (detailed descriptions): 300-800 simvol"""
            else:
                system_message = """You are a professional business consultant and marketing expert.
You provide accurate, professional, and helpful responses in English.

Rules:
1. Provide only the requested content, no extra explanations or introductions
2. Use natural and professional tone
3. Be specific and accurate
4. Return clean formatted content (comma-separated when needed)

Character limits:
- Short texts (keywords, topics): 100-200 characters
- Medium texts (descriptions): 200-500 characters
- Long texts (detailed descriptions): 300-800 characters"""
            
            # Call OpenAI ChatGPT
            response = client.chat.completions.create(
                model="gpt-4o-mini",  # Fast and cost-effective
                messages=[
                    {
                        "role": "system",
                        "content": system_message
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.7,  # Balanced creativity
                max_tokens=600,  # Enough for detailed responses
                presence_penalty=0.1,  # Slight penalty for repetition
                frequency_penalty=0.1  # Encourage variety
            )
            
            generated_content = response.choices[0].message.content.strip()
            
            # Remove quotes if AI added them
            generated_content = generated_content.strip('"\'')
            
            logger.info(f" Successfully generated content for user: {request.user.email} ({len(generated_content)} chars)")
            
            return Response({
                'content': generated_content,
                'generated_content': generated_content,
                'status': 'success',
                'language': language,
                'char_count': len(generated_content)
            }, status=status.HTTP_200_OK)
            
        except openai.APIError as e:
            logger.error(f" OpenAI API error: {str(e)}")
            return Response({
                'error': 'OpenAI API error occurred'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        except Exception as e:
            logger.error(f" Error generating content: {str(e)}", exc_info=True)
            return Response({
                'error': f'Failed to generate content: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class OptimizeForPlatformView(APIView):
    """Optimize content for specific social media platforms"""
    
    permission_classes = [permissions.IsAuthenticated]
    
    def post(self, request):
        try:
            content = request.data.get('content')
            platform = request.data.get('platform', 'general')
            
            if not content:
                return Response({
                    'error': 'Content is required'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # Detect language
            language = detect_language(content)
            
            # Initialize OpenAI client
            client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
            
            if language == 'az':
                system_message = "Siz sosial media ekspertisiniz. M…ôzmunu platformalara uyƒüunla≈üdƒ±rƒ±rsƒ±nƒ±z."
                prompt = f"""A≈üaƒüƒ±dakƒ± m…ôzmunu {platform} platformasƒ± √º√ß√ºn optimalla≈üdƒ±rƒ±n.
Platform x√ºsusiyy…ôtl…ôrin…ô uyƒüun formatda yazƒ±n.

M…ôzmun: {content}

Cavab yalnƒ±z optimalla≈üdƒ±rƒ±lmƒ±≈ü m…ôzmun olmalƒ±dƒ±r."""
            else:
                system_message = "You are a social media expert. You optimize content for different platforms."
                prompt = f"""Optimize the following content for {platform} platform.
Format it according to the platform's best practices.

Content: {content}

Provide only the optimized content."""
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": system_message
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.7,
                max_tokens=400
            )
            
            optimized_content = response.choices[0].message.content.strip()
            optimized_content = optimized_content.strip('"\'')
            
            return Response({
                'content': optimized_content,
                'status': 'success',
                'language': language
            }, status=status.HTTP_200_OK)
            
        except Exception as e:
            logger.error(f" Error optimizing content: {str(e)}", exc_info=True)
            return Response({
                'error': f'Failed to optimize content: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class AnalyzeLogoView(APIView):
    """Analyze company logo with AI to extract brand information"""
    
    permission_classes = [permissions.IsAuthenticated]
    parser_classes = [MultiPartParser, FormParser]
    
    def post(self, request):
        try:
            logo_file = request.FILES.get('logo')
            
            if not logo_file:
                return Response({
                    'error': 'Logo file is required'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            logger.info(f" Analyzing logo for user: {request.user.email}")
            
            # Get company context if available
            company_name = request.data.get('company_name', '')
            industry = request.data.get('industry', '')
            business_desc = request.data.get('business_description', '')
            
            # Build context string
            context_info = ""
            if company_name or industry or business_desc:
                context_info = "\n\nCOMPANY CONTEXT:\n"
                if company_name:
                    context_info += f"Company Name: {company_name}\n"
                if industry:
                    context_info += f"Industry: {industry}\n"
                if business_desc:
                    context_info += f"Business: {business_desc}\n"
                context_info += "\nUse this context to provide more accurate brand analysis.\n"
            
            # Read and convert image if needed
            image_data = logo_file.read()
            content_type = logo_file.content_type
            filename = logo_file.name
            
            # Convert to supported format (AVIF -> PNG/JPEG)
            image_data, mime_type = convert_image_to_supported_format(
                image_data, 
                content_type=content_type, 
                filename=filename
            )
            
            # Determine format for data URL
            if 'png' in mime_type.lower():
                data_url_format = 'image/png'
            elif 'jpeg' in mime_type.lower() or 'jpg' in mime_type.lower():
                data_url_format = 'image/jpeg'
            else:
                data_url_format = 'image/png'  # Default to PNG
            
            base64_image = base64.b64encode(image_data).decode('utf-8')
            
            # Initialize OpenAI client
            client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
            
            # Create detailed prompt for logo analysis
            prompt = f"""You are a brand identity expert. Analyze this image (company logo, brand image, or any design) and extract brand information.
{context_info}

Even if the image is simple, abstract, or not a traditional logo, provide your best analysis based on what you see.

Return ONLY a valid JSON object with this structure (no explanations, no markdown, no code blocks):

{{
  "primary_color": "#HEXCODE of dominant color",
  "secondary_colors": ["#HEX1", "#HEX2"],
  "color_palette": ["#HEX1", "#HEX2", "#HEX3", "#HEX4"],
  "design_style": "describe the visual style (e.g., modern, minimalist, bold, elegant, playful)",
  "logo_type": "wordmark, symbol, combination, emblem, lettermark, or abstract",
  "brand_personality": ["3-5 personality traits like professional, friendly, innovative"],
  "emotional_tone": "the emotional feeling (e.g., confident, energetic, calm, playful)",
  "industry_vibe": "what industry this suggests (tech, creative, corporate, etc.)",
  "font_style": "if text visible, describe it; otherwise say 'No text visible'",
  "typography_suggestions": {{
    "headings": "recommended font style for headings",
    "body": "recommended font style for body text"
  }},
  "complementary_colors": ["#HEX1", "#HEX2", "#HEX3"],
  "brand_keywords": ["5-7 descriptive keywords about the visual identity"]
}}

IMPORTANT: 
- Analyze ANY image provided, even if it's not a perfect logo
- Always return valid JSON
- Use your best judgment to extract colors and suggest hex codes
- Be creative but professional in your analysis"""
            
            # Call OpenAI Vision API
            # Use gpt-4o which supports vision
            logger.info(f"Calling OpenAI Vision API with image size: {len(image_data)} bytes")
            
            response = client.chat.completions.create(
                model="gpt-4o",  # Supports vision
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:{data_url_format};base64,{base64_image}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=1500,
                temperature=0.5
            )
            
            logger.info(f"Received response from OpenAI")
            
            # Check if response is valid
            if not response.choices or len(response.choices) == 0:
                raise ValueError("No response from OpenAI")
            
            # Parse the response
            analysis_text = response.choices[0].message.content
            
            if not analysis_text:
                raise ValueError("Empty response from OpenAI")
            
            analysis_text = analysis_text.strip()
            
            logger.info(f"Raw AI response length: {len(analysis_text)} chars")
            logger.info(f"Raw AI response preview: {analysis_text[:200]}...")
            
            # Remove markdown code blocks if present
            if '```' in analysis_text:
                # Extract content between ``` markers
                parts = analysis_text.split('```')
                if len(parts) >= 3:
                    analysis_text = parts[1]
                    # Remove 'json' or 'JSON' prefix if present
                    if analysis_text.lower().startswith('json'):
                        analysis_text = analysis_text[4:]
                elif len(parts) == 2:
                    analysis_text = parts[1]
            
            analysis_text = analysis_text.strip()
            
            # Parse JSON
            try:
                brand_analysis = json.loads(analysis_text)
            except json.JSONDecodeError:
                # If JSON parsing fails, try to extract JSON object manually
                import re
                json_match = re.search(r'\{.*\}', analysis_text, re.DOTALL)
                if json_match:
                    analysis_text = json_match.group(0)
                    brand_analysis = json.loads(analysis_text)
                else:
                    # AI refused to analyze - create a default response
                    logger.warning(f"AI refused to analyze, creating fallback response")
                    brand_analysis = {
                        "primary_color": "#3B82F6",
                        "secondary_colors": ["#1E40AF", "#60A5FA"],
                        "color_palette": ["#3B82F6", "#1E40AF", "#60A5FA", "#DBEAFE"],
                        "design_style": "Could not analyze - please try a different image",
                        "logo_type": "Unknown",
                        "brand_personality": ["Professional", "Modern", "Trustworthy"],
                        "emotional_tone": "Professional and approachable",
                        "industry_vibe": "General business",
                        "font_style": "No text visible or unable to analyze",
                        "typography_suggestions": {
                            "headings": "Modern sans-serif like Inter or Roboto",
                            "body": "Clean sans-serif like Open Sans"
                        },
                        "complementary_colors": ["#10B981", "#F59E0B", "#8B5CF6"],
                        "brand_keywords": ["Professional", "Modern", "Business", "Clean", "Digital"],
                        "note": "This is a default analysis. Please try uploading a clearer logo image for better results."
                    }
            
            logger.info(f" Successfully analyzed logo for user: {request.user.email}")
            
            return Response({
                'brand_analysis': brand_analysis,
                'status': 'success'
            }, status=status.HTTP_200_OK)
            
        except json.JSONDecodeError as e:
            logger.error(f" JSON parsing error: {str(e)}")
            logger.error(f"Response text: {analysis_text if 'analysis_text' in locals() else 'No response'}")
            return Response({
                'error': 'Failed to parse AI response',
                'details': str(e),
                'raw_response': analysis_text[:500] if 'analysis_text' in locals() else 'No response'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        except ValueError as e:
            logger.error(f" Validation error: {str(e)}")
            return Response({
                'error': str(e)
            }, status=status.HTTP_400_BAD_REQUEST)
        except openai.APIError as e:
            logger.error(f" OpenAI API error: {str(e)}")
            return Response({
                'error': f'OpenAI API error: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        except Exception as e:
            logger.error(f" Error analyzing logo: {str(e)}", exc_info=True)
            return Response({
                'error': f'Failed to analyze logo: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


# ============================================================================
# WASK.CO AI LOGO & SLOGAN GENERATOR
# ============================================================================

def scrape_website_info(url):
    """Web saytdan m…ôlumat √ßƒ±xarƒ±r"""
    if not BS4_AVAILABLE:
        raise ValueError("Web scraping …ôl√ßatan deyil. beautifulsoup4 paketi install edilm…ôyib.")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Extract information
        title = soup.find('title')
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        og_title = soup.find('meta', attrs={'property': 'og:title'})
        og_desc = soup.find('meta', attrs={'property': 'og:description'})
        
        # Extract main content
        h1 = soup.find('h1')
        paragraphs = soup.find_all('p')[:3]
        
        # Priority: OG tags > Meta tags > HTML tags
        product_name = ''
        if og_title:
            product_name = og_title.get('content', '')
        elif title:
            product_name = title.text.strip()
        elif h1:
            product_name = h1.text.strip()
        
        product_description = ''
        if og_desc:
            product_description = og_desc.get('content', '')
        elif meta_desc:
            product_description = meta_desc.get('content', '')
        else:
            product_description = ' '.join([p.text.strip() for p in paragraphs if p.text.strip()])[:500]
        
        if not product_name or not product_description:
            raise ValueError("Saytdan kifay…ôt q…ôd…ôr m…ôlumat √ßƒ±xarƒ±la bilm…ôdi")
        
        return {
            'product_name': product_name[:255],  # Limit length
            'product_description': product_description,
            'url': url
        }
    except requests.exceptions.RequestException as e:
        raise ValueError(f"Sayta m√ºraci…ôt edil…ô bilm…ôdi: {str(e)}")
    except Exception as e:
        raise ValueError(f"Saytdan m…ôlumat √ßƒ±xarƒ±la bilm…ôdi: {str(e)}")


def upload_product_image(image_file, user_id):
    """M…ôhsul ≈ü…ôklini y√ºkl…ôyir"""
    ext = image_file.name.split('.')[-1].lower()
    filename = f"logos/user_{user_id}_{uuid.uuid4()}.{ext}"
    path = default_storage.save(filename, ContentFile(image_file.read()))
    return default_storage.url(path)


def call_wask_api(product_name, product_description, style='minimalist', color='#3B82F6', tags=None, image_url=None):
    """
    AI il…ô logo v…ô slogan yaradƒ±r (t…ôsvir …ôsasƒ±nda)
    """
    if tags is None:
        tags = []
    try:
        import openai
        from openai import OpenAI
        
        client = OpenAI(api_key=settings.OPENAI_API_KEY)
        
        if not settings.OPENAI_API_KEY:
            raise ValueError("API key konfiqurasiya edilm…ôyib")
        
        # Generate slogan (Az…ôrbaycan dilind…ô)
        logger.info("üìù Slogan yaradƒ±lƒ±r (Az…ôrbaycan dilind…ô)...")
        slogan_prompt = f"""Bu ≈üirk…ôt/m…ôhsul √º√ß√ºn g√ºcl√º, yadda qalan slogan yarat:

Ad: {product_name}
T…ôsvir: {product_description}

T…ôl…ôbl…ôr:
- Qƒ±sa v…ô yadda qalan (3-7 s√∂z)
- Professional v…ô ilhamverici
- Az…ôrbaycan dilind…ô
- Brendin mahiyy…ôtini …ôks etdir…ôn

YALNIZ sloganƒ± qaytar, ba≈üqa he√ß n…ô yazma."""

        slogan_response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": slogan_prompt}],
            max_tokens=50,
            temperature=0.9
        )
        
        slogan_raw = slogan_response.choices[0].message.content.strip()
        # Remove quotes if present
        slogan = slogan_raw.strip('"').strip("'").strip()
        
        # Validate slogan
        if not slogan or len(slogan) < 3:
            logger.warning(f" Slogan √ßox qƒ±sadƒ±r: '{slogan}'. Default slogan istifad…ô olunur.")
            slogan = f"Empower Your Business"  # Fallback slogan
        
        logger.info(f" Slogan yaradƒ±ldƒ±: '{slogan}'")
        
        # Generate logo with style and color options
        logger.info(f" Logo yaradƒ±lƒ±r ({style} stilind…ô, {color} r…ôngind…ô)...")
        
        # Style descriptions
        style_descriptions = {
            'minimalist': 'Extremely simple, minimal details, clean lines',
            'elegant': 'Refined, sophisticated, graceful design',
            'modern': 'Contemporary, dynamic, forward-thinking',
            'professional': 'Business-oriented, trustworthy, corporate',
            'playful': 'Fun, vibrant, colorful, energetic'
        }
        
        style_desc = style_descriptions.get(style, 'simple and clean')
        
        # Category-specific icon suggestions (Az…ôrbaycan dilind…ô)
        category_icons = {
            'T…ôhsil': 'book, pen, graduation cap, school, education symbol, learning icon, academic symbol',
            'Tech': 'computer, code brackets, circuit, technology symbol, digital icon, microchip, tech gear',
            'Finans': 'money symbol, dollar sign, coin, financial graph, banking icon, currency, finance symbol',
            'Saƒülamlƒ±q': 'medical cross, heart, health symbol, medicine icon, wellness symbol, healthcare',
            'E-commerce': 'shopping cart, bag, package, delivery icon, online shopping symbol, ecommerce',
            'Xidm…ôt': 'customer service, handshake, support icon, service symbol, help icon, service badge',
            'ƒ∞stehsal': 'factory, gear, manufacturing symbol, production icon, industry symbol, factory icon',
            'Da≈üƒ±nmaz ∆èmlak': 'house, building, key, real estate symbol, property icon, home symbol',
            'Marketing': 'megaphone, target, marketing symbol, advertising icon, promotion, marketing badge',
            'Dizayn': 'paint brush, palette, design tools, creative symbol, art icon, design symbol',
            'M…ôtb…ôx': 'chef hat, cooking pot, fork and knife, food icon, restaurant symbol, kitchen icon',
            'Moda': 'hanger, fashion symbol, clothing icon, style symbol, accessory, fashion badge',
            'ƒ∞dman': 'ball, trophy, fitness symbol, sport icon, athletic symbol, sports icon',
            'S…ôyah…ôt': 'airplane, map, compass, travel symbol, journey icon, suitcase, travel icon',
            'ƒ∞nc…ôs…ôn…ôt': 'paint brush, palette, art symbol, creative icon, gallery symbol, art badge'
        }
        
        # Map English category names to Azerbaijani (if needed)
        category_mapping = {
            'Tech': 'Tech',
            'Finans': 'Finans',
            'Saƒülamlƒ±q': 'Saƒülamlƒ±q',
            'T…ôhsil': 'T…ôhsil',
            'E-commerce': 'E-commerce',
            'Xidm…ôt': 'Xidm…ôt',
            'ƒ∞stehsal': 'ƒ∞stehsal',
            'Da≈üƒ±nmaz ∆èmlak': 'Da≈üƒ±nmaz ∆èmlak',
            'Marketing': 'Marketing',
            'Dizayn': 'Dizayn',
            'M…ôtb…ôx': 'M…ôtb…ôx',
            'Moda': 'Moda',
            'ƒ∞dman': 'ƒ∞dman',
            'S…ôyah…ôt': 'S…ôyah…ôt',
            'ƒ∞nc…ôs…ôn…ôt': 'ƒ∞nc…ôs…ôn…ôt'
        }
        
        # Build category-specific icon suggestions
        icon_suggestions = []
        for tag in tags:
            if tag in category_icons:
                icons = category_icons[tag]
                icon_suggestions.append(f"- {tag} category: Can incorporate elements like {icons}")
        
        tags_context = ''
        if tags:
            tags_context = f"\n\nCategories/Industries: {', '.join(tags)}"
            if icon_suggestions:
                tags_context += f"\n\nCategory Context (use as inspiration, not literal):\n"
                tags_context += "\n".join(icon_suggestions)
                tags_context += f"\n\nImportant: Create a simple, abstract icon that represents the essence of these categories. For example, if 'T…ôhsil' (Education) is selected, create a simple educational icon/symbol inspired by education concept - NOT literally showing books or pens as the logo shape, but a cohesive simple symbol that represents education. The logo should be relevant to the categories but remain a unified, simple icon design."
        
        # Convert hex color to descriptive name for better AI understanding
        color_names = {
            '#3B82F6': 'bright blue',
            '#8B5CF6': 'vibrant purple',
            '#EF4444': 'vivid red',
            '#10B981': 'fresh green',
            '#F59E0B': 'warm orange',
            '#6366F1': 'deep indigo',
            '#000000': 'solid black',
            '#FFFFFF': 'pure white'
        }
        color_name = color_names.get(color, 'bright blue')
        
        logo_prompt = f"""Create a {style} logo icon for {product_name} in {color_name} color. 

Company/Brand: {product_name}
Business Description: {product_description}
{tags_context}

CRITICAL DESIGN REQUIREMENTS:
1. The logo MUST visually represent the business based on the description: "{product_description}"
2. If categories are selected, the logo design should be inspired by and relevant to those categories
3. The logo should be a simple, unified icon that represents the business - NOT multiple separate objects or literal representations
4. For example, if "T…ôhsil" (Education) category is selected, create a simple educational icon/symbol - NOT a literal book shape, but a simple symbol that represents education/learning
5. The design should capture the essence and meaning of the categories, creating a meaningful symbol, not showing literal objects
6. Create a cohesive, simple icon that represents the business and categories conceptually

CRITICAL COLOR REQUIREMENTS:
- The entire logo icon MUST be {color_name} colored (hex code: {color})
- Use {color} as the primary and dominant color for all logo elements
- The logo should be in {color_name} color, NOT multicolor
- Background MUST be completely transparent (PNG format)
- Only the logo icon itself should be {color_name}, everything else transparent

Style: {style_desc}

Requirements:
- {style} design style
- Logo icon in {color_name} color ({color})
- Single cohesive icon design, no text
- Simple, clean design that clearly represents the business
- MUST be relevant to both the description and selected categories
- Include appropriate icons/elements from the categories if selected
- Completely transparent background (PNG format)
- Professional quality
- The logo icon itself should be {color_name} colored, background completely transparent"""

        logo_response = client.images.generate(
            model="dall-e-3",
            prompt=logo_prompt,
            size="1024x1024",
            quality="standard",
            n=1
        )
        
        logo_url = logo_response.data[0].url
        logger.info(f" Logo yaradƒ±ldƒ±")
        
        return {
            'logo_url': logo_url,
            'slogan': slogan,
            'request_id': f'openai_{logo_response.created}'
        }
        
    except openai.APITimeoutError as e:
        logger.error(f"‚ùå OpenAI API timeout error: {str(e)}")
        raise ValueError(f"OpenAI API cavab verm…ôdi. Z…ôhm…ôt olmasa yenid…ôn c…ôhd edin: {str(e)}")
    except openai.APIError as e:
        logger.error(f"‚ùå OpenAI API error: {str(e)}")
        raise ValueError(f"OpenAI API x…ôtasƒ±: {str(e)}. Z…ôhm…ôt olmasa API key-in d√ºzg√ºn olduƒüunu yoxlayƒ±n.")
    except Exception as e:
        logger.error(f"‚ùå Logo v…ô slogan yaratma x…ôtasƒ±: {str(e)}", exc_info=True)
        raise Exception(f"Logo v…ô slogan yaradƒ±la bilm…ôdi: {str(e)}")


def download_and_save_logo(logo_url, user_id):
    """Logo y√ºkl…ôyib transparent background il…ô saxlayƒ±r"""
    try:
        logger.info(f"üì• Logo y√ºkl…ônir: {logo_url}")
        response = requests.get(logo_url, timeout=60)  # Increased timeout for server
        response.raise_for_status()
        
        # Load image with PIL
        from PIL import Image
        import io
        
        logo_image = Image.open(io.BytesIO(response.content))
        logger.info(f"   Original image size: {logo_image.size}, mode: {logo_image.mode}")
        
        # Convert to RGBA if not already
        if logo_image.mode != 'RGBA':
            logo_image = logo_image.convert('RGBA')
        
        # Remove white/light background and make transparent
        logger.info("üîÑ Arxa fon transparent edilir...")
        
        # Get image dimensions
        width, height = logo_image.size
        
        # Optimize: Use NumPy for faster processing if available
        # Note: NumPy is optional - code will work without it using fallback method
        try:
            import numpy as np  # noqa: F401, pylint: disable=import-error
            # Convert to NumPy array for faster processing
            img_array = np.array(logo_image)
            
            # Create mask for white/light pixels (threshold: RGB > 240)
            # For RGBA images, check RGB channels only
            white_mask = (img_array[:, :, 0] > 240) & (img_array[:, :, 1] > 240) & (img_array[:, :, 2] > 240)
            
            # Set alpha channel to 0 for white/light pixels
            img_array[:, :, 3] = np.where(white_mask, 0, 255)
            
            # Convert back to PIL Image
            logo_image = Image.fromarray(img_array, 'RGBA')
            logger.info("   Background removal completed using NumPy (fast method)")
        except ImportError:
            # NumPy not available - use simpler method for small images
            logger.info("   NumPy not available, using simplified background removal")
            # Most DALL-E generated logos already have transparent backgrounds
            # Just keep the image as-is for now
            logger.info("   Keeping original image (most AI-generated logos already have transparent backgrounds)")
        except Exception as np_error:
            # NumPy error - fallback to keeping original
            logger.warning(f"   NumPy processing error: {str(np_error)}, keeping original image")
        
        # Save transparent logo to memory
        output = io.BytesIO()
        logo_image.save(output, format='PNG', optimize=True)
        output.seek(0)
        
        # Save to storage
        filename = f"generated_logos/user_{user_id}_{uuid.uuid4()}.png"
        logger.info(f"   Saving logo to: {filename}")
        
        try:
            path = default_storage.save(filename, ContentFile(output.read()))
            saved_url = default_storage.url(path)
            logger.info(f"‚úÖ Transparent logo saxlandƒ±: {saved_url}")
            return saved_url
        except Exception as save_error:
            logger.error(f"‚ùå File save error: {str(save_error)}")
            logger.error(f"   Storage backend: {type(default_storage).__name__}")
            logger.error(f"   MEDIA_ROOT: {getattr(settings, 'MEDIA_ROOT', 'Not set')}")
            raise Exception(f"Logo fayl sistemin…ô yazƒ±la bilm…ôdi: {str(save_error)}")
            
    except requests.exceptions.Timeout:
        logger.error(f"‚ùå Logo y√ºkl…ôm…ô timeout oldu (60 saniy…ô)")
        raise Exception("Logo y√ºkl…ôm…ô √ßox uzun √ß…ôkdi. Z…ôhm…ôt olmasa yenid…ôn c…ôhd edin.")
    except requests.exceptions.RequestException as e:
        logger.error(f"‚ùå Network error logo y√ºkl…ôy…ôrk…ôn: {str(e)}")
        raise Exception(f"Logo y√ºkl…ôn…ô bilm…ôdi (≈ü…ôb…ôk…ô x…ôtasƒ±): {str(e)}")
    except Exception as e:
        logger.error(f"‚ùå Logo y√ºkl…ônm…ôdi: {str(e)}", exc_info=True)
        raise Exception(f"Logo y√ºkl…ôn…ô bilm…ôdi: {str(e)}")


@api_view(['POST'])
@permission_classes([permissions.IsAuthenticated])
def generate_logo_slogan(request):
    """
    Wask.co AI istifad…ô ed…ôr…ôk logo v…ô slogan yaradƒ±r
    
    Modes:
    - Manual: product_name + product_description (+ optional image)
    - Link: product_link (auto-scrapes website)
    """
    user = request.user
    
    # Get inputs
    product_name = request.data.get('product_name') or request.POST.get('product_name')
    product_description = request.data.get('product_description') or request.POST.get('product_description')
    style = request.data.get('style') or request.POST.get('style', 'minimalist')
    color = request.data.get('color') or request.POST.get('color', '#3B82F6')
    tags = request.data.get('tags') or request.POST.getlist('tags') or []
    
    # Validation
    if not product_name or not product_description:
        return Response({
            "error": "product_name v…ô product_description t…ôl…ôb olunur"
        }, status=status.HTTP_400_BAD_REQUEST)
    
    try:
        logger.info(f" Logo v…ô slogan yaratma ba≈üladƒ± - User: {user.email}")
        logger.info(f"   ≈ûirk…ôt: {product_name}")
        logger.info(f"   Stil: {style}")
        logger.info(f"   R…ông: {color}")
        logger.info(f"   Tags: {tags}")
        
        # Logo v…ô slogan yaradƒ±lƒ±r
        logger.info(f" Logo v…ô slogan yaradƒ±lƒ±r...")
        wask_response = call_wask_api(
            product_name=product_name,
            product_description=product_description,
            style=style,
            color=color,
            tags=tags,
            image_url=None
        )
        
        # Generated logo
        wask_logo_url = wask_response.get('logo_url')
        if not wask_logo_url:
            raise ValueError("Logo yaradƒ±la bilm…ôdi")
        
        logger.info(f" Logo y√ºkl…ônir...")
        saved_logo_url = download_and_save_logo(wask_logo_url, user.id)
        
        # Convert to absolute URL
        if not saved_logo_url.startswith('http'):
            saved_logo_url = request.build_absolute_uri(saved_logo_url)
        
        logger.info(f" Logo uƒüurla yaradƒ±ldƒ±")
        logger.info(f"   Logo URL: {saved_logo_url}")
        logger.info(f"   Slogan: {wask_response.get('slogan', 'YOX')}")
        logger.info(f"   Product Name: {product_name}")
        
        # Return response
        response_data = {
            "logo_url": saved_logo_url,
            "slogan": wask_response.get('slogan', ''),
            "metadata": {
                "product_name": product_name,
                "generated_at": datetime.now().isoformat(),
                "request_id": wask_response.get('request_id', '')
            }
        }
        
        logger.info(f"üì§ Response g√∂nd…ôrilir: slogan={bool(response_data['slogan'])}, logo_url={bool(response_data['logo_url'])}")
        
        return Response(response_data, status=status.HTTP_200_OK)
        
    except ValueError as e:
        logger.error(f"‚ùå Validation error: {str(e)}")
        return Response(
            {"error": str(e)},
            status=status.HTTP_400_BAD_REQUEST
        )
    except openai.APITimeoutError as e:
        logger.error(f"‚ùå OpenAI API timeout: {str(e)}")
        return Response({
            "error": "OpenAI API cavab verm…ôdi. Z…ôhm…ôt olmasa bir az sonra yenid…ôn c…ôhd edin.",
            "details": str(e)
        }, status=status.HTTP_504_GATEWAY_TIMEOUT)
    except openai.APIError as e:
        logger.error(f"‚ùå OpenAI API error: {str(e)}")
        return Response({
            "error": "OpenAI API x…ôtasƒ± ba≈ü verdi. Z…ôhm…ôt olmasa API key-in d√ºzg√ºn olduƒüunu yoxlayƒ±n.",
            "details": str(e)
        }, status=status.HTTP_502_BAD_GATEWAY)
    except requests.exceptions.Timeout as e:
        logger.error(f"‚ùå Request timeout: {str(e)}")
        return Response({
            "error": "Logo y√ºkl…ôm…ô √ßox uzun √ß…ôkdi. Z…ôhm…ôt olmasa yenid…ôn c…ôhd edin.",
            "details": str(e)
        }, status=status.HTTP_504_GATEWAY_TIMEOUT)
    except requests.exceptions.RequestException as e:
        logger.error(f"‚ùå Network error: {str(e)}")
        return Response({
            "error": "≈û…ôb…ôk…ô x…ôtasƒ± ba≈ü verdi. Z…ôhm…ôt olmasa internet …ôlaq…ônizi yoxlayƒ±n.",
            "details": str(e)
        }, status=status.HTTP_502_BAD_GATEWAY)
    except Exception as e:
        logger.error(f"‚ùå Unexpected error: {str(e)}", exc_info=True)
        import traceback
        error_trace = traceback.format_exc()
        logger.error(f"   Traceback: {error_trace}")
        return Response({
            "error": "Logo v…ô slogan yaratma zamanƒ± g√∂zl…ônilm…ôz x…ôta ba≈ü verdi",
            "details": str(e)
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


# ============================================================================
# AI AD CREATIVE GENERATOR
# ============================================================================

@api_view(['POST'])
@permission_classes([permissions.IsAuthenticated])
def create_ad_creative(request):
    """
    AI il…ô professional reklam ≈ü…ôkli yaradƒ±r (Fal.ai Flux + Web Scraping)
    ≈ûirk…ôtin m√∂vcud logo v…ô slogan-ƒ±nƒ± istifad…ô edir (yeni yaratmƒ±r!)
    
    Input:
    - product_name: M…ôhsul adƒ±
    - product_description: M…ôhsul t…ôsviri
    - product_link: M…ôhsul link-i (scraping √º√ß√ºn)
    - product_image: M…ôhsul ≈ü…ôkli (optional)
    - ad_format: social_square | story | landscape | portrait
    - style: modern | professional | playful | elegant
    - target_audience: H…ôd…ôf auditoriya (optional)
    - apply_branding: true/false (company logo+slogan …ôlav…ô edilsin?)
    
    Output:
    - ad_image_url: Reklam ≈ü…ôkli (company logo+slogan il…ô)
    - ad_copy: Reklam m…ôtni
    - headline: Ba≈ülƒ±q
    - hashtags: Hashtag-lar
    """
    try:
        # Get inputs - support both request.data and request.POST for FormData
        product_name = request.data.get('product_name') or request.POST.get('product_name')
        product_description = request.data.get('product_description') or request.POST.get('product_description')
        product_link = request.data.get('product_link') or request.POST.get('product_link')
        product_image = request.FILES.get('product_image')
        ad_format = request.data.get('ad_format') or request.POST.get('ad_format', 'social_square')
        style = request.data.get('style') or request.POST.get('style', 'modern')
        target_audience = request.data.get('target_audience') or request.POST.get('target_audience')
        
        # Debug logging
        logger.info(f"üì• Received request data:")
        logger.info(f"   product_name: {product_name}")
        logger.info(f"   product_description: {product_description}")
        logger.info(f"   product_link: {product_link}")
        logger.info(f"   ad_format: {ad_format}")
        logger.info(f"   style: {style}")
        logger.info(f"   has_product_image: {bool(product_image)}")
        
        # Validation
        if not product_link and (not product_name or not product_description):
            logger.error(" Validation failed: Missing required fields")
            return Response({
                "error": "product_name v…ô product_description v…ô ya product_link t…ôl…ôb olunur",
                "received": {
                    "product_name": product_name,
                    "product_description": product_description,
                    "product_link": product_link
                }
            }, status=status.HTTP_400_BAD_REQUEST)
        
        # Validate ad format
        from .ad_creative import AdCreativeGenerator
        if ad_format not in AdCreativeGenerator.FORMATS:
            return Response({
                "error": f"Ad format d√ºzg√ºn deyil. M√∂vcud formatlar: {', '.join(AdCreativeGenerator.FORMATS.keys())}"
            }, status=status.HTTP_400_BAD_REQUEST)
        
        logger.info(f" Ad creative generation started")
        logger.info(f"   Product: {product_name or 'from link'}")
        logger.info(f"   Format: {ad_format}")
        logger.info(f"   Style: {style}")
        
        # Link mode: Scrape website
        if product_link:
            logger.info(f"   Scraping: {product_link}")
            if not product_link.startswith('http'):
                product_link = 'https://' + product_link
            
            scraped = scrape_website_info(product_link)
            product_name = scraped['product_name']
            product_description = scraped['product_description']
        
        # Upload product image if provided
        product_image_url = None
        if product_image:
            logger.info(f"   Uploading product image...")
            product_image_url = upload_product_image(product_image, request.user.id)
            if not product_image_url.startswith('http'):
                product_image_url = request.build_absolute_uri(product_image_url)
        
        # Generate ad creative with Fal.ai
        generator = AdCreativeGenerator(user=request.user)
        try:
            result = generator.generate_ad_creative(
                product_name=product_name,
                product_description=product_description,
                ad_format=ad_format,
                style=style,
                target_audience=target_audience,
                product_image_url=product_image_url,
                product_url=product_link  # Pass the product URL for scraping
            )
        except ValueError as ve:
            # Handle scraping failures
            logger.error(f" ValueError: {str(ve)}")
            return Response({
                "error": str(ve),
                "suggestion": "Link i≈ül…ôm…ôdi. Z…ôhm…ôt olmasa Manuel rejim…ô ke√ßin v…ô m…ôhsul m…ôlumatlarƒ±nƒ± daxil edin."
            }, status=status.HTTP_400_BAD_REQUEST)
        
        # Convert relative URL to absolute
        if not result['ad_image_url'].startswith('http'):
            result['ad_image_url'] = request.build_absolute_uri(result['ad_image_url'])
        
        # Apply company branding (logo + slogan from CompanyProfile)
        apply_branding = request.data.get('apply_branding', 'true').lower() == 'true'
        
        if apply_branding:
            try:
                from accounts.models import CompanyProfile
                from posts.branding import ImageBrandingService
                from posts.models import Post
                
                company_profile = CompanyProfile.objects.get(user=request.user)
                
                if company_profile.branding_enabled and company_profile.logo:
                    logger.info(f" Applying company branding to ad creative...")
                    logger.info(f"   Company logo: {company_profile.logo.name}")
                    logger.info(f"   Company slogan: {company_profile.slogan or 'None'}")
                    
                    # Create temporary post to use existing branding system
                    temp_post = Post.objects.create(
                        user=request.user,
                        content=result['ad_copy'],
                        status='draft',
                        title='Ad Creative'
                    )
                    
                    # Download ad image and save to temp post
                    if result['ad_image_url'].startswith('http'):
                        ad_img_response = requests.get(result['ad_image_url'], timeout=10)
                        from django.core.files.base import ContentFile
                        temp_post.custom_image.save(
                            f"temp_ad_{uuid.uuid4()}.png",
                            ContentFile(ad_img_response.content),
                            save=True
                        )
                    
                    # Apply branding using existing system
                    branding_service = ImageBrandingService(company_profile)
                    branded_image = branding_service.apply_branding(temp_post.custom_image.path)
                    output = branding_service.save_branded_image(branded_image, format='PNG')
                    
                    # Save branded version
                    branded_filename = f"ad_creatives/branded_ad_{uuid.uuid4()}.png"
                    branded_path = default_storage.save(branded_filename, ContentFile(output.read()))
                    result['ad_image_url'] = request.build_absolute_uri(default_storage.url(branded_path))
                    
                    # Clean up temp post
                    temp_post.delete()
                    
                    logger.info(f" Company branding applied to ad creative!")
                else:
                    logger.info(f"Branding skipped (enabled: {company_profile.branding_enabled}, has_logo: {bool(company_profile.logo)})")
                    
            except CompanyProfile.DoesNotExist:
                logger.warning(f"  No company profile - skipping branding")
            except Exception as e:
                logger.error(f"  Failed to apply branding: {e}")
                # Continue without branding
        
        logger.info(f" Ad creative generation complete!")
        
        return Response({
            "ad_image_url": result['ad_image_url'],
            "ad_copy": result['ad_copy'],
            "headline": result.get('headline', ''),
            "hashtags": result.get('hashtags', []),
            "cta": result.get('cta', 'Learn More'),
            "metadata": {
                "product_name": product_name,
                "format": ad_format,
                "style": style,
                "branding_applied": apply_branding,
                "generated_at": datetime.now().isoformat()
            }
        }, status=status.HTTP_200_OK)
        
    except ValueError as e:
        logger.error(f" Validation error: {str(e)}")
        return Response(
            {"error": str(e)},
            status=status.HTTP_400_BAD_REQUEST
        )
    except Exception as e:
        logger.error(f" Ad creative generation error: {str(e)}", exc_info=True)
        return Response({
            "error": "Reklam ≈ü…ôkli yaratma zamanƒ± x…ôta ba≈ü verdi",
            "details": str(e)
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


@api_view(['POST'])
@permission_classes([permissions.IsAuthenticated])
def image_to_video(request):
    """
    Convert image to video using Fal.ai Kling Video model
    
    POST /api/ai/fal-ai/image-to-video/
    
    Body:
    {
        "image_url": "https://example.com/image.jpg",
        "prompt": "Optional: describe the video motion",
        "duration": 5,  # Optional: video duration in seconds (default: 5)
        "fps": 24  # Optional: frames per second (default: 24)
    }
    
    Returns:
    {
        "video_url": "https://...",
        "status": "completed",
        "job_id": "..."
    }
    """
    user = request.user
    
    try:
        image_url = request.data.get('image_url')
        if not image_url:
            return Response({
                "error": "image_url t…ôl…ôb olunur"
            }, status=status.HTTP_400_BAD_REQUEST)
        
        prompt = request.data.get('prompt')
        duration = request.data.get('duration', 5)
        fps = request.data.get('fps', 24)
        
        logger.info(f" Image-to-video request from user: {user.email}")
        logger.info(f"   Image URL: {image_url}")
        logger.info(f"   Prompt: {prompt}")
        
        # Check if Fal.ai service is available
        if not FAL_AI_AVAILABLE:
            return Response({
                "error": "Fal.ai service m√∂vcud deyil. Z…ôhm…ôt olmasa fal-client paketi install edin: pip install fal-client"
            }, status=status.HTTP_503_SERVICE_UNAVAILABLE)
        
        # Initialize Fal.ai service
        try:
            fal_service = FalAIService(user=user)
        except ImportError as import_err:
            safe_log_error(logger.error, f" FalAIService import error: {str(import_err)}")
            return Response({
                "error": f"Fal.ai service ba≈ülatƒ±la bilm…ôdi: {str(import_err)}"
            }, status=status.HTTP_503_SERVICE_UNAVAILABLE)
        
        # Convert image to video
        result = fal_service.image_to_video(
            image_url=image_url,
            prompt=prompt,
            duration=duration,
            fps=fps
        )
        
        # Optionally download and save video
        save_to_storage = request.data.get('save_to_storage', False)
        if save_to_storage:
            saved_url = fal_service.download_and_save(
                result['video_url'],
                user.id,
                prefix="fal_ai_videos"
            )
            result['saved_video_url'] = saved_url
        
        return Response({
            "success": True,
            "video_url": result['video_url'],
            "status": result['status'],
            "job_id": result['job_id'],
            "saved_video_url": result.get('saved_video_url')
        }, status=status.HTTP_200_OK)
        
    except ValueError as e:
        logger.error(f" Validation error: {str(e)}")
        return Response({
            "error": str(e)
        }, status=status.HTTP_400_BAD_REQUEST)
    except TimeoutError as e:
        logger.error(f"  Timeout error: {str(e)}")
        return Response({
            "error": "Video yaratma zamanƒ± g√∂zl…ôm…ô m√ºdd…ôti bitdi. Z…ôhm…ôt olmasa, yenid…ôn c…ôhd edin."
        }, status=status.HTTP_504_GATEWAY_TIMEOUT)
    except Exception as e:
        logger.error(f" Image-to-video error: {str(e)}", exc_info=True)
        return Response({
            "error": "Video yaratma zamanƒ± x…ôta ba≈ü verdi",
            "details": str(e)
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


@api_view(['POST'])
@permission_classes([permissions.IsAuthenticated])
def edit_image(request):
    """
    Edit image using Fal.ai Nano Banana Pro model
    
    POST /api/ai/fal-ai/edit-image/
    
    Body:
    {
        "image_url": "https://example.com/image.jpg",
        "prompt": "Make the background blue, add sunset",
        "strength": 0.8  # Optional: edit strength 0.0-1.0 (default: 0.8)
    }
    
    Returns:
    {
        "image_url": "https://...",
        "status": "completed",
        "job_id": "..."
    }
    """
    user = request.user
    
    try:
        image_url = request.data.get('image_url')
        prompt = request.data.get('prompt')
        
        if not image_url:
            return Response({
                "error": "image_url t…ôl…ôb olunur"
            }, status=status.HTTP_400_BAD_REQUEST)
        
        if not prompt:
            return Response({
                "error": "prompt t…ôl…ôb olunur (≈ü…ôkild…ô n…ô d…ôyi≈üiklik etm…ôk ist…ôyirs…ôn?)"
            }, status=status.HTTP_400_BAD_REQUEST)
        
        strength = float(request.data.get('strength', 0.8))
        strength = max(0.0, min(1.0, strength))  # Clamp between 0 and 1
        
        logger.info(f" Image edit request from user: {user.email}")
        logger.info(f"   Image URL: {image_url}")
        logger.info(f"   Prompt: {prompt}")
        logger.info(f"   Strength: {strength}")
        
        # Check if Fal.ai service is available
        if not FAL_AI_AVAILABLE:
            return Response({
                "error": "Fal.ai service m√∂vcud deyil. Z…ôhm…ôt olmasa fal-client paketi install edin: pip install fal-client"
            }, status=status.HTTP_503_SERVICE_UNAVAILABLE)
        
        # Initialize Fal.ai service
        try:
            fal_service = FalAIService(user=user)
        except ImportError as import_err:
            safe_log_error(logger.error, f" FalAIService import error: {str(import_err)}")
            return Response({
                "error": f"Fal.ai service ba≈ülatƒ±la bilm…ôdi: {str(import_err)}"
            }, status=status.HTTP_503_SERVICE_UNAVAILABLE)
        
        # Edit image
        result = fal_service.edit_image(
            image_url=image_url,
            prompt=prompt,
            strength=strength
        )
        
        # Optionally download and save image
        save_to_storage = request.data.get('save_to_storage', False)
        if save_to_storage:
            saved_url = fal_service.download_and_save(
                result['image_url'],
                user.id,
                prefix="fal_ai_edited"
            )
            result['saved_image_url'] = saved_url
        
        return Response({
            "success": True,
            "image_url": result['image_url'],
            "status": result['status'],
            "job_id": result['job_id'],
            "saved_image_url": result.get('saved_image_url')
        }, status=status.HTTP_200_OK)
        
    except ValueError as e:
        logger.error(f" Validation error: {str(e)}")
        return Response({
            "error": str(e)
        }, status=status.HTTP_400_BAD_REQUEST)
    except TimeoutError as e:
        logger.error(f"  Timeout error: {str(e)}")
        return Response({
            "error": "≈û…ôkil redakt…ôsi zamanƒ± g√∂zl…ôm…ô m√ºdd…ôti bitdi. Z…ôhm…ôt olmasa, yenid…ôn c…ôhd edin."
        }, status=status.HTTP_504_GATEWAY_TIMEOUT)
    except Exception as e:
        logger.error(f" Image edit error: {str(e)}", exc_info=True)
        return Response({
            "error": "≈û…ôkil redakt…ôsi zamanƒ± x…ôta ba≈ü verdi",
            "details": str(e)
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


@api_view(['POST'])
@permission_classes([permissions.IsAuthenticated])
def nano_banana_text_to_image(request):
    """
    Generate image from text using Fal.ai Nano Banana model
    
    POST /api/ai/fal-ai/nano-banana/text-to-image/
    
    Body:
    {
        "prompt": "A beautiful sunset over mountains",
        "product_name": "Optional product name for context",
        "product_description": "Optional product description for context",
        "width": 1024,  # Optional
        "height": 1024,  # Optional
        "enhance_prompt": true  # Optional: enhance prompt with AI (default: true)
    }
    
    Returns:
    {
        "image_url": "https://...",
        "status": "completed",
        "job_id": "...",
        "enhanced_prompt": "..."
    }
    """
    user = request.user
    
    try:
        prompt = request.data.get('prompt')
        if not prompt:
            return Response({
                "error": "prompt t…ôl…ôb olunur"
            }, status=status.HTTP_400_BAD_REQUEST)
        
        product_name = request.data.get('product_name')
        product_description = request.data.get('product_description')
        width = int(request.data.get('width', 1024))
        height = int(request.data.get('height', 1024))
        enhance_prompt = request.data.get('enhance_prompt', True)
        
        logger.info(f" Nano Banana text-to-image request from user: {user.email}")
        logger.info(f"   Prompt: {prompt}")
        
        # Check if Fal.ai service is available
        if not FAL_AI_AVAILABLE:
            return Response({
                "error": "Fal.ai service m√∂vcud deyil. Z…ôhm…ôt olmasa fal-client paketi install edin: pip install fal-client"
            }, status=status.HTTP_503_SERVICE_UNAVAILABLE)
        
        # Initialize Fal.ai service
        try:
            fal_service = FalAIService(user=user)
        except ImportError as import_err:
            safe_log_error(logger.error, f" FalAIService import error: {str(import_err)}")
            return Response({
                "error": f"Fal.ai service ba≈ülatƒ±la bilm…ôdi: {str(import_err)}"
            }, status=status.HTTP_503_SERVICE_UNAVAILABLE)
        
        # Enhance prompt if requested
        enhanced_prompt = prompt
        if enhance_prompt:
            enhanced_prompt = fal_service.enhance_prompt(
                prompt,
                product_name=product_name,
                product_description=product_description,
                context="ad_creative"
            )
        
        # Generate image
        result = fal_service.text_to_image(
            prompt=enhanced_prompt,
            width=width,
            height=height
        )
        
        # Optionally download and save image
        save_to_storage = request.data.get('save_to_storage', False)
        if save_to_storage:
            saved_url = fal_service.download_and_save(
                result['image_url'],
                user.id,
                prefix="nano_banana_generated"
            )
            result['saved_image_url'] = saved_url
        
        return Response({
            "success": True,
            "image_url": result['image_url'],
            "status": result['status'],
            "job_id": result['job_id'],
            "enhanced_prompt": enhanced_prompt if enhance_prompt else None,
            "saved_image_url": result.get('saved_image_url')
        }, status=status.HTTP_200_OK)
        
    except ValueError as e:
        logger.error(f" Validation error: {str(e)}")
        return Response({
            "error": str(e)
        }, status=status.HTTP_400_BAD_REQUEST)
    except TimeoutError as e:
        logger.error(f"  Timeout error: {str(e)}")
        return Response({
            "error": "≈û…ôkil yaratma zamanƒ± g√∂zl…ôm…ô m√ºdd…ôti bitdi. Z…ôhm…ôt olmasa, yenid…ôn c…ôhd edin."
        }, status=status.HTTP_504_GATEWAY_TIMEOUT)
    except Exception as e:
        logger.error(f" Text-to-image error: {str(e)}", exc_info=True)
        return Response({
            "error": "≈û…ôkil yaratma zamanƒ± x…ôta ba≈ü verdi",
            "details": str(e)
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


@api_view(['POST'])
@permission_classes([permissions.IsAuthenticated])
def nano_banana_image_to_image(request):
    """
    Transform image using Fal.ai Nano Banana model (image-to-image)
    
    POST /api/ai/fal-ai/nano-banana/image-to-image/
    
    Body:
    {
        "image_url": "https://example.com/image.jpg",
        "prompt": "Make it more vibrant, add professional lighting",
        "product_name": "Optional product name for context",
        "product_description": "Optional product description for context",
        "strength": 0.8,  # Optional: transformation strength 0.0-1.0 (default: 0.8)
        "enhance_prompt": true  # Optional: enhance prompt with AI (default: true)
    }
    
    Returns:
    {
        "image_url": "https://...",
        "status": "completed",
        "job_id": "...",
        "enhanced_prompt": "..."
    }
    """
    user = request.user
    
    try:
        image_url = request.data.get('image_url')
        prompt = request.data.get('prompt')
        
        if not image_url:
            return Response({
                "error": "image_url t…ôl…ôb olunur"
            }, status=status.HTTP_400_BAD_REQUEST)
        
        if not prompt:
            return Response({
                "error": "prompt t…ôl…ôb olunur"
            }, status=status.HTTP_400_BAD_REQUEST)
        
        product_name = request.data.get('product_name')
        product_description = request.data.get('product_description')
        strength = float(request.data.get('strength', 0.8))
        strength = max(0.0, min(1.0, strength))  # Clamp between 0 and 1
        enhance_prompt = request.data.get('enhance_prompt', True)
        
        logger.info(f" Nano Banana image-to-image request from user: {user.email}")
        logger.info(f"   Image URL: {image_url}")
        logger.info(f"   Prompt: {prompt}")
        
        # Check if Fal.ai service is available
        if not FAL_AI_AVAILABLE:
            return Response({
                "error": "Fal.ai service m√∂vcud deyil. Z…ôhm…ôt olmasa fal-client paketi install edin: pip install fal-client"
            }, status=status.HTTP_503_SERVICE_UNAVAILABLE)
        
        # Initialize Fal.ai service
        try:
            fal_service = FalAIService(user=user)
        except ImportError as import_err:
            safe_log_error(logger.error, f" FalAIService import error: {str(import_err)}")
            return Response({
                "error": f"Fal.ai service ba≈ülatƒ±la bilm…ôdi: {str(import_err)}"
            }, status=status.HTTP_503_SERVICE_UNAVAILABLE)
        
        # Enhance prompt if requested
        enhanced_prompt = prompt
        if enhance_prompt:
            enhanced_prompt = fal_service.enhance_prompt(
                prompt,
                product_name=product_name,
                product_description=product_description,
                context="ad_creative"
            )
        
        # Transform image
        result = fal_service.image_to_image(
            image_url=image_url,
            prompt=enhanced_prompt,
            strength=strength
        )
        
        # Optionally download and save image
        save_to_storage = request.data.get('save_to_storage', False)
        if save_to_storage:
            saved_url = fal_service.download_and_save(
                result['image_url'],
                user.id,
                prefix="nano_banana_transformed"
            )
            result['saved_image_url'] = saved_url
        
        return Response({
            "success": True,
            "image_url": result['image_url'],
            "status": result['status'],
            "job_id": result['job_id'],
            "enhanced_prompt": enhanced_prompt if enhance_prompt else None,
            "saved_image_url": result.get('saved_image_url')
        }, status=status.HTTP_200_OK)
        
    except ValueError as e:
        logger.error(f" Validation error: {str(e)}")
        return Response({
            "error": str(e)
        }, status=status.HTTP_400_BAD_REQUEST)
    except TimeoutError as e:
        logger.error(f"  Timeout error: {str(e)}")
        return Response({
            "error": "≈û…ôkil transformasiyasƒ± zamanƒ± g√∂zl…ôm…ô m√ºdd…ôti bitdi. Z…ôhm…ôt olmasa, yenid…ôn c…ôhd edin."
        }, status=status.HTTP_504_GATEWAY_TIMEOUT)
    except Exception as e:
        logger.error(f" Image-to-image error: {str(e)}", exc_info=True)
        return Response({
            "error": "≈û…ôkil transformasiyasƒ± zamanƒ± x…ôta ba≈ü verdi",
            "details": str(e)
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


@api_view(['POST'])
@permission_classes([permissions.IsAuthenticated])
def kling_video_text_to_video(request):
    """
    Generate video from text using Fal.ai Kling Video model
    
    POST /api/ai/fal-ai/kling-video/text-to-video/
    
    Body:
    {
        "prompt": "A beautiful sunset over mountains with birds flying",
        "product_name": "Optional product name for context",
        "product_description": "Optional product description for context",
        "duration": 5,  # Optional: video duration in seconds (default: 5)
        "fps": 24,  # Optional: frames per second (default: 24)
        "width": 1024,  # Optional
        "height": 576,  # Optional
        "enhance_prompt": true  # Optional: enhance prompt with AI (default: true)
    }
    
    Returns:
    {
        "video_url": "https://...",
        "status": "completed",
        "job_id": "...",
        "enhanced_prompt": "..."
    }
    """
    user = request.user
    
    try:
        prompt = request.data.get('prompt')
        if not prompt:
            return Response({
                "error": "prompt t…ôl…ôb olunur"
            }, status=status.HTTP_400_BAD_REQUEST)
        
        product_name = request.data.get('product_name')
        product_description = request.data.get('product_description')
        duration = int(request.data.get('duration', 5))
        fps = int(request.data.get('fps', 24))
        width = int(request.data.get('width', 1024))
        height = int(request.data.get('height', 576))
        enhance_prompt = request.data.get('enhance_prompt', True)
        
        logger.info(f" Kling Video text-to-video request from user: {user.email}")
        logger.info(f"   Prompt: {prompt}")
        
        # Check if Fal.ai service is available
        if not FAL_AI_AVAILABLE:
            return Response({
                "error": "Fal.ai service m√∂vcud deyil. Z…ôhm…ôt olmasa fal-client paketi install edin: pip install fal-client"
            }, status=status.HTTP_503_SERVICE_UNAVAILABLE)
        
        # Initialize Fal.ai service
        try:
            fal_service = FalAIService(user=user)
        except ImportError as import_err:
            safe_log_error(logger.error, f" FalAIService import error: {str(import_err)}")
            return Response({
                "error": f"Fal.ai service ba≈ülatƒ±la bilm…ôdi: {str(import_err)}"
            }, status=status.HTTP_503_SERVICE_UNAVAILABLE)
        
        # Enhance prompt if requested
        enhanced_prompt = prompt
        if enhance_prompt:
            enhanced_prompt = fal_service.enhance_prompt(
                prompt,
                product_name=product_name,
                product_description=product_description,
                context="video"
            )
        
        # Generate video
        result = fal_service.text_to_video(
            prompt=enhanced_prompt,
            duration=duration,
            fps=fps,
            width=width,
            height=height
        )
        
        # Optionally download and save video
        save_to_storage = request.data.get('save_to_storage', False)
        if save_to_storage:
            saved_url = fal_service.download_and_save(
                result['video_url'],
                user.id,
                prefix="kling_video_generated"
            )
            result['saved_video_url'] = saved_url
        
        return Response({
            "success": True,
            "video_url": result['video_url'],
            "status": result['status'],
            "job_id": result['job_id'],
            "enhanced_prompt": enhanced_prompt if enhance_prompt else None,
            "saved_video_url": result.get('saved_video_url')
        }, status=status.HTTP_200_OK)
        
    except ValueError as e:
        logger.error(f" Validation error: {str(e)}")
        return Response({
            "error": str(e)
        }, status=status.HTTP_400_BAD_REQUEST)
    except TimeoutError as e:
        logger.error(f"  Timeout error: {str(e)}")
        return Response({
            "error": "Video yaratma zamanƒ± g√∂zl…ôm…ô m√ºdd…ôti bitdi. Z…ôhm…ôt olmasa, yenid…ôn c…ôhd edin."
        }, status=status.HTTP_504_GATEWAY_TIMEOUT)
    except Exception as e:
        logger.error(f" Text-to-video error: {str(e)}", exc_info=True)
        return Response({
            "error": "Video yaratma zamanƒ± x…ôta ba≈ü verdi",
            "details": str(e)
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


@api_view(['POST'])
@permission_classes([permissions.IsAuthenticated])
def create_product_post(request):
    """
    FOUR-STEP MARKETING WORKFLOW for Product Post Creation (AZERBAIJANI CONTENT)
    
    Execute a comprehensive marketing workflow: background removal, structured product analysis,
    high-conversion content generation (in Azerbaijani), and technical AI prompt creation for Nano Banana.
    
    POST /api/ai/product-post/
    
    Body (multipart/form-data):
    {
        "product_image": File (required) - Product image to process,
        "product_name": str (optional) - Product name (will be AI-suggested if not provided),
        "product_description": str (optional) - Product description,
        "num_images": int (default: 3) - Number of ad variations to generate (1-5)
    }
    
    WORKFLOW STEPS:
    
    1. IMAGE PROCESSING (Background Removal):
       - Accepts uploaded image
       - Digitally removes background to isolate product
       - Creates transparent subject for analysis
    
    2. PRODUCT ANALYSIS (Structured Breakdown):
       - Product Name/Type: Identifies exact product category
       - Color Palette: Primary and secondary colors
       - Material & Texture: Surface materials and finish
       - Intended Use: Product function and purpose
       - Target Industry: Market sector identification
    
    3. ADVERTISING CONTENT GENERATION:
       - Hook: Catchy headline (50-80 chars)
       - Body: Benefits and features (150-250 chars)
       - Call to Action (CTA): Purchase encouragement (40-60 chars)
       - Hashtags: 10-15 relevant, high-traffic hashtags
       - Tone: Professional, engaging, persuasive (Instagram/Facebook style)
    
    4. GENERATIVE AI PROMPT CREATION:
       - Technical prompt optimized for Nano Banana/Stable Diffusion/Midjourney
       - Format: [Subject] + [Action/Pose] + [Context] + [Background] + [Lighting] + [Specs]
       - Goal: Place product in lifestyle context with professional model
       - Example: "Model wearing [Product] walking down busy NY street, golden hour, cinematic 4K"
    
    Returns:
    {
        "success": true,
        "message": "Four-step marketing workflow completed successfully",
        "workflow_summary": {
            "step_1": "Background removal completed",
            "step_2": "Product analysis with structured breakdown completed",
            "step_3": "High-conversion advertising content generated",
            "step_4": "Technical AI prompts created"
        },
        "posts": [
            {
                "id": "uuid",
                "hook": "Catchy headline",
                "body": "Benefits-focused content",
                "cta": "Clear call to action",
                "full_caption": "Complete caption text",
                "hashtags": ["#tag1", "#tag2", ...],
                "complete_content": "Hook + Body + CTA + Hashtags",
                "image_generation_prompt": "Technical prompt for AI image generation",
                "status": "pending_approval",
                "design_context": "Lifestyle photography context"
            },
            ...
        ],
        "product_analysis": {
            "product_name_type": "Exact product name and type",
            "product_type": "Category",
            "color_palette": {
                "primary_colors": [...],
                "secondary_colors": [...],
                "color_description": "..."
            },
            "material_texture": {
                "materials": [...],
                "texture": "...",
                "finish": "..."
            },
            "intended_use": "Function and purpose",
            "target_industry": "Market sector",
            "visual_analysis": {...},
            "features": [...],
            "benefits": [...],
            "target_audience": "...",
            "selling_points": [...],
            "lifestyle_context": "..."
        },
        "images": {
            "original_image_url": "https://...",
        "background_removed_image_url": "https://..."
        },
        "num_created": 3
    }
    """
    user = request.user
    
    try:
        # Get product image
        product_image = request.FILES.get('product_image')
        if not product_image:
            return Response({
                "error": "M…ôhsul r…ôsmi t…ôl…ôb olunur"
            }, status=status.HTTP_400_BAD_REQUEST)
        
        product_name = request.data.get('product_name', '')
        product_description = request.data.get('product_description', '')
        num_images = int(request.data.get('num_images', 3))
        
        logger.info(f"üõçÔ∏è Product post creation request from user: {user.email}")
        logger.info(f"   Product name: {product_name}")
        logger.info(f"   Number of images: {num_images}")
        
        # Step 1: Save product image to local storage
        logger.info("üì§ Step 1: Saving product image to local storage...")
        
        from django.core.files.storage import default_storage
        from django.core.files.base import ContentFile
        
        # Save product image to media folder
        product_image.seek(0)  # Reset file pointer
        product_filename = f"product_images/product_{uuid.uuid4()}.{product_image.name.split('.')[-1]}"
        saved_path = default_storage.save(product_filename, product_image)
        
        # Get URL for the saved image
        from django.conf import settings
        base_url = request.build_absolute_uri('/').rstrip('/')
        media_url = getattr(settings, 'MEDIA_URL', '/media/')
        if not media_url.startswith('http'):
            original_image_url = f"{base_url}{media_url}{saved_path}"
        else:
            original_image_url = f"{media_url}{saved_path}"
        
        logger.info(f"‚úÖ Product image saved to: {saved_path}")
        logger.info(f"‚úÖ Product image URL: {original_image_url}")
        
        # Step 2: Remove background using Fal.ai
        # NOTE: Background removal might not work perfectly, so we'll use original image if needed
        background_removed_url = None
        if FAL_AI_AVAILABLE:
            try:
                logger.info("üé® Step 2: Attempting background removal...")
                fal_service = FalAIService(user=user)
                
                # Try to remove background, but use lower strength to preserve product
                bg_removal_result = fal_service.edit_image(
                    image_url=original_image_url,
                    prompt="remove background completely, make background transparent, keep product exactly as is, do not modify product",
                    strength=0.7  # Lower strength to preserve product better
                )
                
                background_removed_url = bg_removal_result['image_url']
                logger.info(f"‚úÖ Background removal attempted: {background_removed_url}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Background removal failed: {str(e)}, using original image")
                background_removed_url = original_image_url
        else:
            logger.warning("‚ö†Ô∏è Fal.ai not available, using original image")
            background_removed_url = original_image_url
        
        # For image-to-image, we'll use the original image if background removal didn't work well
        # This ensures the product is preserved
        image_for_generation = background_removed_url if background_removed_url != original_image_url else original_image_url
        
        # Step 3: Analyze product using ChatGPT (ENHANCED WITH STRUCTURED BREAKDOWN)
        logger.info("üîç Step 3: Analyzing product with structured breakdown...")
        product_analysis = None
        product_type = None
        
        # Use product_name from user, or ask ChatGPT to suggest based on description
        final_product_name = product_name.strip() if product_name else None
        
        try:
            openai_client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
            
            # If user didn't provide name, ask ChatGPT to suggest one based on description
            if not final_product_name and product_description:
                logger.info("üí° Asking ChatGPT to suggest product name...")
                name_response = openai_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {
                            "role": "system",
                            "content": "S…ôn pe≈ü…ôkar m…ôhsul analiti√ßisisin. M…ôhsulun t…ôsvirin…ô …ôsas…ôn d…ôqiq m…ôhsul adƒ±nƒ± m√º…ôyy…ônl…ô≈üdirirs…ôn."
                        },
                        {
                            "role": "user",
                            "content": f"""A≈üaƒüƒ±dakƒ± m…ôhsul t…ôsvirin…ô …ôsas…ôn konkret m…ôhsul adƒ±nƒ± m√º…ôyy…ônl…ô≈üdir. Generic adlar yazma (m…ôs…ôl…ôn: "telefon", "qulaqcƒ±q"), konkret ad yaz (m…ôs…ôl…ôn: "iPhone 15 Pro", "AirPods Pro").

M…ôhsul t…ôsviri: {product_description}

Yalnƒ±z m…ôhsulun adƒ±nƒ± yaz, …ôlav…ô m…ôtn yazma."""
                        }
                    ],
                    temperature=0.7,
                    max_tokens=50
                )
                suggested_name = name_response.choices[0].message.content.strip()
                if suggested_name and len(suggested_name) > 2:
                    final_product_name = suggested_name
                    logger.info(f"‚úÖ ChatGPT suggested product name: {final_product_name}")
            
            # If still no name, use default
            if not final_product_name:
                final_product_name = "M…ôhsul"
            
            # ENHANCED: Structured product analysis matching the required format (AZERBAIJANI)
            analysis_prompt = f"""A≈üaƒüƒ±dakƒ± m…ôhsul √º√ß√ºn …ôtraflƒ± vizual yoxlama aparƒ±n v…ô STRUKTURLA≈ûDIRILMI≈û ANALƒ∞Z t…ôqdim edin. JSON formatƒ±nda cavab verin:

M…ôhsul Adƒ±: {final_product_name}
M…ôhsul T…ôsviri: {product_description or 'T…ôsvir verilm…ôyib'}

T∆èL∆èBOLUNANstrukturu:
{{
    "product_name_type": "D…ôqiq m…ôhsul adƒ± v…ô n√∂v√º (m…ôs…ôl…ôn: 'Simsiz Bluetooth Qulaqcƒ±q', 'D…ôri √áanta', 'Aƒüƒ±llƒ± Saat')",
    "product_type": "Kateqoriya n√∂v√º (m…ôs…ôl…ôn: Elektronika, Moda, Ev Dekorasiyasƒ±, Aksesuar)",
    "color_palette": {{
        "primary_colors": ["∆èsas r…ông 1", "∆èsas r…ông 2"],
        "secondary_colors": ["Aksent r…ông 1", "Aksent r…ông 2"],
        "color_description": "R…ông sxeminin qƒ±sa t…ôsviri"
    }},
    "material_texture": {{
        "materials": ["∆èsas material", "ƒ∞kinci material"],
        "texture": "∆ètraflƒ± tekstura t…ôsviri (m…ôs…ôl…ôn: Hamar ƒ∞p…ôk, Fƒ±r√ßalanmƒ±≈ü Metal, Mat Plastik, Yum≈üaq D…ôri)",
        "finish": "S…ôth √ºzl√ºy√º (m…ôs…ôl…ôn: Parlaq, Mat, Fƒ±r√ßalanmƒ±≈ü, Cilalanmƒ±≈ü)"
    }},
    "intended_use": "M…ôhsulun funksiyasƒ± v…ô m…ôqs…ôdi n…ôdir? (1-2 c√ºml…ô)",
    "target_industry": "∆èsas s…ônaye/bazar sektoru (m…ôs…ôl…ôn: Moda v…ô Geyim, ƒ∞stehlak Elektronikasƒ±, Ev v…ô Ya≈üayƒ±≈ü, G√∂z…ôllik v…ô Kosmetika)",
    "visual_analysis": {{
        "shape": "∆ètraflƒ± forma t…ôsviri",
        "size": "√ñl√ß√º t…ôsviri",
        "design_style": "Dizayn stili (m…ôs…ôl…ôn: Minimalist, M√ºasir, Klassik, Vintage, √áaƒüda≈ü)",
        "special_details": "Unikal x√ºsusiyy…ôtl…ôr v…ô detallar"
    }},
    "features": ["∆èsas x√ºsusiyy…ôt 1", "∆èsas x√ºsusiyy…ôt 2", "∆èsas x√ºsusiyy…ôt 3"],
    "benefits": ["Fayda 1", "Fayda 2", "Fayda 3"],
    "target_audience": "H…ôd…ôf demoqrafik t…ôsvir",
    "selling_points": ["Satƒ±≈ü n√∂qt…ôsi 1", "Satƒ±≈ü n√∂qt…ôsi 2", "Satƒ±≈ü n√∂qt…ôsi 3"],
    "keywords": ["a√ßar s√∂z 1", "a√ßar s√∂z 2", "a√ßar s√∂z 3"],
    "visual_description": "Tam vizual t…ôsvir (r…ông, forma, material, dizayn, detallar - 150-200 simvol)",
    "lifestyle_context": "Fotoqrafiya √º√ß√ºn t…ôklif olunan h…ôyat t…ôrzi konteksti (m…ôs…ôl…ôn: '≈û…ôh…ôr pe≈ü…ôkar m√ºhiti', 'A√ßƒ±q hava mac…ôra s…ôhn…ôsi', 'L√ºks ev interyeri')"
}}

Yalnƒ±z JSON cavab verin, …ôlav…ô m…ôtn yazMAYIN."""
            
            logger.info("ü§ñ Getting structured product analysis from ChatGPT...")
            analysis_response = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "S…ôn ekspert m…ôhsul analitiki v…ô marketinq strateqis…ôn. S…ôn …ôtraflƒ±, strukturla≈üdƒ±rƒ±lmƒ±≈ü m…ôhsul analizi t…ôqdim edirs…ôn. H…ômi≈ü…ô Az…ôrbaycan dilind…ô JSON formatƒ±nda cavab verirs…ôn."
                    },
                    {
                        "role": "user",
                        "content": analysis_prompt
                    }
                ],
                temperature=0.7,
                max_tokens=1500
            )
            
            analysis_text = analysis_response.choices[0].message.content.strip()
            # Remove markdown code blocks if present
            if analysis_text.startswith('```json'):
                analysis_text = analysis_text[7:]
            if analysis_text.startswith('```'):
                analysis_text = analysis_text[3:]
            if analysis_text.endswith('```'):
                analysis_text = analysis_text[:-3]
            analysis_text = analysis_text.strip()
            
            product_analysis = json.loads(analysis_text)
            product_type = product_analysis.get('product_type', 'm…ôhsul')
            
            # Ensure product_name is set correctly
            product_analysis['name'] = final_product_name
            product_analysis['product_name'] = final_product_name
            
            # Override description if user provided one
            if product_description:
                product_analysis['description'] = product_description
            
            logger.info(f"‚úÖ Product analyzed: {final_product_name} (Type: {product_type})")
            
        except Exception as e:
            logger.error(f"‚ùå Product analysis failed: {str(e)}", exc_info=True)
            # Use fallback analysis
            product_analysis = {
                "name": final_product_name or "M…ôhsul",
                "product_name": final_product_name or "M…ôhsul",
                "product_type": "m…ôhsul",
                "product_category": "√ºmumi",
                "description": product_description or "M…ôhsul t…ôsviri",
                "visual_description": "M…ôhsulun vizual t…ôsviri",
                "features": [],
                "benefits": [],
                "target_audience": "Geni≈ü auditoriya",
                "selling_points": [],
                "keywords": []
            }
            product_type = "m…ôhsul"
        
        # Step 4: Generate HIGH-CONVERSION ADVERTISING CONTENT (Instagram/Facebook Style - AZERBAIJANI)
        logger.info("‚úçÔ∏è Step 4: Generating high-conversion social media content in Azerbaijani...")
        generated_content = None
        try:
            content_prompt = f"""A≈üaƒüƒ±daki m…ôhsul √º√ß√ºn {num_images} d…ôn…ô c…ôlbedici, y√ºks…ôk konversiyalƒ± sosial media ba≈ülƒ±qlarƒ± yaradƒ±n (Instagram/Facebook stili):

M…ôhsul Analizi:
{json.dumps(product_analysis, ensure_ascii=False, indent=2)}

√áOX VACƒ∞B: M…ôhsulun adƒ± "{product_analysis.get('name', 'M…ôhsul')}"-dƒ±r. Postlarda konkret m…ôhsul adƒ±nƒ± istifad…ô edin - generic terminl…ôr yazmayƒ±n!

TON: Pe≈ü…ôkar, c…ôlbedici v…ô inandƒ±rƒ±cƒ±

H∆èR POST √ú√á√úN STRUKTUR (QATI FORMAT):
1. HOOK (QARMAQ√áQ…ô - diqq…ôti c…ôlb ed…ôn ba≈ülƒ±q (1 g√ºcl√º c√ºml…ô, 50-80 simvol)
2. BODY (∆èSAS M∆èTN): Faydalarƒ± v…ô x√ºsusiyy…ôtl…ôri vurƒüulayƒ±n (2-3 c√ºml…ô, 150-250 simvol)
   - M√º≈üt…ôrinin …ôld…ô etdikl…ôrin…ô fokuslanƒ±n
   - Emosional t…ôsirl…ôr istifad…ô edin
   - Unikal satƒ±≈ü n√∂qt…ôl…ôrini vurƒüulayƒ±n
3. CALL TO ACTION (CTA - F∆èALIYY∆èT∆è √áAƒûIRI≈û): Alƒ±≈üƒ± v…ô ya qar≈üƒ±lƒ±qlƒ± …ôlaq…ôni t…ô≈üviq edin (1 aydƒ±n c√ºml…ô, 40-60 simvol)
   - N√ºmun…ôl…ôr: "ƒ∞ndi sifari≈ü et!", "Bu g√ºn…ô s…ônin olsun!", "M…ôhdud sayda!", "H…ôyatƒ±nƒ± d…ôyi≈üdir!"
4. HASHTAGS (HE≈ûTEQL∆èR): 10-15 relevant, y√ºks…ôk trafikli he≈üteq

JSON formatƒ±nda cavab verin:
{{
    "posts": [
        {{
            "hook": "Diqq…ôti c…ôlb ed…ôn c…ôlbedici ba≈ülƒ±q",
            "body": "Faydaya fokuslanmƒ±≈ü m…ôzmun, x√ºsusiyy…ôtl…ôri vurƒüulayƒ±r. D…ôy…ôr t…ôklifini vurƒüulayƒ±n. Arzu yaradƒ±n.",
            "cta": "Qar≈üƒ±lƒ±qlƒ± …ôlaq…ôni t…ô≈üviq ed…ôn aydƒ±n f…ôaliyy…ôt…ô √ßaƒüƒ±rƒ±≈ü",
            "hashtags": ["#he≈üteq1", "#he≈üteq2", "#he≈üteq3", ... (10-15 he≈üteq)],
            "full_caption": "Hook + Body + CTA birl…ô≈üdirilmi≈ü tam ba≈ülƒ±q",
            "design_context": "H…ôyat t…ôrzi fotoqrafiya konteksti (m…ôs…ôl…ôn: 'Model m…ôhsulu ≈ü…ôh…ôr k√º√ß…ôsind…ô geyinir', 'M…ôhsul m√ºasir i≈ü yerind…ô', 'M…ôhsul l√ºks h…ôyat t…ôrzi m√ºhitind…ô')"
        }},
        ...
    ]
}}

HE≈ûTEQ T∆èL∆èBL∆èRƒ∞:
- Populyar (#moda, #stil, #baku, #azerbaijan) v…ô ni≈ü he≈üt…ôql…ôrin qarƒ±≈üƒ±ƒüƒ±
- M…ôhsul kateqoriyasƒ± he≈üt…ôql…ôri daxil edin
- H…ôyat t…ôrzi/ist…ôk he≈üt…ôql…ôri daxil edin
- F…ôaliyy…ôt/CTA he≈üt…ôql…ôri daxil edin (#indisifari≈ü, #m…ôhdudsay)
- H…ôm Az…ôrbaycan, h…ôm d…ô beyn…ôlxalq he≈üt…ôql…ôr istifad…ô edin

Yalnƒ±z JSON cavab verin, …ôlav…ô m…ôtn yazMAYIN."""
            
            response = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "S…ôn pe≈ü…ôkar sosial media marketinq eksperti v…ô y√ºks…ôk konversiyalƒ± reklam m…ôzmunu yaratmaqda ixtisasla≈üan kopyrayters…ôn. H…ômi≈ü…ô Az…ôrbaycan dilind…ô JSON formatƒ±nda cavab verirs…ôn."
                    },
                    {
                        "role": "user",
                        "content": content_prompt
                    }
                ],
                temperature=0.8,
                max_tokens=3000
            )
            
            content_text = response.choices[0].message.content.strip()
            # Remove markdown code blocks
            if content_text.startswith('```json'):
                content_text = content_text[7:]
            if content_text.startswith('```'):
                content_text = content_text[3:]
            if content_text.endswith('```'):
                content_text = content_text[:-3]
            content_text = content_text.strip()
            
            generated_content = json.loads(content_text)
            logger.info(f"‚úÖ Content generated: {len(generated_content.get('posts', []))} posts")
        except Exception as e:
            logger.error(f"‚ùå Content generation failed: {str(e)}", exc_info=True)
            return Response({
                "error": f"M…ôzmun yaradƒ±la bilm…ôdi: {str(e)}"
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        
        # Step 5: Generate TECHNICAL PROMPTS for Nano Banana / Stable Diffusion (OPTIMIZED FORMAT)
        logger.info("üìù Step 5: Generating technical AI prompts for image generation...")
        posts_data = generated_content.get('posts', [])
        created_posts = []
        
        # Extract detailed analysis for prompt generation
        color_palette = product_analysis.get('color_palette', {})
        primary_colors = color_palette.get('primary_colors', [])
        secondary_colors = color_palette.get('secondary_colors', [])
        
        material_texture = product_analysis.get('material_texture', {})
        materials = material_texture.get('materials', [])
        texture = material_texture.get('texture', '')
        finish = material_texture.get('finish', '')
        
        visual_analysis = product_analysis.get('visual_analysis', {})
        design_style = visual_analysis.get('design_style', 'modern')
        shape = visual_analysis.get('shape', '')
        
        product_name_type = product_analysis.get('product_name_type', product_analysis.get('name', 'Product'))
        product_type = product_analysis.get('product_type', 'Product')
        visual_description = product_analysis.get('visual_description', '')
        lifestyle_context = product_analysis.get('lifestyle_context', 'professional studio setting')
        
        logger.info(f"üìù Generating {num_images} technical prompts optimized for Nano Banana/Stable Diffusion...")
        
        for idx, post_data in enumerate(posts_data[:num_images], 1):
            try:
                logger.info(f"üé® Creating technical prompt {idx}/{num_images}...")
                
                # Get design context from generated content
                design_context = post_data.get('design_context', lifestyle_context)
                
                # TECHNICAL PROMPT FORMAT for Nano Banana (AZERBAIJANI): [Subject] + [Action/Pose] + [Outfit/Context] + [Background/Environment] + [Lighting/Style] + [Aspect Ratio]
                
                # H∆èR POST √ú√á√úN TAMAMƒ∞L∆è F∆èRQLƒ∞ S∆èHN∆èL∆èR - 10+ m√ºxt…ôlif variant
                scenes = [
                    {
                        "action": "pe≈ü…ôkar biznes modeli m…ôhsulu tutub n√ºmayi≈ü etdirir",
                        "pose": "√∂z√ºn…ôinamlƒ± poz, birba≈üa g√∂z t…ômasƒ±, g√ºl√ºms…ôm…ô",
                        "context": "qƒ±zƒ±l saat vaxtƒ± i≈üƒ±qlƒ± Nyu York k√º√ß…ôsind…ô g…ôzir",
                        "background": "bulanƒ±q ≈ü…ôh…ôr m…ônz…ôr…ôsi bokeh effekti il…ô, yum≈üaq ax≈üam i≈üƒ±qlarƒ±, sarƒ± taksi, insanlar",
                        "lighting": "qƒ±zƒ±l saat i≈üƒ±qlandƒ±rmasƒ±, isti g√ºn batƒ±mƒ± tonlarƒ±, subyektd…ô hal…ô i≈üƒ±qlandƒ±rmasƒ±",
                        "style": "urban professional, street fashion, cinematic",
                        "mood": "energetic, confident, urban lifestyle"
                    },
                    {
                        "action": "h…ôyat t…ôrzi modeli m…ôhsulu t…ôbii ≈ü…ôkild…ô istifad…ô edir",
                        "pose": "t…ôbii poz, m…ôhsulla s…ômimi qar≈üƒ±lƒ±qlƒ± …ôlaq…ô, rahat",
                        "context": "l√ºks atmosferli m√ºasir minimalist studiya",
                        "background": "t…ômiz aƒü/boz qradiyent fon, pe≈ü…ôkar studiya qurulu≈üu, minimal dekor",
                        "lighting": "pe≈ü…ôkar studiya i≈üƒ±qlandƒ±rmasƒ±, yum≈üaq …ôsas i≈üƒ±q, inc…ô doldurma i≈üƒ±ƒüƒ±, dramatik hal…ô i≈üƒ±ƒüƒ±",
                        "style": "minimalist, clean, luxury studio",
                        "mood": "serene, elegant, sophisticated"
                    },
                    {
                        "action": "pe≈ü…ôkar model m…ôhsulu z…ôrif ≈ü…ôkild…ô n√ºmayi≈ü etdirir",
                        "pose": "dinamik h…ôr…ôk…ôt pozu, √º√ßd√∂rd…ô bucaq, …ôl h…ôr…ôk…ôti",
                        "context": "m√ºasir memarlƒ±qlƒ± y√ºks…ôk s…ôviyy…ôli ≈ü…ôh…ôr m√ºhiti",
                        "background": "m√ºasir memarlƒ±q elementl…ôri, ≈ü√º≈ü…ô v…ô polad strukturlar, d…ôrinlik sah…ôsi, qeyri-m√º…ôyy…ôn binalar",
                        "lighting": "t…ôbii g√ºn i≈üƒ±ƒüƒ± pe≈ü…ôkar i≈üƒ±qlandƒ±rma il…ô qarƒ±≈üƒ±q, kinematik r…ông qreydini",
                        "style": "architectural, contemporary, modern",
                        "mood": "dynamic, professional, aspirational"
                    },
                    {
                        "action": "g…ônc pe≈ü…ôkar qadƒ±n m…ôhsulu g√∂st…ôrir",
                        "pose": "d√ºz dayanmƒ±≈ü, m…ôhsulu yuxarƒ± tutub, g√ºcl√º baxƒ±≈ü",
                        "context": "m√ºasir ofis m√ºhiti, ≈ü√º≈ü…ô divarlar, m√ºasir mebel",
                        "background": "blurred office environment, glass walls, modern furniture, city view through windows",
                        "lighting": "bright natural daylight, soft window light, professional office lighting",
                        "style": "corporate, professional, business",
                        "mood": "confident, powerful, successful"
                    },
                    {
                        "action": "model m…ôhsulu kafe m√ºhitind…ô istifad…ô edir",
                        "pose": "oturmu≈ü poz, m…ôhsulu masada g√∂st…ôrir, rahat",
                        "context": "trendy urban cafe, wooden tables, plants",
                        "background": "cozy cafe interior, blurred background, warm atmosphere, coffee cups, plants",
                        "lighting": "warm ambient lighting, soft natural light from windows, cozy atmosphere",
                        "style": "lifestyle, casual, warm",
                        "mood": "relaxed, comfortable, everyday luxury"
                    },
                    {
                        "action": "pe≈ü…ôkar model m…ôhsulu a√ßƒ±q havada n√ºmayi≈ü etdirir",
                        "pose": "g…ôzinti pozu, m…ôhsulu t…ôbii ≈ü…ôkild…ô tutub, g√ºl√ºms…ôm…ô",
                        "context": "park v…ô ya baƒü m√ºhiti, aƒüaclar, √ßi√ß…ôkl…ôr",
                        "background": "blurred park scenery, trees, flowers, green nature, soft bokeh",
                        "lighting": "soft natural daylight, dappled sunlight through trees, fresh outdoor lighting",
                        "style": "natural, outdoor, fresh",
                        "mood": "fresh, natural, vibrant"
                    },
                    {
                        "action": "model m…ôhsulu l√ºks interyerd…ô g√∂st…ôrir",
                        "pose": "z…ôrif poz, m…ôhsulu d…ôbd…ôb…ôli m√ºhitd…ô n√ºmayi≈ü etdirir",
                        "context": "l√ºks ev interyeri, d…ôbd…ôb…ôli mebel, inc…ôs…ôn…ôt …ôs…ôrl…ôri",
                        "background": "luxury home interior, elegant furniture, art pieces, marble surfaces, gold accents",
                        "lighting": "dramatic interior lighting, warm golden tones, elegant shadows",
                        "style": "luxury, opulent, high-end",
                        "mood": "luxurious, elegant, premium"
                    },
                    {
                        "action": "g…ônc model m…ôhsulu ≈ü…ôh…ôr panoramasƒ± il…ô g√∂st…ôrir",
                        "pose": "balkon v…ô ya terrasda, m…ôhsulu ≈ü…ôh…ôr…ô doƒüru tutub",
                        "context": "y√ºks…ôk bina balkonu, ≈ü…ôh…ôr panoramasƒ±",
                        "background": "city skyline panorama, tall buildings, urban landscape, distant view",
                        "lighting": "bright daylight, clear sky, urban atmosphere, professional",
                        "style": "panoramic, urban, expansive",
                        "mood": "aspirational, grand, impressive"
                    },
                    {
                        "action": "model m…ôhsulu qaranlƒ±q studiyada n√ºmayi≈ü etdirir",
                        "pose": "dramatik poz, kontrast i≈üƒ±qlandƒ±rma, g√ºcl√º siluet",
                        "context": "qaranlƒ±q studiya, minimal dekor",
                        "background": "dark studio background, dramatic shadows, minimal setup, professional",
                        "lighting": "dramatic studio lighting, high contrast, rim lighting, moody atmosphere",
                        "style": "dramatic, high-contrast, artistic",
                        "mood": "dramatic, bold, artistic"
                    },
                    {
                        "action": "model m…ôhsulu d…ôniz k…ônarƒ±nda g√∂st…ôrir",
                        "pose": "rahat poz, d…ôniz…ô baxƒ±r, m…ôhsulu t…ôbii ≈ü…ôkild…ô tutub",
                        "context": "d…ôniz k…ônarƒ±, qum, dalƒüalar",
                        "background": "ocean beach, waves, sand, blue sky, coastal scenery, blurred",
                        "lighting": "bright coastal daylight, blue sky reflection, fresh ocean atmosphere",
                        "style": "coastal, fresh, vacation",
                        "mood": "relaxed, vacation, premium lifestyle"
                    }
                ]
                
                # H…ôr post √º√ß√ºn f…ôrqli scene se√ß (idx-1 istifad…ô edirik √ß√ºnki idx 1-d…ôn ba≈ülayƒ±r)
                scene = scenes[(idx - 1) % len(scenes)]
                
                logger.info(f"   Scene {idx}: {scene['style']} - {scene['mood']} - {scene['context']}")
                
                # H∆èR POST √ú√á√úN F∆èRQLƒ∞ STƒ∞L V∆è MOOD (10 m√ºxt…ôlif variant)
                styles = [
                    "modern, premium, luxury advertising",
                    "contemporary, sleek, high-end commercial",
                    "sophisticated, elegant, professional",
                    "dynamic, energetic, urban",
                    "minimalist, clean, refined",
                    "artistic, creative, unique",
                    "corporate, professional, business",
                    "lifestyle, casual, approachable",
                    "dramatic, bold, impactful",
                    "natural, fresh, authentic"
                ]
                
                lighting_styles = [
                    "studio lighting, professional photography, high-end commercial, golden hour, soft shadows",
                    "natural daylight, soft window light, professional office lighting, bright and clear",
                    "dramatic studio lighting, high contrast, rim lighting, moody atmosphere",
                    "warm ambient lighting, cozy atmosphere, soft natural light",
                    "bright coastal daylight, blue sky reflection, fresh atmosphere",
                    "qƒ±zƒ±l saat i≈üƒ±qlandƒ±rmasƒ±, isti tonlar, hal…ô effekti",
                    "pe≈ü…ôkar studiya i≈üƒ±ƒüƒ±, yum≈üaq …ôsas i≈üƒ±q, dramatik hal…ô",
                    "t…ôbii g√ºn i≈üƒ±ƒüƒ±, kinematik r…ông qreydini",
                    "dramatik interyer i≈üƒ±ƒüƒ±, isti qƒ±zƒ±l tonlar",
                    "parlaq g√ºn i≈üƒ±ƒüƒ±, aydƒ±n g√∂y, t…ôz…ô atmosfer"
                ]
                
                # MODERN Dƒ∞ZAYN ELEMENTL∆èRƒ∞ - Aurora-like effects, glowing lights, gradients
                modern_design_elements = [
                    {
                        "background": "vibrant teal to emerald green gradient background, aurora-like glowing arc effect in upper half, abstract glowing light streaks, dynamic energy",
                        "effects": "glowing aurora borealis effect, bright horizontal light line creating horizon, glossy reflective surface, modern luxury aesthetic",
                        "colors": "deep teal transitioning to bright emerald green, white glowing accents, metallic reflections"
                    },
                    {
                        "background": "purple to pink gradient background, neon light streaks, cyberpunk-inspired glowing effects, futuristic atmosphere",
                        "effects": "neon glow effects, electric light streaks, holographic reflections, modern tech aesthetic",
                        "colors": "deep purple to vibrant pink gradient, cyan neon accents, metallic silver highlights"
                    },
                    {
                        "background": "blue to cyan gradient background, flowing light waves, ethereal glow effects, dynamic movement",
                        "effects": "flowing light waves, soft glow halos, water-like reflections, serene modern aesthetic",
                        "colors": "deep blue to bright cyan gradient, white light accents, glass-like transparency"
                    },
                    {
                        "background": "orange to red gradient background, fire-like glowing effects, warm energy waves, passionate atmosphere",
                        "effects": "fire-like glowing streaks, warm energy waves, dramatic light flares, energetic modern aesthetic",
                        "colors": "deep orange to vibrant red gradient, golden light accents, warm reflections"
                    },
                    {
                        "background": "gold to amber gradient background, luxury light rays, premium glow effects, opulent atmosphere",
                        "effects": "luxury light rays, golden glow halos, premium reflections, high-end aesthetic",
                        "colors": "rich gold to warm amber gradient, white luxury accents, metallic gold highlights"
                    },
                    {
                        "background": "indigo to violet gradient background, cosmic light effects, star-like glows, mystical atmosphere",
                        "effects": "cosmic light streaks, star-like sparkles, nebula-like glow, mystical modern aesthetic",
                        "colors": "deep indigo to vibrant violet gradient, silver star accents, cosmic blue highlights"
                    },
                    {
                        "background": "turquoise to mint green gradient background, fresh light waves, natural glow effects, refreshing atmosphere",
                        "effects": "fresh light waves, natural glow halos, water-like transparency, clean modern aesthetic",
                        "colors": "bright turquoise to mint green gradient, white fresh accents, crystal-like clarity"
                    },
                    {
                        "background": "coral to peach gradient background, warm sunset glow, soft light rays, cozy atmosphere",
                        "effects": "warm sunset glow, soft light rays, gentle reflections, inviting modern aesthetic",
                        "colors": "vibrant coral to soft peach gradient, golden light accents, warm highlights"
                    },
                    {
                        "background": "navy to royal blue gradient background, electric light bolts, dynamic energy, powerful atmosphere",
                        "effects": "electric light bolts, dynamic energy waves, powerful glow effects, strong modern aesthetic",
                        "colors": "deep navy to royal blue gradient, electric blue accents, metallic highlights"
                    },
                    {
                        "background": "magenta to fuchsia gradient background, vibrant neon glow, electric atmosphere, bold modern aesthetic",
                        "effects": "vibrant neon glow, electric light streaks, bold reflections, striking modern aesthetic",
                        "colors": "rich magenta to fuchsia gradient, bright pink accents, neon highlights"
                    }
                ]
                
                selected_style = styles[(idx - 1) % len(styles)]
                selected_lighting = lighting_styles[(idx - 1) % len(lighting_styles)]
                design_element = modern_design_elements[(idx - 1) % len(modern_design_elements)]
                
                logger.info(f"   Style: {selected_style}")
                logger.info(f"   Lighting: {selected_lighting[:80]}...")
                logger.info(f"   Design Element: {design_element['background'][:60]}...")
                
                # Build UNIQUE TECHNICAL PROMPT for each post (AZERBAIJANI)
                # H∆èR POST √ú√á√úN TAMAMƒ∞L∆è F∆èRQLƒ∞ PROMPT + MODERN Dƒ∞ZAYN ELEMENTL∆èRƒ∞
                image_generation_prompt = f"""Professional product advertising photography, ultra realistic, 8K quality, modern luxury aesthetic

[SUBYEKTƒ∞ ƒ∞STƒ∞NAD]: Pe≈ü…ôkar moda/h…ôyat t…ôrzi modeli (ya≈ü 25-35, c…ôlbedici, baxƒ±mlƒ±) {product_name_type} m…ôhsulunu t…ôqdim edir. M…ôhsul detallarƒ±: {visual_description}. R…ôngl…ôr: {', '.join(primary_colors) if primary_colors else 'original'} …ôsas r…ôngl…ôrl…ô {', '.join(secondary_colors) if secondary_colors else 'original'} aksent r…ôngl…ôri. Materiallar: {', '.join(materials) if materials else 'original'}. Tekstura: {texture if texture else 'original'}. √úzl√ºk: {finish if finish else 'original'}.

[H∆èR∆èK∆èT/POZ]: {scene['action']}, {scene['pose']}

[GEYƒ∞M/KONTEKST]: {scene['context']}. Model m…ôhsulu tamamlayan, lakin onunla r…ôqab…ôt aparmayan geyim geyinir. Pe≈ü…ôkar stilizasiya, {scene['mood']} atmosfer.

[MODERN FON Dƒ∞ZAYNI]: {design_element['background']}. {design_element['effects']}. {design_element['colors']}. Pe≈ü…ôkar kommersiya fotoqrafiyasƒ± √ß…ôkili≈ü, {scene['style']} dizayn, modern luxury aesthetic.

[ƒ∞≈ûIQLANDIRMA/STƒ∞L]: {scene['lighting']}. {design_element['effects']}. Kinematik 4K keyfiyy…ôt. Y√ºks…ôk s…ôviyy…ôli kommersiya fotoqrafiyasƒ±. Pe≈ü…ôkar r…ông qreydini. M…ôhsul v…ô modeld…ô k…ôskin fokus. {scene['mood']} …ôhval-ruhiyy…ô, {scene['style']} estetikasƒ±, modern dynamic energy.

[TEXNƒ∞Kƒ∞ SPESIFIKASIYALAR]: Aspekt nisb…ôti: 16:9, Rezolyusiya: 4K UHD, Stil: {selected_style}, Keyfiyy…ôt: Ultra y√ºks…ôk keyfiyy…ôt, pe≈ü…ôkar retouching

[KRƒ∞Tƒ∞K T∆èLIMATLAR]:
- M…ôhsul orijinal ≈ü…ôkild…ô g√∂st…ôrildiyi kimi TAM OLARAQ qalmalƒ±dƒ±r
- M…ôhsulu d…ôyi≈üdirm…ôyin, modifikasiya etm…ôyin v…ô ya …ôv…ôz etm…ôyin
- B√ºt√ºn m…ôhsul detallarƒ±nƒ±, r…ôngl…ôri, materiallarƒ± v…ô dizaynƒ± eyni saxlayƒ±n
- Yalnƒ±z fonu, …ôtraf m√ºhiti d…ôyi≈üdirin v…ô model …ôlav…ô edin
- Fokus: H…ôm m…ôhsulda, h…ôm d…ô modeld…ô k…ôskin
- ∆èhval-ruhiyy…ô: {scene['mood']}, y√ºks…ôk konversiyalƒ± reklam, modern luxury
- Kompozisiya: √ú√ßd…ô bir qaydasƒ±, pe≈ü…ôkar kommersiya t…ôrtibatƒ±, {scene['style']} kompozisiya

Lighting: {selected_lighting}, {design_element['effects']}
Quality: ultra realistic, 8K, professional photography, sharp focus, detailed
Style: {selected_style}, {scene['mood']} atmosphere, {scene['style']} composition, modern luxury aesthetic
Mood: {scene['mood']}, {selected_style}, dynamic energy, premium luxury
Background: {design_element['background']}, {design_element['effects']}, {design_element['colors']}
Effects: glowing aurora effects, abstract light streaks, gradient transitions, modern luxury aesthetic, dynamic energy"""
                
                # Step 5: Generate image using Nano Banana with the created prompt
                generated_image_url = None
                if FAL_AI_AVAILABLE:
                    try:
                        logger.info(f"üé® Step 5: Nano Banana il…ô ≈ü…ôkil yaradƒ±lƒ±r {idx}/{num_images}...")
                        logger.info(f"   Input image: {background_removed_url}")
                        logger.info(f"   Prompt: {image_generation_prompt[:200]}...")
                        
                        # Use image-to-image with lower strength to preserve product
                        nano_result = fal_service.image_to_image(
                            image_url=background_removed_url,
                            prompt=image_generation_prompt,
                            strength=0.6  # Lower strength to preserve product details
                        )
                        
                        # Download and save the generated image
                        if nano_result and nano_result.get('image_url'):
                            logger.info(f"‚úÖ Nano Banana ≈ü…ôkil yaradƒ±ldƒ±: {nano_result['image_url']}")
                            
                            # Download and save to storage with unique name per post
                            scene_name = scene['style'].replace(' ', '_').replace(',', '').lower()[:30]
                            prefix = f"product_ad_{scene_name}_post{idx}"
                            saved_url = fal_service.download_and_save(
                                nano_result['image_url'],
                                user.id,
                                prefix=prefix
                            )
                            
                            # Convert to full URL if needed (saved_url is like /media/...)
                            if saved_url and not saved_url.startswith('http'):
                                from django.conf import settings
                                base_url = request.build_absolute_uri('/').rstrip('/')
                                # saved_url already includes /media/, so just prepend base_url
                                if saved_url.startswith('/'):
                                    generated_image_url = f"{base_url}{saved_url}"
                                else:
                                    generated_image_url = f"{base_url}/{saved_url}"
                            else:
                                generated_image_url = saved_url
                            
                            logger.info(f"‚úÖ ≈û…ôkil saxlanƒ±ldƒ±: {generated_image_url}")
                        else:
                            logger.warning("‚ö†Ô∏è Nano Banana n…ôtic…ô qaytarmadƒ±")
                            
                    except Exception as nano_error:
                        logger.error(f"‚ùå Nano Banana x…ôtasƒ±: {str(nano_error)}", exc_info=True)
                        logger.info("   Prompt saxlanacaq, ≈ü…ôkil manual yaradƒ±la bil…ôr")
                else:
                    logger.info("   Fal.ai m√∂vcud deyil, yalnƒ±z prompt yaradƒ±ldƒ±")
                
                # Create post WITH generated image
                from posts.models import Post
                
                # Extract structured content
                hook = post_data.get('hook', '')
                body = post_data.get('body', '')
                cta = post_data.get('cta', '')
                full_caption = post_data.get('full_caption', f"{hook}\n\n{body}\n\n{cta}")
                hashtags = post_data.get('hashtags', [])
                
                # Format hashtags properly
                hashtags_str = ' '.join(hashtags) if hashtags else ''
                
                # Combine full content
                complete_content = f"{full_caption}\n\n{hashtags_str}".strip()
                
                post = Post.objects.create(
                    user=user,
                    title=hook if hook else f"{product_name_type} - Ad {idx}",
                    content=complete_content,
                    hashtags=hashtags,
                    description=body[:200] if body else '',
                    image_url=generated_image_url or '',  # Nano Banana generated image or empty
                    ai_generated=True,
                    ai_prompt=image_generation_prompt,
                    status='pending_approval',
                    requires_approval=True
                )
                
                created_posts.append({
                    "id": str(post.id),
                    "hook": hook,
                    "body": body,
                    "cta": cta,
                    "full_caption": full_caption,
                    "hashtags": hashtags,
                    "complete_content": complete_content,
                    "image_url": generated_image_url,  # Nano Banana generated image
                    "image_generation_prompt": image_generation_prompt,  # Include prompt in response
                    "status": post.status,
                    "design_context": post_data.get('design_context', '')
                })
                
                if generated_image_url:
                    logger.info(f"‚úÖ Post {idx} yaradƒ±ldƒ± (Nano Banana ≈ü…ôkil il…ô): {post.id}")
                else:
                    logger.info(f"‚úÖ Post {idx} yaradƒ±ldƒ± (prompt il…ô, ≈ü…ôkil yoxdur): {post.id}")
                
            except Exception as e:
                logger.error(f"‚ùå Failed to create post {idx}: {str(e)}", exc_info=True)
                continue
        
        if not created_posts:
            return Response({
                "error": "He√ß bir post yaradƒ±la bilm…ôdi"
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        
        logger.info(f"‚úÖ Successfully created {len(created_posts)} product posts with complete workflow")
        
        # Prepare structured response (AZERBAIJANI)
        return Response({
            "success": True,
            "message": "D√∂rd addƒ±mlƒ± marketinq i≈ü axƒ±nƒ± uƒüurla tamamlandƒ±",
            "workflow_summary": {
                "step_1": "Arxa fon silm…ô tamamlandƒ±",
                "step_2": "Strukturla≈üdƒ±rƒ±lmƒ±≈ü m…ôhsul analizi tamamlandƒ±",
                "step_3": "Y√ºks…ôk konversiyalƒ± reklam m…ôzmunu yaradƒ±ldƒ±",
                "step_4": "Texniki AI promptlarƒ± yaradƒ±ldƒ±",
                "step_5": "Nano Banana il…ô professional ≈ü…ôkill…ôr yaradƒ±ldƒ±"
            },
            "posts": created_posts,
            "product_analysis": {
                "product_name_type": product_analysis.get('product_name_type', ''),
                "product_type": product_analysis.get('product_type', ''),
                "color_palette": product_analysis.get('color_palette', {}),
                "material_texture": product_analysis.get('material_texture', {}),
                "intended_use": product_analysis.get('intended_use', ''),
                "target_industry": product_analysis.get('target_industry', ''),
                "visual_analysis": product_analysis.get('visual_analysis', {}),
                "features": product_analysis.get('features', []),
                "benefits": product_analysis.get('benefits', []),
                "target_audience": product_analysis.get('target_audience', ''),
                "selling_points": product_analysis.get('selling_points', []),
                "lifestyle_context": product_analysis.get('lifestyle_context', '')
            },
            "images": {
                "original_image_url": original_image_url,
                "background_removed_image_url": background_removed_url
            },
            "num_created": len(created_posts)
        }, status=status.HTTP_201_CREATED)
        
    except ValueError as e:
        logger.error(f"‚ùå Validation error: {str(e)}")
        return Response({
            "error": str(e)
        }, status=status.HTTP_400_BAD_REQUEST)
    except Exception as e:
        logger.error(f"‚ùå Product post creation error: {str(e)}", exc_info=True)
        return Response({
            "error": f"M…ôhsul postu yaradƒ±la bilm…ôdi: {str(e)}"
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


@api_view(['POST'])
@permission_classes([permissions.IsAuthenticated])
def create_product_post_from_url(request):
    """
    YENƒ∞ FUNKSƒ∞YA: Sayt linkind…ôn m…ôhsul m…ôlumatlarƒ±nƒ± √ß…ôkib avtomatik post yaradƒ±r
    
    Workflow:
    1. URL-d…ôn s…ôhif…ô m…ôzmununu √ß…ôkir (web scraping)
    2. AI il…ô m…ôhsul m…ôlumatlarƒ±nƒ± analiz edir (ad, t…ôsvir, ≈ü…ôkil)
    3. M…ôhsul ≈ü…ôklini y√ºkl…ôyir
    4. create_product_post il…ô eyni 5 addƒ±mlƒ± prosesi t…ôtbiq edir
    
    Request body:
    {
        "product_url": "https://example.com/product/123",
        "num_images": 1  // optional, default 1
    }
    
    Response: create_product_post il…ô eyni format + source info
    """
    try:
        from .url_product_scraper import (
            scrape_product_page,
            extract_product_info_with_ai,
            download_image_from_url,
            validate_product_data
        )
        
        logger.info("=" * 80)
        logger.info("üîó YENƒ∞ FUNKSƒ∞YA: URL-d…ôn M…ôhsul Postu Yaradƒ±lƒ±r")
        logger.info("=" * 80)
        
        # Get parameters
        product_url = request.data.get('product_url')
        num_images = int(request.data.get('num_images', 1))  # Default 1
        
        if not product_url:
            return Response({
                "error": "M…ôhsul URL-i t…ôl…ôb olunur (product_url)"
            }, status=status.HTTP_400_BAD_REQUEST)
        
        # Validate URL format
        if not product_url.startswith('http'):
            return Response({
                "error": "Yanlƒ±≈ü URL formatƒ±. URL http:// v…ô ya https:// il…ô ba≈ülamalƒ±dƒ±r"
            }, status=status.HTTP_400_BAD_REQUEST)
        
        logger.info(f"üìå M…ôhsul URL: {product_url}")
        logger.info(f"üìå Yaradƒ±lacaq post sayƒ±: {num_images}")
        
        # Step 1: Try Apify scraping first
        logger.info(f"üõí Step 1: Apify il…ô m…ôhsul m…ôlumatlarƒ± √ß…ôkilir...")
        
        extracted_data = None
        apify_data = scrape_product_with_apify(product_url)
        
        if apify_data:
            logger.info(f"‚úÖ Apify-d…ôn m…ôhsul m…ôlumatlarƒ± alƒ±ndƒ±")
            logger.info(f"   Ad: {apify_data.get('name', 'N/A')}")
            logger.info(f"   Brend: {apify_data.get('brand', 'N/A')}")
            logger.info(f"   Qiym…ôt: {apify_data.get('price', 'N/A')} {apify_data.get('currency', '')}")
            
            # Convert Apify data format to expected format
            extracted_data = {
                'product_name': apify_data.get('name', ''),
                'product_type': apify_data.get('brand', ''),
                'price': f"{apify_data.get('price', '')} {apify_data.get('currency', '')}".strip(),
                'main_image_url': apify_data.get('image', ''),
                'description': apify_data.get('description', ''),
                'url': apify_data.get('url', product_url),
                'brand': apify_data.get('brand', ''),
                'availability': apify_data.get('availability', ''),
                'raw_apify_data': apify_data  # Keep raw data for reference
            }
            
            logger.info(f"‚úÖ M…ôhsul m…ôlumatlarƒ± hazƒ±rlandƒ± (Apify-d…ôn)")
        else:
            logger.warning(f"‚ö†Ô∏è Apify scraping uƒüursuz, k√∂hn…ô metodu istifad…ô edirik...")
            
            # Step 2: Fallback to old method - Web Scraping
            logger.info(f"üåê Step 2: Sayt m…ôzmunu √ß…ôkilir...")
        
        try:
            scrape_result = scrape_product_page(product_url)
            html_content = scrape_result['html']
            final_url = scrape_result['final_url']
            
            logger.info(f"‚úÖ Sayt m…ôzmunu √ß…ôkildi ({len(html_content)} bytes)")
            if final_url != product_url:
                logger.info(f"   Redirect: {final_url}")
            
        except Exception as scraping_error:
            logger.error(f"‚ùå Web scraping x…ôtasƒ±: {str(scraping_error)}")
            return Response({
                "error": f"Sayt a√ßƒ±la bilm…ôdi: {str(scraping_error)}"
            }, status=status.HTTP_400_BAD_REQUEST)
        
            # Step 3: AI Analysis (fallback)
            logger.info(f"ü§ñ Step 3: AI il…ô m…ôhsul m…ôlumatlarƒ± √ßƒ±xarƒ±lƒ±r...")
        
        # try:
        #     extracted_data = extract_product_info_with_ai(html_content, final_url)
            
        #     logger.info(f"‚úÖ M…ôhsul m…ôlumatlarƒ± √ßƒ±xarƒ±ldƒ±:")
        #     logger.info(f"   Ad: {extracted_data.get('product_name', 'N/A')}")
        #     logger.info(f"   N√∂v: {extracted_data.get('product_type', 'N/A')}")
        #     logger.info(f"   Qiym…ôt: {extracted_data.get('price', 'N/A')}")
            
        #     # Safely log image URL
        #     img_url = extracted_data.get('main_image_url')
        #     if img_url:
        #         logger.info(f"   ≈û…ôkil: {img_url[:80]}...")
        #     else:
        #         logger.warning("   ‚ö†Ô∏è ≈û…ôkil URL-i tapƒ±lmadƒ±!")
        #         logger.info(f"   √áƒ±xarƒ±lan m…ôlumatlar: {json.dumps(extracted_data, ensure_ascii=False, indent=2)}")
            
        # except Exception as ai_error:
        #     logger.error(f"‚ùå AI analiz x…ôtasƒ±: {str(ai_error)}", exc_info=True)
        #     return Response({
        #         "error": f"M…ôhsul m…ôlumatlarƒ± √ßƒ±xarƒ±la bilm…ôdi: {str(ai_error)}. ≈û…ôkil y√ºkl…ôm…ô metodunu istifad…ô edin."
        #     }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        
        # # H…ôl…ôlik post yaratmadan, yalnƒ±z m…ôlumatlarƒ± qaytarƒ±rƒ±q
        # logger.info(f"üìä M…ôhsul m…ôlumatlarƒ± hazƒ±rdƒ±r, qaytarƒ±lƒ±r...")
        
        #     return Response({
        #     "success": True,
        #     "product_data": extracted_data,
        #     "message": "M…ôhsul m…ôlumatlarƒ± uƒüurla √ß…ôkildi"
        # }, status=status.HTTP_200_OK)
        
        # TODO: Post yaratma kodu - sonra aktivl…ô≈üdiril…ôc…ôk
        # COMMENTED OUT - H…ôl…ôlik post yaratmƒ±rƒ±q, yalnƒ±z m…ôlumatlarƒ± qaytarƒ±rƒ±q
        # Step 3 v…ô sonrasƒ±: Post yaratma workflow-u (sonra aktivl…ô≈üdiril…ôc…ôk)
        
    except Exception as e:
        logger.error(f"‚ùå URL-d…ôn post yaratma x…ôtasƒ±: {str(e)}", exc_info=True)
        return Response({
            "error": f"URL-d…ôn post yaradƒ±la bilm…ôdi: {str(e)}"
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


@api_view(['POST'])
@permission_classes([permissions.IsAuthenticated])
def analyze_instagram_profile(request):
    """
    Instagram profil analizi - Metrics ‚Üí Rules ‚Üí GPT architecture
    
    Architecture:
    1. METRICS: Compute all metrics from raw data (deterministic)
    2. RULES: Apply business rules to trigger recommendations (deterministic)
    3. GPT: Generate bio suggestions and explain triggered rules (creative, but constrained)
    
    Input:
    {
        "instagram_username": "@username",
        "current_bio": "Hazƒ±rkƒ± bio m…ôtn",
        "followers_count": 1500,
        "following_count": 800,
        "posts_count": 120,
        "posting_frequency": "3-4",
        "niche": "Fashion/Tech/Food/..."
    }
    
    Output:
    {
        "profile_info": {...metrics...},
        "bio_suggestions": [...],
        "hashtag_strategy": {...},  # Removed AI guessing
        "content_strategy": {...},  # From rules
        "posting_schedule": {...},  # From rules (fixed times)
        "engagement_tips": [...],
        "growth_strategy": {...},
        "triggered_rules": [...]  # New: show which rules triggered
    }
    """
    try:
        from .metrics.instagram import InstagramMetrics
        from .rules.instagram import InstagramRuleEngine
        
        logger.info("=" * 80)
        logger.info("üì± Instagram Profil Analizi Ba≈ülayƒ±r (Metrics ‚Üí Rules ‚Üí GPT)")
        logger.info("=" * 80)
        
        # Parse input data
        logger.info(f"üì• Request data: {json.dumps(request.data, indent=2)}")
        
        username = request.data.get('instagram_username', '').strip().lstrip('@')
        current_bio = request.data.get('current_bio', '').strip()
        followers = request.data.get('followers_count', 0)
        following = request.data.get('following_count', 0)
        posts = request.data.get('posts_count', 0)
        posting_frequency = request.data.get('posting_frequency', '').strip()
        niche = request.data.get('niche', '').strip()
        
        # Parse to int safely
        try:
            followers = int(followers) if followers else 0
            following = int(following) if following else 0
            posts = int(posts) if posts else 0
        except (ValueError, TypeError):
            pass
        
        logger.info(f"üìä Parsed values - followers: {followers}, following: {following}, posts: {posts}")
        
        if not username:
            return Response({
                "error": "Instagram username t…ôl…ôb olunur"
            }, status=status.HTTP_400_BAD_REQUEST)
        
        # STEP 0: SCRAPE POSTS FOR ANALYSIS (optional)
        post_analysis = None
        try:
            logger.info("üì∏ Step 0: Scraping posts for timestamp analysis...")
            profile_url = f"https://www.instagram.com/{username}/"
            scraped_data = scrape_instagram_with_apify(profile_url)
            
            if scraped_data and scraped_data.get('posts'):
                posts_data = scraped_data.get('posts', [])
                logger.info(f"‚úÖ {len(posts_data)} post scraped for analysis")
                post_analysis = InstagramMetrics.analyze_post_timestamps(posts_data)
                logger.info(f"üìä Post analysis: {post_analysis.get('optimal_posting_times', [])}")
            else:
                logger.warning("‚ö†Ô∏è Post scraping failed or no posts found")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Post scraping error: {str(e)}")
        
        # STEP 1: COMPUTE METRICS (deterministic)
        logger.info("üìä Step 1: Computing metrics...")
        metrics = InstagramMetrics.compute_all_metrics(
            username=username,
            followers=followers,
            following=following,
            posts=posts,
            posting_frequency=posting_frequency,
            niche=niche,
            current_bio=current_bio
        )
        logger.info(f"‚úÖ Metrics computed: engagement_rate={metrics['engagement_rate']}%, stage={metrics['account_stage_az']}")
        
        # STEP 2: APPLY RULES (deterministic)
        logger.info("‚öôÔ∏è Step 2: Applying business rules...")
        rule_engine = InstagramRuleEngine(metrics)
        triggered_rules = rule_engine.evaluate_all_rules()
        content_strategy = rule_engine.get_content_strategy()
        posting_schedule = rule_engine.get_posting_schedule()
        hashtag_strategy = rule_engine.get_hashtag_recommendations()
        
        logger.info(f"‚úÖ Rules applied: {len(triggered_rules)} rules triggered")
        for rule in triggered_rules:
            logger.info(f"   - [{rule.severity}] {rule.rule_id}: {rule.message}")
        
        # STEP 3: GPT for bio suggestions and rule explanations (creative, but constrained)
        logger.info("ü§ñ Step 3: GPT for bio suggestions and explanations...")
        
        client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
        
        # Prepare triggered rules summary for GPT
        rules_summary = "\n".join([
            f"- [{rule.severity.upper()}] {rule.category}: {rule.message} ‚Üí {rule.recommendation}"
            for rule in triggered_rules
        ])
        
        # Analyze current bio for context
        bio_analysis = ""
        if current_bio:
            bio_analysis = f"""
HAZIRKI BIO ANALƒ∞Zƒ∞:
"{current_bio}"

Bio analizi:
- Uzunluq: {len(current_bio)} simvol
- Emoji var: {'B…ôli' if any(ord(c) > 127000 for c in current_bio) else 'Xeyr'}
- CTA var: {'B…ôli' if any(word in current_bio.lower() for word in ['link', 'linkin', 'dm', 'yaz', '…ôlaq…ô', 'contact']) else 'Xeyr'}
- Niche m…ôlumatƒ±: {'Var' if niche and niche.lower() in current_bio.lower() else 'Yoxdur'}

Bio-nun g√ºcl√º t…ôr…ôfl…ôri: {', '.join([f'"{part.strip()}"' for part in current_bio.split('|')[:2] if part.strip()]) if '|' in current_bio else 'Strukturla≈üdƒ±rƒ±lmamƒ±≈ü'}
"""
        else:
            bio_analysis = "Hazƒ±rkƒ± bio bo≈üdur - yeni bio yaradƒ±lmalƒ±dƒ±r."

        gpt_prompt = f"""Instagram profil analizi - Bio t…ôklifl…ôri v…ô qaydalarƒ±n izahƒ± (Az…ôrbaycan dilind…ô).

CRITICAL RULES FOR GPT:
- DO NOT hallucinate data, metrics, or numbers
- DO NOT create fake hashtags or guess hashtag competition levels
- DO NOT invent posting times - use only provided times
- ONLY explain and elaborate on provided rules and recommendations
- Bio-lar MUTLAQ profil…ô spesifik olmalƒ±dƒ±r - generic bio-lar yaratma!

PROFIL METRƒ∞KL∆èRƒ∞ (REAL DATA):
- Username: @{username}
- Followers: {followers:,}
- Following: {following:,}
- Posts: {posts}
- Engagement rate: {metrics['engagement_rate']}%
- Hesab m…ôrh…ôl…ôsi: {metrics['account_stage_az']}
- Niche/Sah…ô: {niche if niche else '√úmumi'}
{bio_analysis}

TRƒ∞GGERED RULES (REAL BUSINESS LOGIC):
{rules_summary if rules_summary else "He√ß bir kritik problem tapƒ±lmadƒ±"}

TASKS FOR GPT:
1. BIO T∆èKLƒ∞FL∆èRƒ∞ (5 variant) - √áOX ∆èH∆èMƒ∞YY∆èTLƒ∞Dƒ∞R:
   - H…ôr bio MUTLAQ bu profili …ôks etdirm…ôlidir:
     * Username (@{username}) v…ô ya onun m…ônasƒ±nƒ± n…ôz…ôr…ô al
     * Niche ({niche if niche else '√úmumi'}) konkret ≈ü…ôkild…ô g√∂st…ôr
     * Hesab m…ôrh…ôl…ôsi ({metrics['account_stage_az']}) - starter √º√ß√ºn daha friendly, established √º√ß√ºn daha professional
     * Hazƒ±rkƒ± bio-nun g√ºcl√º t…ôr…ôfl…ôrini saxla v…ô z…ôif t…ôr…ôfl…ôrini d√ºz…ôlt
   
   - Bio struktur:
     * 1-ci s…ôtir: Value proposition (niche + unique selling point)
     * 2-ci s…ôtir: Call-to-action v…ô ya engagement elementi
     * 3-c√º s…ôtir: Link v…ô ya contact info (…ôg…ôr varsa)
   
   - Emoji strategiyasƒ±:
     * Niche-…ô uyƒüun emojil…ôr ({niche if niche else 'generic'} √º√ß√ºn)
     * √áox emoji yazma (2-4 emoji kifay…ôtdir)
     * Emoji-l…ôr m…ôtnin m…ônasƒ±nƒ± g√ºcl…ôndirsin
   
   - Call-to-action:
     * Hesab m…ôrh…ôl…ôsin…ô g√∂r…ô: starter √º√ß√ºn "Follow for more", established √º√ß√ºn "DM for collab"
     * Niche-spesifik CTA: {niche if niche else 'generic niche'} √º√ß√ºn uyƒüun CTA
   
   - Uzunluq: 150 simvoldan az (optimal: 100-130 simvol)
   
   - H…ôr bio variantƒ± F∆èRQLƒ∞ olmalƒ±dƒ±r:
     * Variant 1: Professional/formal ton
     * Variant 2: Friendly/casual ton
     * Variant 3: Creative/artistic ton
     * Variant 4: Minimalist/clean ton
     * Variant 5: Bold/attention-grabbing ton
   
   - Explanation h…ôr bio √º√ß√ºn:
     * Niy…ô bu bio bu profil √º√ß√ºn uyƒüundur
     * Hansƒ± elementl…ôr profil…ô spesifikdir
     * Niy…ô bu ton v…ô struktur se√ßilib

2. ENGAGEMENT Tƒ∞PL∆èRƒ∞ (10 konkret tip):
   - Triggered rules-…ô …ôsaslanaraq konkret addƒ±mlar
   - Praktik, t…ôtbiq oluna bil…ôn m…ôsl…ôh…ôtl…ôr
   - Niche-spesifik t√∂vsiy…ôl…ôr

3. GROWTH STRATEGƒ∞YASI:
   - 30 g√ºnl√ºk plan (h…ôft…ôlik breakdown)
   - Real h…ôd…ôfl…ôr (metrics-…ô …ôsas…ôn)
   - √ñl√ß√ºl…ô bil…ôn n…ôtic…ôl…ôr

4. √úMUMƒ∞ Qƒ∞YM∆èTL∆èNDƒ∞RM∆è:
   - G√ºcl√º t…ôr…ôfl…ôr (metrics-d…ôn)
   - Z…ôif t…ôr…ôfl…ôr (triggered rules-d…ôn)
   - ƒ∞mkanlar
   - Prioritet addƒ±mlar

JSON formatda qaytarƒ±n:
{{
    "bio_suggestions": [
        {{
            "bio": "Tam bio m…ôtn (100-130 simvol, profil…ô spesifik)",
            "explanation": "DETALLI izah: Niy…ô bu bio bu profil (@{username}, {niche if niche else 'niche'}, {metrics['account_stage_az']}) √º√ß√ºn uyƒüundur. Hansƒ± elementl…ôr profil…ô spesifikdir. Niy…ô bu ton se√ßilib. Hansƒ± CTA v…ô emoji strategiyasƒ± istifad…ô olunub."
        }},
        {{
            "bio": "F…ôrqli ton v…ô strukturda bio (yuxarƒ±dakƒ±ndan f…ôrqli)",
            "explanation": "DETALLI izah: Bu variantƒ±n f…ôrqi n…ôdir, niy…ô bu ton se√ßilib, hansƒ± elementl…ôr profil…ô spesifikdir."
        }},
        {{
            "bio": "√ú√ß√ºnc√º variant (f…ôrqli yana≈üma)",
            "explanation": "DETALLI izah..."
            }},
            {{
            "bio": "D√∂rd√ºnc√º variant (f…ôrqli yana≈üma)",
            "explanation": "DETALLI izah..."
            }},
            {{
            "bio": "Be≈üinci variant (f…ôrqli yana≈üma)",
            "explanation": "DETALLI izah..."
        }}
    ],
    "engagement_tips": [
        "Tip 1: konkret addƒ±m",
        "Tip 2: konkret addƒ±m",
        ...
    ],
    "growth_strategy": {{
        "30_day_plan": {{
            "week_1": "...",
            "week_2": "...",
            "week_3": "...",
            "week_4": "..."
        }},
        "realistic_goals": {{
            "followers_growth": "+X%",
            "engagement_target": "Y%"
        }},
        "metrics_to_track": ["metric1", "metric2", ...]
    }},
    "overall_assessment": {{
        "strengths": ["g√ºcl√º t…ôr…ôf 1", ...],
        "weaknesses": ["z…ôif t…ôr…ôf 1", ...],
        "opportunities": ["f√ºrs…ôt 1", ...],
        "priority_actions": ["√∂ncelikli addƒ±m 1", ...]
    }}
}}

B√ºt√ºn m…ôtnl…ôr Az…ôrbaycan dilind…ô olmalƒ±dƒ±r.
REMEMBER: DO NOT hallucinate data. Use only provided metrics and rules."""

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": f"""Siz pe≈ü…ôkar Social Media Marketing (SMM) m…ôsl…ôh…ôt√ßisisiniz v…ô Instagram bio copywriter-siniz.

Sƒ∞Zƒ∞N V∆èZƒ∞F∆èNƒ∞Z:
1. Bio yaradark…ôn MUTLAQ profil…ô spesifik olmalƒ±sƒ±nƒ±z - generic bio-lar yaratmayƒ±n!
2. Username (@{username}), niche ({niche if niche else '√úmumi'}), v…ô hesab m…ôrh…ôl…ôsi ({metrics['account_stage_az']}) …ôsasƒ±nda personal bio-lar yaradƒ±n
3. Hazƒ±rkƒ± bio-nu analiz edin v…ô onun g√ºcl√º t…ôr…ôfl…ôrini saxlayƒ±n, z…ôif t…ôr…ôfl…ôrini d√ºz…ôldin
4. H…ôr bio variantƒ± f…ôrqli ton v…ô strukturda olmalƒ±dƒ±r
5. HE√á VAXT m…ôlumat uydurmayƒ±n - yalnƒ±z verilmi≈ü faktlar …ôsasƒ±nda i≈ül…ôyin
6. B√ºt√ºn cavablar Az…ôrbaycan dilind…ô olmalƒ±dƒ±r

BIO YARADMA PRƒ∞NSƒ∞PL∆èRƒ∞:
- Profil…ô spesifik ol (username, niche, stage)
- Unique value proposition g√∂st…ôr
- Call-to-action …ôlav…ô et
- Emoji-l…ôri m…ôqs…ôdy√∂nl√º istifad…ô et
- 100-130 simvol optimal uzunluqdur"""
                },
                {
                    "role": "user",
                    "content": gpt_prompt
                }
            ],
            temperature=0.8,  # Higher creativity for personalized bios
            max_tokens=2500  # More tokens for detailed explanations
        )
        
        gpt_text = response.choices[0].message.content.strip()
        
        # Clean up JSON
        if gpt_text.startswith('```'):
            gpt_text = re.sub(r'^```json?\s*', '', gpt_text)
            gpt_text = re.sub(r'\s*```$', '', gpt_text)
        
        gpt_data = json.loads(gpt_text)
        
        logger.info(f"‚úÖ GPT response parsed successfully")
        logger.info(f"   Bio suggestions: {len(gpt_data.get('bio_suggestions', []))}")
        logger.info(f"   Engagement tips: {len(gpt_data.get('engagement_tips', []))}")
        
        # STEP 4: ASSEMBLE FINAL RESPONSE
        logger.info("üì¶ Step 4: Assembling final response...")
        
        return Response({
            "success": True,
            "profile_info": {
                "username": username,
                "followers": followers,
                "following": following,
                "posts": posts,
                "engagement_rate": metrics['engagement_rate'],
                "posting_frequency": metrics['posting_frequency_text'],
                "niche": niche,
                "account_stage": metrics['account_stage_az'],
                "following_ratio": metrics['following_ratio']
            },
            "bio_suggestions": gpt_data.get('bio_suggestions', []),
            "hashtag_strategy": hashtag_strategy,
            "content_strategy": content_strategy,
            "posting_schedule": posting_schedule,
            "post_analysis": post_analysis,  # Post timestamp analysis
            "engagement_tips": gpt_data.get('engagement_tips', []),
            "growth_strategy": gpt_data.get('growth_strategy', {}),
            "overall_assessment": gpt_data.get('overall_assessment', {}),
            "triggered_rules": [
                {
                    "rule_id": rule.rule_id,
                    "severity": rule.severity,
                    "category": rule.category,
                    "message": rule.message,
                    "recommendation": rule.recommendation
                }
                for rule in triggered_rules
            ],
            "generated_at": datetime.now().isoformat(),
            "architecture_version": "metrics_rules_gpt_v1"
        }, status=status.HTTP_200_OK)
        
    except json.JSONDecodeError as e:
        logger.error(f"‚ùå JSON parse x…ôtasƒ±: {str(e)}")
        logger.error(f"GPT cavabƒ±: {gpt_text[:500] if 'gpt_text' in locals() else 'N/A'}")
        return Response({
            "error": "GPT cavabƒ± parse edil…ô bilm…ôdi"
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    except Exception as e:
        logger.error(f"‚ùå Instagram analiz x…ôtasƒ±: {str(e)}", exc_info=True)
        return Response({
            "error": f"Analiz x…ôtasƒ±: {str(e)}"
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


@api_view(['POST'])
@permission_classes([permissions.IsAuthenticated])
def translate_text(request):
    """
    Translate text between languages using OpenAI
    
    Input:
    {
        "text": "Text to translate",
        "target_language": "az" | "en" | "ru" | "tr",
        "source_language": "auto" | "az" | "en" | "ru" | "tr"
    }
    
    Output:
    {
        "original_text": "...",
        "translated_text": "...",
        "source_language": "...",
        "target_language": "..."
    }
    """
    try:
        text = request.data.get('text', '').strip()
        target_language = request.data.get('target_language', 'az').strip()
        source_language = request.data.get('source_language', 'auto').strip()
        
        if not text:
            return Response({
                "error": "Text t…ôl…ôb olunur"
            }, status=status.HTTP_400_BAD_REQUEST)
        
        # Language mapping
        language_names = {
            'az': 'Az…ôrbaycan',
            'en': 'English',
            'ru': '–†—É—Å—Å–∫–∏–π',
            'tr': 'T√ºrk√ße'
        }
        
        target_lang_name = language_names.get(target_language, 'Az…ôrbaycan')
        source_lang_name = language_names.get(source_language, 'Auto-detect') if source_language != 'auto' else 'Auto-detect'
        
        logger.info(f"üåê Translation request: {source_lang_name} ‚Üí {target_lang_name}")
        
        client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
        
        translate_prompt = f"""Translate the following text to {target_lang_name}.

Source language: {source_lang_name}
Target language: {target_lang_name}

Text to translate:
{text}

Instructions:
- Translate accurately preserving the meaning
- Keep the same tone and style
- Preserve any emojis or special characters
- If the text is already in {target_lang_name}, return it as is
- Return ONLY the translated text, no explanations

Translated text:"""
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": f"You are a professional translator. Translate text accurately between {source_lang_name} and {target_lang_name}. Preserve meaning, tone, and style."
                },
                {
                    "role": "user",
                    "content": translate_prompt
                }
            ],
            temperature=0.3,  # Lower temperature for accurate translation
            max_tokens=1000
        )
        
        translated_text = response.choices[0].message.content.strip()
        
        # Clean up if there are any extra explanations
        if '\n' in translated_text:
            translated_text = translated_text.split('\n')[0]
        
        logger.info(f"‚úÖ Translation completed: {len(text)} ‚Üí {len(translated_text)} chars")
        
        return Response({
            "success": True,
            "original_text": text,
            "translated_text": translated_text,
            "source_language": source_language,
            "target_language": target_language,
            "source_language_name": source_lang_name,
            "target_language_name": target_lang_name
        }, status=status.HTTP_200_OK)
        
    except Exception as e:
        logger.error(f"‚ùå Translation x…ôtasƒ±: {str(e)}", exc_info=True)
        return Response({
            "error": f"Translation x…ôtasƒ±: {str(e)}"
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


@api_view(['POST'])
@permission_classes([permissions.IsAuthenticated])
def analyze_facebook_profile(request):
    """
    Facebook Page analizi v…ô SMM t√∂vsiy…ôl…ôri
    
    Input:
    {
        "page_name": "Page Name",
        "current_about": "Hazƒ±rkƒ± about m…ôtn",
        "followers_count": 5000,
        "likes_count": 4800,
        "posts_count": 200,
        "posting_frequency": "3-4",
        "niche": "Business/Education/..."
    }
    """
    try:
        logger.info("=" * 80)
        logger.info("üìò Facebook Page Analizi Ba≈ülayƒ±r")
        logger.info("=" * 80)
        
        # Get input data
        page_name = request.data.get('page_name', '').strip()
        current_about = request.data.get('current_about', '').strip()
        followers = int(request.data.get('followers_count', 0))
        likes = int(request.data.get('likes_count', 0))
        posts = int(request.data.get('posts_count', 0))
        posting_frequency = request.data.get('posting_frequency', '').strip()
        niche = request.data.get('niche', '').strip()
        
        if not page_name:
            return Response({
                "error": "Facebook Page adƒ± t…ôl…ôb olunur"
            }, status=status.HTTP_400_BAD_REQUEST)
        
        # Map posting frequency
        frequency_map = {
            '1-2': 'H…ôft…ôd…ô 1-2 d…ôf…ô',
            '3-4': 'H…ôft…ôd…ô 3-4 d…ôf…ô',
            '5-7': 'H…ôft…ôd…ô 5-7 d…ôf…ô',
            'daily': 'G√ºnd…ô 1 d…ôf…ô',
            '2plus': 'G√ºnd…ô 2+ d…ôf…ô'
        }
        posting_frequency_text = frequency_map.get(posting_frequency, posting_frequency or 'T…ôyin olunmayƒ±b')
        
        logger.info(f"üìä Page: {page_name}")
        logger.info(f"   Followers: {followers:,}")
        logger.info(f"   Posts: {posts}")
        logger.info(f"   Payla≈üƒ±m sƒ±xƒ±lƒ±ƒüƒ±: {posting_frequency_text}")
        logger.info(f"   Niche: {niche}")
        
        # Calculate metrics
        engagement_ratio = likes / followers if followers > 0 else 0
        
        # Determine page stage
        if followers < 1000:
            page_stage = "starter"
            page_stage_az = "Ba≈ülanƒüƒ±c"
        elif followers < 10000:
            page_stage = "growing"
            page_stage_az = "ƒ∞nki≈üaf m…ôrh…ôl…ôsi"
        elif followers < 100000:
            page_stage = "established"
            page_stage_az = "M√∂hk…ôm"
        else:
            page_stage = "popular"
            page_stage_az = "Populyar"
        
        logger.info(f"üéØ Page m…ôrh…ôl…ôsi: {page_stage_az}")
        
        # Call OpenAI
        logger.info(f"ü§ñ OpenAI analizi ba≈ülayƒ±r...")
        
        client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
        
        analysis_prompt = f"""Facebook Page analizi v…ô SMM t√∂vsiy…ôl…ôri (Az…ôrbaycan dilind…ô).

PAGE M∆èLUMATLARI:
- Page adƒ±: {page_name}
- Hazƒ±rkƒ± About: "{current_about if current_about else 'Bo≈ü'}"
- Followers: {followers:,}
- Likes: {likes:,}
- Posts: {posts}
- Payla≈üƒ±m sƒ±xƒ±lƒ±ƒüƒ±: {posting_frequency_text}
- Niche/Sah…ô: {niche if niche else '√úmumi'}
- Page m…ôrh…ôl…ôsi: {page_stage_az}
- Engagement ratio: {engagement_ratio:.2f}

FACEBOOK X√úSUSƒ∞YY∆èTL∆èRƒ∞:
- Facebook algoritmi engagement v…ô comments prioritiz…ô edir
- Video content daha √ßox reach alƒ±r
- Community engagement √ßox vacibdir
- Facebook Groups il…ô inteqrasiya t√∂vsiy…ô olunur
- Live videos y√ºks…ôk engagement verir

T∆èHLƒ∞L V∆è T√ñVSƒ∞Y∆èL∆èR HAZIRLYIN:

1. ABOUT/PAGE DESCRIPTION T∆èKLƒ∞FL∆èRƒ∞ (5 variant):
   - Aydƒ±n, informasiya verici
   - SEO-friendly keywords
   - Call-to-action daxil etsin
   - Contact information v…ô linkl…ôr

2. CONTENT STRATEGƒ∞YASI:
   - Content n√∂vl…ôri (faiz payƒ± il…ô): Video, Link posts, Photo, Text, Live
   - Post tezliyi t√∂vsiy…ôsi
   - Video content strategiyasƒ±
   - Facebook Groups strategiyasƒ±
   - Content pillars (3-5 …ôsas m√∂vzu)

3. POSTƒ∞NG SCHEDULE (REAL VAXT FORMATINDA):
   - ∆èn yax≈üƒ± post saatlarƒ± (h…ôft…ô g√ºnl…ôri + h…ôft…ô sonu) - REAL VAXT (m…ôs…ôl…ôn: "18:00", "13:00")
   - H…ôr zaman slot √º√ß√ºn effektivlik s…ôb…ôbi
   - Top 3 …ôn effektiv posting saatlarƒ± (real vaxt, effektivlik skoru, detallƒ± s…ôb…ôb)

4. ENGAGEMENT ARTIRILMASI:
   - Facebook-specific engagement tips (10 tip)
   - Comments strategiyasƒ±
   - Shares strategiyasƒ±
   - Facebook Groups v…ô Communities
   - Live video strategiyasƒ±

5. GROWTH STRATEGƒ∞YASI:
   - 30 g√ºnl√ºk plan
   - Realistic growth h…ôd…ôfl…ôri
   - Facebook Ads inteqrasiyasƒ±
   - Metrics track etm…ôk √º√ß√ºn

JSON formatda qaytarƒ±n (Instagram strukturuna ox≈üar, amma Facebook √º√ß√ºn uyƒüunla≈üdƒ±rƒ±lmƒ±≈ü):
{{
    "about_suggestions": [
        {{
            "about": "...",
            "explanation": "Niy…ô bu about i≈ül…ôy…ôc…ôk"
        }}
    ],
    "content_strategy": {{
        "content_mix": {{
            "video": 30,
            "link_posts": 20,
            "photo": 25,
            "text": 15,
            "live": 10
        }},
        "post_frequency": "h…ôft…ôd…ô X post",
        "video_frequency": "h…ôft…ôd…ô X video",
        "live_frequency": "ayda X live",
        "content_pillars": ["M√∂vzu 1", "M√∂vzu 2", ...]
    }},
    "posting_schedule": {{
        "weekdays": {{
            "morning": {{
                "time_range": "08:00-10:00",
                "best_time": "09:00",
                "effectiveness": "Detallƒ± izah"
            }},
            "afternoon": {{
                "time_range": "12:00-14:00",
                "best_time": "13:00",
                "effectiveness": "Detallƒ± izah"
            }},
            "evening": {{
                "time_range": "18:00-21:00",
                "best_time": "19:00",
                "effectiveness": "Detallƒ± izah"
            }},
            "best_time": "19:00",
            "best_time_reason": "Detallƒ± s…ôb…ôb"
        }},
        "weekend": {{
            "best_time": "11:00",
            "alternative_times": ["11:00", "20:00"],
            "best_time_reason": "Detallƒ± s…ôb…ôb"
        }},
        "top_3_best_times": [
            {{
                "time": "19:00",
                "day_type": "H…ôft…ô i√ßi",
                "effectiveness_score": "95%",
                "reason": "Detallƒ± s…ôb…ôb"
            }},
            ...
        ]
    }},
    "engagement_tips": ["Tip 1", "Tip 2", ...],
    "growth_strategy": {{
        "30_day_plan": {{
            "week_1": "...",
            "week_2": "...",
            "week_3": "...",
            "week_4": "..."
        }},
        "realistic_goals": {{
            "followers_growth": "+X%",
            "engagement_target": "Y%"
        }},
        "metrics_to_track": ["metric1", "metric2", ...]
    }},
    "overall_assessment": {{
        "strengths": ["g√ºcl√º t…ôr…ôf 1", ...],
        "weaknesses": ["z…ôif t…ôr…ôf 1", ...],
        "opportunities": ["f√ºrs…ôt 1", ...],
        "priority_actions": ["√∂ncelikli addƒ±m 1", ...]
    }}
}}

B√ºt√ºn m…ôtnl…ôr Az…ôrbaycan dilind…ô olmalƒ±dƒ±r."""

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "Siz pe≈ü…ôkar Social Media Marketing (SMM) m…ôsl…ôh…ôt√ßisisiniz. Facebook Page-l…ôri analiz edib konkret, t…ôtbiq oluna bil…ôn t√∂vsiy…ôl…ôr verirsiniz. B√ºt√ºn cavablar Az…ôrbaycan dilind…ô olmalƒ±dƒ±r."
                },
                {
                    "role": "user",
                    "content": analysis_prompt
                }
            ],
            temperature=0.7,
            max_tokens=3000
        )
        
        analysis_text = response.choices[0].message.content.strip()
        
        # Clean up JSON
        if analysis_text.startswith('```'):
            analysis_text = re.sub(r'^```json?\s*', '', analysis_text)
            analysis_text = re.sub(r'\s*```$', '', analysis_text)
        
        analysis_data = json.loads(analysis_text)
        
        logger.info(f"‚úÖ Analiz tamamlandƒ±")
        
        # Calculate estimated engagement
        estimated_engagement = 0
        if posts > 0 and followers > 0:
            base_engagement = {
                "starter": 4.0,
                "growing": 3.5,
                "established": 2.5,
                "popular": 2.0
            }.get(page_stage, 3.0)
            
            frequency_multiplier = {
                '1-2': 1.2,
                '3-4': 1.0,
                '5-7': 0.9,
                'daily': 0.8,
                '2plus': 0.7
            }.get(posting_frequency, 1.0)
            
            estimated_engagement = round(base_engagement * frequency_multiplier, 1)
        
        return Response({
            "success": True,
            "profile_info": {
                "page_name": page_name,
                "followers": followers,
                "likes": likes,
                "posts": posts,
                "engagement_rate": estimated_engagement,
                "posting_frequency": posting_frequency_text,
                "niche": niche,
                "page_stage": page_stage_az,
                "engagement_ratio": round(engagement_ratio, 2)
            },
            "analysis": analysis_data,
            "generated_at": datetime.now().isoformat()
        }, status=status.HTTP_200_OK)
        
    except json.JSONDecodeError as e:
        logger.error(f"‚ùå JSON parse x…ôtasƒ±: {str(e)}")
        return Response({
            "error": "AI cavabƒ± parse edil…ô bilm…ôdi"
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    except Exception as e:
        logger.error(f"‚ùå Facebook analiz x…ôtasƒ±: {str(e)}", exc_info=True)
        return Response({
            "error": f"Analiz zamanƒ± x…ôta ba≈ü verdi: {str(e)}"
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


@api_view(['POST'])
@permission_classes([permissions.IsAuthenticated])
def analyze_linkedin_profile(request):
    """
    LinkedIn Company Page v…ô ya Personal Profile analizi
    
    Input:
    {
        "profile_name": "Company/Personal Name",
        "current_headline": "Hazƒ±rkƒ± headline",
        "followers_count": 3000,
        "connections_count": 1500,
        "posts_count": 80,
        "posting_frequency": "3-4",
        "niche": "B2B/Technology/..."
    }
    """
    try:
        logger.info("=" * 80)
        logger.info("üíº LinkedIn Profil Analizi Ba≈ülayƒ±r")
        logger.info("=" * 80)
        
        # Get input data
        profile_name = request.data.get('profile_name', '').strip()
        current_headline = request.data.get('current_headline', '').strip()
        followers = int(request.data.get('followers_count', 0))
        connections = int(request.data.get('connections_count', 0))
        posts = int(request.data.get('posts_count', 0))
        posting_frequency = request.data.get('posting_frequency', '').strip()
        niche = request.data.get('niche', '').strip()
        
        if not profile_name:
            return Response({
                "error": "LinkedIn profil adƒ± t…ôl…ôb olunur"
            }, status=status.HTTP_400_BAD_REQUEST)
        
        # Map posting frequency
        frequency_map = {
            '1-2': 'H…ôft…ôd…ô 1-2 d…ôf…ô',
            '3-4': 'H…ôft…ôd…ô 3-4 d…ôf…ô',
            '5-7': 'H…ôft…ôd…ô 5-7 d…ôf…ô',
            'daily': 'G√ºnd…ô 1 d…ôf…ô',
            '2plus': 'G√ºnd…ô 2+ d…ôf…ô'
        }
        posting_frequency_text = frequency_map.get(posting_frequency, posting_frequency or 'T…ôyin olunmayƒ±b')
        
        logger.info(f"üìä Profil: {profile_name}")
        logger.info(f"   Followers: {followers:,}")
        logger.info(f"   Posts: {posts}")
        logger.info(f"   Payla≈üƒ±m sƒ±xƒ±lƒ±ƒüƒ±: {posting_frequency_text}")
        logger.info(f"   Niche: {niche}")
        
        # Calculate metrics
        connection_ratio = connections / followers if followers > 0 else 0
        
        # Determine profile stage
        if followers < 500:
            profile_stage = "starter"
            profile_stage_az = "Ba≈ülanƒüƒ±c"
        elif followers < 5000:
            profile_stage = "growing"
            profile_stage_az = "ƒ∞nki≈üaf m…ôrh…ôl…ôsi"
        elif followers < 50000:
            profile_stage = "established"
            profile_stage_az = "M√∂hk…ôm"
        else:
            profile_stage = "influencer"
            profile_stage_az = "ƒ∞nfluenser"
        
        logger.info(f"üéØ Profil m…ôrh…ôl…ôsi: {profile_stage_az}")
        
        # Call OpenAI
        logger.info(f"ü§ñ OpenAI analizi ba≈ülayƒ±r...")
        
        client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
        
        analysis_prompt = f"""LinkedIn profil analizi v…ô SMM t√∂vsiy…ôl…ôri (Az…ôrbaycan dilind…ô).

PROFIL M∆èLUMATLARI:
- Profil adƒ±: {profile_name}
- Hazƒ±rkƒ± Headline: "{current_headline if current_headline else 'Bo≈ü'}"
- Followers: {followers:,}
- Connections: {connections:,}
- Posts: {posts}
- Payla≈üƒ±m sƒ±xƒ±lƒ±ƒüƒ±: {posting_frequency_text}
- Niche/Sah…ô: {niche if niche else '√úmumi'}
- Profil m…ôrh…ôl…ôsi: {profile_stage_az}
- Connection ratio: {connection_ratio:.2f}

LINKEDIN X√úSUSƒ∞YY∆èTL∆èRƒ∞:
- LinkedIn B2B v…ô professional network platformasƒ±dƒ±r
- Long-form content daha yax≈üƒ± i≈ül…ôyir
- Industry insights v…ô thought leadership prioritiz…ô olunur
- Comments v…ô discussions √ßox vacibdir
- LinkedIn Articles v…ô native video content
- Professional tone v…ô value-driven content

T∆èHLƒ∞L V∆è T√ñVSƒ∞Y∆èL∆èR HAZIRLYIN:

1. HEADLINE T∆èKLƒ∞FL∆èRƒ∞ (5 variant):
   - Professional, keyword-rich
   - Value proposition g√∂st…ôrsin
   - Industry v…ô expertise vurƒüulayƒ±n
   - SEO-friendly

2. CONTENT STRATEGƒ∞YASI:
   - Content n√∂vl…ôri (faiz payƒ± il…ô): Articles, Native Posts, Video, Carousel, Document
   - Post tezliyi t√∂vsiy…ôsi
   - Long-form content strategiyasƒ±
   - Industry insights strategiyasƒ±
   - Content pillars (3-5 …ôsas m√∂vzu)

3. POSTƒ∞NG SCHEDULE (REAL VAXT FORMATINDA):
   - ∆èn yax≈üƒ± post saatlarƒ± (h…ôft…ô g√ºnl…ôri) - REAL VAXT (m…ôs…ôl…ôn: "08:00", "12:00")
   - LinkedIn-d…ô h…ôft…ô sonu az aktivlik var
   - Top 3 …ôn effektiv posting saatlarƒ± (real vaxt, effektivlik skoru, detallƒ± s…ôb…ôb)

4. ENGAGEMENT ARTIRILMASI:
   - LinkedIn-specific engagement tips (10 tip)
   - Comments v…ô discussions strategiyasƒ±
   - LinkedIn Groups strategiyasƒ±
   - Thought leadership content
   - Networking strategiyasƒ±

5. GROWTH STRATEGƒ∞YASI:
   - 30 g√ºnl√ºk plan
   - Realistic growth h…ôd…ôfl…ôri
   - LinkedIn Ads inteqrasiyasƒ±
   - Metrics track etm…ôk √º√ß√ºn

JSON formatda qaytarƒ±n:
{{
    "headline_suggestions": [
        {{
            "headline": "...",
            "explanation": "Niy…ô bu headline i≈ül…ôy…ôc…ôk"
        }}
    ],
    "content_strategy": {{
        "content_mix": {{
            "articles": 25,
            "native_posts": 35,
            "video": 20,
            "carousel": 15,
            "document": 5
        }},
        "post_frequency": "h…ôft…ôd…ô X post",
        "article_frequency": "ayda X article",
        "content_pillars": ["M√∂vzu 1", "M√∂vzu 2", ...]
    }},
    "posting_schedule": {{
        "weekdays": {{
            "morning": {{
                "time_range": "07:00-09:00",
                "best_time": "08:00",
                "effectiveness": "Detallƒ± izah"
            }},
            "midday": {{
                "time_range": "12:00-13:00",
                "best_time": "12:30",
                "effectiveness": "Detallƒ± izah"
            }},
            "afternoon": {{
                "time_range": "17:00-18:00",
                "best_time": "17:30",
                "effectiveness": "Detallƒ± izah"
            }},
            "best_time": "08:00",
            "best_time_reason": "Detallƒ± s…ôb…ôb"
        }},
        "top_3_best_times": [
            {{
                "time": "08:00",
                "day_type": "H…ôft…ô i√ßi",
                "effectiveness_score": "95%",
                "reason": "Detallƒ± s…ôb…ôb"
            }},
            ...
        ]
    }},
    "engagement_tips": ["Tip 1", "Tip 2", ...],
    "growth_strategy": {{
        "30_day_plan": {{
            "week_1": "...",
            "week_2": "...",
            "week_3": "...",
            "week_4": "..."
        }},
        "realistic_goals": {{
            "followers_growth": "+X%",
            "engagement_target": "Y%"
        }},
        "metrics_to_track": ["metric1", "metric2", ...]
    }},
    "overall_assessment": {{
        "strengths": ["g√ºcl√º t…ôr…ôf 1", ...],
        "weaknesses": ["z…ôif t…ôr…ôf 1", ...],
        "opportunities": ["f√ºrs…ôt 1", ...],
        "priority_actions": ["√∂ncelikli addƒ±m 1", ...]
    }}
}}

B√ºt√ºn m…ôtnl…ôr Az…ôrbaycan dilind…ô olmalƒ±dƒ±r."""

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "Siz pe≈ü…ôkar Social Media Marketing (SMM) m…ôsl…ôh…ôt√ßisisiniz. LinkedIn profill…ôrini analiz edib konkret, t…ôtbiq oluna bil…ôn t√∂vsiy…ôl…ôr verirsiniz. B√ºt√ºn cavablar Az…ôrbaycan dilind…ô olmalƒ±dƒ±r."
                },
                {
                    "role": "user",
                    "content": analysis_prompt
                }
            ],
            temperature=0.7,
            max_tokens=3000
        )
        
        analysis_text = response.choices[0].message.content.strip()
        
        # Clean up JSON
        if analysis_text.startswith('```'):
            analysis_text = re.sub(r'^```json?\s*', '', analysis_text)
            analysis_text = re.sub(r'\s*```$', '', analysis_text)
        
        analysis_data = json.loads(analysis_text)
        
        logger.info(f"‚úÖ Analiz tamamlandƒ±")
        
        # Calculate estimated engagement
        estimated_engagement = 0
        if posts > 0 and followers > 0:
            base_engagement = {
                "starter": 3.5,
                "growing": 3.0,
                "established": 2.5,
                "influencer": 2.0
            }.get(profile_stage, 2.8)
            
            frequency_multiplier = {
                '1-2': 1.3,
                '3-4': 1.1,
                '5-7': 1.0,
                'daily': 0.9,
                '2plus': 0.8
            }.get(posting_frequency, 1.0)
            
            estimated_engagement = round(base_engagement * frequency_multiplier, 1)
        
        return Response({
            "success": True,
            "profile_info": {
                "profile_name": profile_name,
                "followers": followers,
                "connections": connections,
                "posts": posts,
                "engagement_rate": estimated_engagement,
                "posting_frequency": posting_frequency_text,
                "niche": niche,
                "profile_stage": profile_stage_az,
                "connection_ratio": round(connection_ratio, 2)
            },
            "analysis": analysis_data,
            "generated_at": datetime.now().isoformat()
        }, status=status.HTTP_200_OK)
        
    except json.JSONDecodeError as e:
        logger.error(f"‚ùå JSON parse x…ôtasƒ±: {str(e)}")
        return Response({
            "error": "AI cavabƒ± parse edil…ô bilm…ôdi"
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    except Exception as e:
        logger.error(f"‚ùå LinkedIn analiz x…ôtasƒ±: {str(e)}", exc_info=True)
        return Response({
            "error": f"Analiz zamanƒ± x…ôta ba≈ü verdi: {str(e)}"
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


def scrape_instagram_with_apify(profile_url):
    """
    Scrape Instagram profile using Apify API
    Actor: apify/instagram-scraper (ID: shu8hvrXbJbY3Eb9W)
    """
    try:
        apify_api_key = getattr(settings, 'APIFY_API_KEY', None)
        
        logger.info(f"üîë APIFY_API_KEY: {apify_api_key[:20] if apify_api_key else 'YOX'}...")
        
        if not apify_api_key or apify_api_key == '':
            logger.warning("‚ö†Ô∏è APIFY_API_KEY yoxdur v…ô ya bo≈üdur, manual input istifad…ô edin")
            return None
        
        logger.info(f"üîç Apify il…ô scraping ba≈ülayƒ±r: {profile_url}")
        
        # Apify Instagram Scraper actor ID
        # https://console.apify.com/actors/shu8hvrXbJbY3Eb9W
        apify_actor_id = "shu8hvrXbJbY3Eb9W"
        
        # ∆èvv…ôlc…ô actor-un input schema-sƒ±nƒ± alaq
        schema_url = f"https://api.apify.com/v2/acts/{apify_actor_id}/input-schema?token={apify_api_key}"
        schema_response = requests.get(schema_url, timeout=10)
        
        if schema_response.status_code == 200:
            schema = schema_response.json()
            logger.info(f"üìã Actor input schema alƒ±ndƒ±")
            logger.info(f"üìã Schema keys: {list(schema.get('properties', {}).keys())}")
        
        # Apify Instagram Scraper input formatƒ±
        # Apify console-da i≈ül…ôy…ôn format: directUrls istifad…ô edir
        # resultsType: "details" - profil detallarƒ± √º√ß√ºn
        run_input = {
            "directUrls": [profile_url],
            "resultsType": "details"
        }
        
        run_url = f"https://api.apify.com/v2/acts/{apify_actor_id}/runs?token={apify_api_key}"
        
        logger.info(f"üì° Apify API request: {run_url[:80]}...")
        logger.info(f"üì¶ Input data: {json.dumps(run_input, indent=2)}")
        
        response = requests.post(run_url, json=run_input, timeout=15)
        
        if response.status_code not in [200, 201]:
            error_text = response.text
            logger.error(f"‚ùå Apify API error ({response.status_code}): {error_text[:500]}")
            return None
        
        run_data = response.json()
        run_id = run_data['data']['id']
        initial_status = run_data['data'].get('status', 'UNKNOWN')
        
        logger.info(f"‚úÖ Apify actor ba≈üladƒ±: {run_id}, initial status: {initial_status}")
        
        max_wait = 120
        wait_interval = 5
        elapsed = 0
        run_status = initial_status
        
        while elapsed < max_wait:
            time.sleep(wait_interval)
            elapsed += wait_interval
            
            status_url = f"https://api.apify.com/v2/actor-runs/{run_id}?token={apify_api_key}"
            status_response = requests.get(status_url, timeout=10)
            status_data = status_response.json()
            
            run_status = status_data['data']['status']
            
            logger.info(f"‚è≥ Scraping status: {run_status} ({elapsed}s)")
            
            if run_status == 'SUCCEEDED':
                logger.info(f"‚úÖ Scraping tamamlandƒ± ({elapsed}s)")
                break
            elif run_status in ['FAILED', 'ABORTED', 'TIMED-OUT']:
                error_message = status_data['data'].get('statusMessage', '')
                logger.error(f"‚ùå Scraping uƒüursuz: {run_status} - {error_message}")
                return None
            elif run_status in ['READY', 'RUNNING']:
                # Run davam edir
                continue
        
        if run_status != 'SUCCEEDED':
            logger.warning(f"‚ö†Ô∏è Scraping timeout v…ô ya uƒüursuz ({max_wait}s), final status: {run_status}")
            return None
        
        # Dataset ID-ni run-dan alaq
        final_status = requests.get(f"https://api.apify.com/v2/actor-runs/{run_id}?token={apify_api_key}", timeout=10).json()
        dataset_id = final_status['data'].get('defaultDatasetId')
        
        if not dataset_id:
            dataset_id = f"runs/{run_id}/dataset"
        
        dataset_url = f"https://api.apify.com/v2/datasets/{dataset_id}/items?token={apify_api_key}"
        dataset_response = requests.get(dataset_url, timeout=10)
        dataset_response.raise_for_status()
        
        items = dataset_response.json()
        
        logger.info(f"üì¶ Dataset-d…ôn {len(items)} item alƒ±ndƒ±")
        
        if not items or len(items) == 0:
            logger.warning("‚ö†Ô∏è He√ß bir m…ôlumat tapƒ±lmadƒ±")
            return None
        
        # ƒ∞lk item-i log ed…ôk ki, struktur g√∂r…ôk
        logger.info(f"üìã Dataset item struktur (ilk 500 simvol): {json.dumps(items[0], indent=2)[:500]}")
        
        profile_data = items[0]
        
        # Error yoxlayaq
        if "error" in profile_data:
            error_msg = profile_data.get("errorDescription", profile_data.get("error", "Unknown error"))
            logger.warning(f"‚ö†Ô∏è Apify error: {error_msg}")
            logger.warning(f"‚ö†Ô∏è Profil private ola bil…ôr v…ô ya scraping m√ºmk√ºn deyil. OG preview v…ô ya manual input istifad…ô edin.")
            return None
        
        # Apify Instagram Scraper - real JSON strukturuna g√∂r…ô parse edirik
        # Field adlarƒ±: username, fullName, biography, followersCount, followsCount, postsCount, etc.
        scraped_data = {
            "username": profile_data.get("username", ""),
            "full_name": profile_data.get("fullName", ""),
            "biography": profile_data.get("biography", ""),
            "followers": profile_data.get("followersCount", 0),
            "following": profile_data.get("followsCount", 0),
            "posts": profile_data.get("postsCount", 0),
            "profile_pic_url": profile_data.get("profilePicUrlHD") or profile_data.get("profilePicUrl", ""),
            "is_verified": profile_data.get("verified", False),
            "is_business": profile_data.get("isBusinessAccount", False),
            "category": profile_data.get("businessCategoryName", ""),
            "is_private": profile_data.get("private", False),
            "highlight_reel_count": profile_data.get("highlightReelCount", 0),
            "igtv_video_count": profile_data.get("igtvVideoCount", 0),
            "latest_posts": profile_data.get("latestPosts", [])
        }
        
        logger.info(f"üìä Scrape edildi: @{scraped_data['username']}, {scraped_data['followers']} followers, {scraped_data['posts']} posts")
        
        return scraped_data
        
    except requests.exceptions.RequestException as e:
        logger.error(f"‚ùå Apify API x…ôtasƒ±: {str(e)}")
        return None
    except Exception as e:
        logger.error(f"‚ùå Scraping x…ôtasƒ±: {str(e)}", exc_info=True)
        return None
        
        run_input = {
            "directUrls": [profile_url],
            "resultsType": "profiles",
            "resultsLimit": 1,
            "searchLimit": 1,
            "addParentData": False
        }
        
        run_url = f"https://api.apify.com/v2/acts/{apify_actor_id}/runs?token={apify_api_key}"
        
        response = requests.post(run_url, json=run_input, timeout=10)
        response.raise_for_status()
        
        run_data = response.json()
        run_id = run_data['data']['id']
        default_dataset_id = run_data['data']['defaultDatasetId']
        
        logger.info(f"‚úÖ Apify actor ba≈üladƒ±: {run_id}")
        
        max_wait = 60
        wait_interval = 3
        elapsed = 0
        
        while elapsed < max_wait:
            time.sleep(wait_interval)
            elapsed += wait_interval
            
            status_url = f"https://api.apify.com/v2/actor-runs/{run_id}?token={apify_api_key}"
            status_response = requests.get(status_url, timeout=10)
            status_data = status_response.json()
            
            run_status = status_data['data']['status']
            
            if run_status == 'SUCCEEDED':
                logger.info(f"‚úÖ Scraping tamamlandƒ± ({elapsed}s)")
                break
            elif run_status in ['FAILED', 'ABORTED', 'TIMED-OUT']:
                logger.error(f"‚ùå Scraping uƒüursuz: {run_status}")
                return None
        else:
            logger.warning(f"‚ö†Ô∏è Scraping timeout ({max_wait}s)")
            return None
        
        dataset_url = f"https://api.apify.com/v2/datasets/{default_dataset_id}/items?token={apify_api_key}"
        dataset_response = requests.get(dataset_url, timeout=10)
        dataset_response.raise_for_status()
        
        items = dataset_response.json()
        
        if not items or len(items) == 0:
            logger.warning("‚ö†Ô∏è He√ß bir m…ôlumat tapƒ±lmadƒ±")
            return None
        
        profile_data = items[0]
        
        scraped_data = {
            "username": profile_data.get("username", ""),
            "full_name": profile_data.get("fullName", ""),
            "biography": profile_data.get("biography", ""),
            "followers": profile_data.get("followersCount", 0),
            "following": profile_data.get("followsCount", 0),
            "posts": profile_data.get("postsCount", 0),
            "profile_pic_url": profile_data.get("profilePicUrlHD") or profile_data.get("profilePicUrl", ""),
            "is_verified": profile_data.get("verified", False),
            "is_business": profile_data.get("isBusinessAccount", False),
            "category": profile_data.get("businessCategoryName", ""),
            "is_private": profile_data.get("private", False),
            "highlight_reel_count": profile_data.get("highlightReelCount", 0),
            "igtv_video_count": profile_data.get("igtvVideoCount", 0),
            "latest_posts": profile_data.get("latestPosts", [])
        }
        
        logger.info(f"üñºÔ∏è Profil ≈ü…ôkli URL: {scraped_data['profile_pic_url'][:100] if scraped_data['profile_pic_url'] else 'YOX'}...")
        
        logger.info(f"üìä Scrape edildi: @{scraped_data['username']}, {scraped_data['followers']} followers")
        
        return scraped_data
        
    except requests.exceptions.RequestException as e:
        logger.error(f"‚ùå Apify API x…ôtasƒ±: {str(e)}")
        return None
    except Exception as e:
        logger.error(f"‚ùå Scraping x…ôtasƒ±: {str(e)}", exc_info=True)
        return None


def scrape_linkedin_with_apify(profile_url):
    """
    Scrape LinkedIn profile using Apify API
    NOTE: Personal profil scraping artƒ±q istifad…ô edilmir, yalnƒ±z ≈üirk…ôt s…ôhif…ôl…ôri d…ôst…ôkl…ônir
    """
    logger.info(f"üë§ LinkedIn personal profil scraping artƒ±q istifad…ô edilmir, OG preview istifad…ô edil…ôc…ôk: {profile_url}")
    return None


def scrape_linkedin_company_with_apify(company_url):
    """
    Scrape LinkedIn company page using Apify API
    Actor: icypeas_official/linkedin-company-scraper (ID: UKWDVj4p6sQlVquWc)
    """
    try:
        apify_api_key = getattr(settings, 'APIFY_API_KEY', None)
        
        if not apify_api_key or apify_api_key == '':
            logger.warning("‚ö†Ô∏è APIFY_API_KEY yoxdur v…ô ya bo≈üdur, LinkedIn ≈üirk…ôt scraping m√ºmk√ºn deyil")
            return None
        
        logger.info(f"üè¢ LinkedIn ≈üirk…ôt s…ôhif…ôsi Apify scraping ba≈ülayƒ±r: {company_url}")
        
        # Apify LinkedIn Company Scraper (specifically for company pages)
        # https://console.apify.com/actors/UKWDVj4p6sQlVquWc
        # Actor: icypeas_official/linkedin-company-scraper
        apify_actor_id = "UKWDVj4p6sQlVquWc"
        
        # Extract company name from URL
        # URL format: https://www.linkedin.com/company/company-name/
        company_name = company_url.rstrip('/').split('/')[-1]
        logger.info(f"üìù Extracted LinkedIn company name: {company_name}")
        logger.info(f"üìù Original URL: {company_url}")
        
        # Get actor input schema to understand the correct format
        schema_url = f"https://api.apify.com/v2/acts/{apify_actor_id}/input-schema?token={apify_api_key}"
        try:
            schema_response = requests.get(schema_url, timeout=10)
            if schema_response.status_code == 200:
                schema = schema_response.json()
                logger.info(f"üìã LinkedIn company actor input schema alƒ±ndƒ±")
                logger.info(f"üìã Schema properties: {list(schema.get('properties', {}).keys())}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Schema alƒ±na bilm…ôdi: {str(e)}")
        
        # Apify input format - LinkedIn Company Scraper expects "companies" field with array of URLs
        # Based on the actor UI: "Companies to search (required)" field
        run_input = {
            "companies": [company_url]  # Array of company URLs
        }
        
        run_url = f"https://api.apify.com/v2/acts/{apify_actor_id}/runs?token={apify_api_key}"
        
        logger.info(f"üì° Apify LinkedIn Company API request: {run_url[:80]}...")
        logger.info(f"üì¶ Input data: {json.dumps(run_input, indent=2)}")
        
        response = requests.post(run_url, json=run_input, timeout=15)
        
        if response.status_code not in [200, 201]:
            error_text = response.text
            logger.error(f"‚ùå Apify LinkedIn Company API error ({response.status_code}): {error_text[:500]}")
            return None
        
        run_data = response.json()
        run_id = run_data['data']['id']
        initial_status = run_data['data'].get('status', 'UNKNOWN')
        
        logger.info(f"‚úÖ Apify LinkedIn Company actor ba≈üladƒ±: {run_id}, initial status: {initial_status}")
        
        max_wait = 120
        wait_interval = 5
        elapsed = 0
        run_status = initial_status
        
        while elapsed < max_wait:
            time.sleep(wait_interval)
            elapsed += wait_interval
            
            status_url = f"https://api.apify.com/v2/actor-runs/{run_id}?token={apify_api_key}"
            status_response = requests.get(status_url, timeout=10)
            status_data = status_response.json()
            
            run_status = status_data['data']['status']
            
            logger.info(f"‚è≥ LinkedIn Company scraping status: {run_status} ({elapsed}s)")
            
            if run_status == 'SUCCEEDED':
                logger.info(f"‚úÖ LinkedIn Company scraping tamamlandƒ± ({elapsed}s)")
                break
            elif run_status in ['FAILED', 'ABORTED', 'TIMED-OUT']:
                error_message = status_data['data'].get('statusMessage', '')
                logger.error(f"‚ùå LinkedIn Company scraping uƒüursuz: {run_status} - {error_message}")
                return None
            elif run_status in ['READY', 'RUNNING']:
                continue
        
        if run_status != 'SUCCEEDED':
            logger.warning(f"‚ö†Ô∏è LinkedIn Company scraping timeout v…ô ya uƒüursuz ({max_wait}s), final status: {run_status}")
            return None
        
        # Get dataset ID
        final_status = requests.get(f"https://api.apify.com/v2/actor-runs/{run_id}?token={apify_api_key}", timeout=10).json()
        default_dataset_id = final_status['data'].get('defaultDatasetId')
        
        if not default_dataset_id:
            default_dataset_id = f"runs/{run_id}/dataset"
        
        # Get company data from dataset
        dataset_url = f"https://api.apify.com/v2/datasets/{default_dataset_id}/items?token={apify_api_key}"
        dataset_response = requests.get(dataset_url, timeout=10)
        dataset_response.raise_for_status()
            
        items = dataset_response.json()
        
        if not items or len(items) == 0:
            logger.warning("‚ö†Ô∏è LinkedIn ≈üirk…ôt s…ôhif…ôsind…ôn he√ß bir m…ôlumat tapƒ±lmadƒ±")
            return None
        
        logger.info(f"‚úÖ LinkedIn Company dataset-d…ôn {len(items)} item alƒ±ndƒ±")
        
        # ƒ∞lk item-i log ed…ôk - full structure
        logger.info(f"üìã LinkedIn Company dataset item struktur (FULL): {json.dumps(items[0], indent=2)}")
        logger.info(f"üìã LinkedIn Company dataset item keys: {list(items[0].keys())}")
        
        company_data = items[0]
        
        # Parse company data based on the actual structure returned by the actor
        # Check for nested structures (basic_info, company_info, etc.)
        basic_info = company_data.get("basic_info", {}) or company_data.get("company_info", {}) or {}
        
        # Company name - try multiple locations
        company_name = (company_data.get("name", "") or 
                       company_data.get("company_name", "") or 
                       company_data.get("fullname", "") or
                       basic_info.get("name", "") or
                       basic_info.get("company_name", "") or
                       basic_info.get("fullname", ""))
        
        # Headline/tagline
        headline = (company_data.get("headline", "") or 
                   company_data.get("tagline", "") or 
                   company_data.get("description", "") or
                   basic_info.get("headline", "") or
                   basic_info.get("tagline", "") or
                   basic_info.get("description", ""))
        
        # About/summary
        about = (company_data.get("about", "") or 
                company_data.get("summary", "") or 
                company_data.get("description", "") or
                basic_info.get("about", "") or
                basic_info.get("summary", "") or
                basic_info.get("description", ""))
        
        # Location - try multiple locations and formats
        location_data = (company_data.get("location", "") or 
                        company_data.get("headquarters", "") or
                        basic_info.get("location", "") or
                        basic_info.get("headquarters", ""))
        
        if isinstance(location_data, dict):
            location = location_data.get("full", "") or location_data.get("city", "") + ", " + location_data.get("country", "")
            location = location.strip(", ")
        elif isinstance(location_data, str):
            location = location_data
        else:
            location = ""
        
        # Company stats - try multiple locations and field names
        followers_count = (company_data.get("followers_count", 0) or 
                          company_data.get("followerCount", 0) or 
                          company_data.get("followers", 0) or 
                          company_data.get("follower_count", 0) or
                          basic_info.get("followers_count", 0) or
                          basic_info.get("followerCount", 0) or
                          basic_info.get("followers", 0))
        
        employees_count = (company_data.get("employees_count", 0) or 
                          company_data.get("employeeCount", 0) or 
                          company_data.get("employees", 0) or 
                          company_data.get("employee_count", 0) or
                          basic_info.get("employees_count", 0) or
                          basic_info.get("employeeCount", 0) or
                          basic_info.get("employees", 0))
        
        # Posts count - LinkedIn companies might have posts
        posts_count = (company_data.get("posts_count", 0) or 
                      company_data.get("postCount", 0) or 
                      company_data.get("posts", 0) or
                      basic_info.get("posts_count", 0) or
                      basic_info.get("postCount", 0) or
                      basic_info.get("posts", 0))
        
        # Connections (for company pages, this might be different)
        connections_count = (company_data.get("connections_count", 0) or 
                            company_data.get("connectionCount", 0) or 
                            company_data.get("connections", 0) or
                            basic_info.get("connections_count", 0) or
                            basic_info.get("connectionCount", 0) or
                            basic_info.get("connections", 0))
        
        # Profile picture/logo
        profile_pic_url = (company_data.get("profile_pic_url", "") or 
                          company_data.get("profilePicture", "") or 
                          company_data.get("logo", "") or 
                          company_data.get("logo_url", "") or
                          basic_info.get("profile_pic_url", "") or
                          basic_info.get("profilePicture", "") or
                          basic_info.get("logo", "") or
                          basic_info.get("logo_url", ""))
        
        # Company URL
        company_url_linkedin = (company_data.get("profile_url", "") or 
                               company_data.get("url", "") or 
                               basic_info.get("profile_url", "") or
                               basic_info.get("url", "") or
                               company_url)
        
        # Company identifier
        public_identifier = (company_data.get("public_identifier", "") or 
                            company_data.get("identifier", "") or
                            basic_info.get("public_identifier", "") or
                            basic_info.get("identifier", "") or
                            company_name.lower().replace(" ", "-"))
        
        # Company type and industry
        company_type = (company_data.get("company_type", "") or 
                       company_data.get("type", "") or
                       basic_info.get("company_type", "") or
                       basic_info.get("type", ""))
        
        industry = (company_data.get("industry", "") or 
                   company_data.get("sector", "") or
                   basic_info.get("industry", "") or
                   basic_info.get("sector", ""))
        
        # Website
        website = (company_data.get("website", "") or 
                  company_data.get("website_url", "") or
                  basic_info.get("website", "") or
                  basic_info.get("website_url", ""))
        
        # Phone, email, address
        phone = (company_data.get("phone", "") or 
                company_data.get("phone_number", "") or
                basic_info.get("phone", "") or
                basic_info.get("phone_number", ""))
        
        email = (company_data.get("email", "") or 
                basic_info.get("email", ""))
        
        address = (company_data.get("address", "") or 
                  company_data.get("headquarters_address", "") or
                  basic_info.get("address", "") or
                  basic_info.get("headquarters_address", ""))
        
        scraped_data = {
            "name": company_name,
            "full_name": company_name,
            "headline": headline,
            "about": about,
            "location": location,
            "followers": followers_count,
            "employees": employees_count,
            "posts": posts_count,  # Posts count
            "connections": connections_count,  # Connections count
            "profile_pic_url": profile_pic_url,
            "profile_url": company_url_linkedin or company_url,
            "public_identifier": public_identifier,
            "company_type": company_type,
            "industry": industry,
            "website": website,
            "phone": phone,
            "email": email,
            "address": address,
            "verified": company_data.get("verified", False) or basic_info.get("verified", False)
        }
        
        logger.info(f"üìä LinkedIn ≈üirk…ôt s…ôhif…ôsi scrape edildi: {scraped_data['name']}, {scraped_data['followers']} followers, {scraped_data['employees']} employees, {scraped_data['posts']} posts, {scraped_data['connections']} connections")
        
        return scraped_data
        
    except requests.exceptions.RequestException as e:
        logger.error(f"‚ùå LinkedIn Company Apify API x…ôtasƒ±: {str(e)}")
        return None
    except Exception as e:
        logger.error(f"‚ùå LinkedIn Company scraping x…ôtasƒ±: {str(e)}", exc_info=True)
        return None


def scrape_product_with_apify(product_url):
    """
    Scrape e-commerce product information using Apify E-commerce Scraping Tool
    Actor: E-commerce Scraping Tool (ID: 2APbAvDfNDOWXbkWf)
    
    Based on the UI screenshot, the actor has 3 scrape types:
    1. Product detail URLs (productLevelUrl)
    2. Category listing URLs (categoryListingUrl)
    3. Keyword for search (keywordForSearch)
    
    We'll use product detail URLs format.
    """
    try:
        apify_api_key = getattr(settings, 'APIFY_API_KEY', None)
        
        if not apify_api_key or apify_api_key == '':
            logger.warning("‚ö†Ô∏è APIFY_API_KEY yoxdur v…ô ya bo≈üdur, m…ôhsul scraping m√ºmk√ºn deyil")
            return None
        
        logger.info(f"üõí M…ôhsul Apify scraping ba≈ülayƒ±r: {product_url}")
        
        # Apify E-commerce Scraping Tool
        # https://console.apify.com/actors/2APbAvDfNDOWXbkWf
        apify_actor_id = "2APbAvDfNDOWXbkWf"
        
        # Based on UI screenshot, "Product detail URLs" field expects array of URL strings
        # The UI shows it accepts multiple URLs with "+ Add" button
        run_input = {
            "productLevelUrl": [product_url]  # Array of URL strings (not objects)
        }
        
        run_url = f"https://api.apify.com/v2/acts/{apify_actor_id}/runs?token={apify_api_key}"
        
        logger.info(f"üì° Apify E-commerce API request")
        logger.info(f"üì¶ Input data: {json.dumps(run_input, indent=2)}")
        
        response = requests.post(run_url, json=run_input, timeout=15)
        
        if response.status_code not in [200, 201]:
            error_text = response.text
            logger.error(f"‚ùå Apify E-commerce API error ({response.status_code}): {error_text[:500]}")
            return None
        
        run_data = response.json()
        run_id = run_data['data']['id']
        initial_status = run_data['data'].get('status', 'UNKNOWN')
        
        logger.info(f"‚úÖ Apify E-commerce actor ba≈üladƒ±: {run_id}, initial status: {initial_status}")
        
        max_wait = 120
        wait_interval = 5
        elapsed = 0
        run_status = initial_status
        
        while elapsed < max_wait:
            time.sleep(wait_interval)
            elapsed += wait_interval
            
            status_url = f"https://api.apify.com/v2/actor-runs/{run_id}?token={apify_api_key}"
            status_response = requests.get(status_url, timeout=10)
            status_data = status_response.json()
            
            run_status = status_data['data']['status']
            
            logger.info(f"‚è≥ E-commerce scraping status: {run_status} ({elapsed}s)")
            
            if run_status == 'SUCCEEDED':
                logger.info(f"‚úÖ E-commerce scraping tamamlandƒ± ({elapsed}s)")
                break
            elif run_status in ['FAILED', 'ABORTED', 'TIMED-OUT']:
                error_message = status_data['data'].get('statusMessage', '')
                logger.error(f"‚ùå E-commerce scraping uƒüursuz: {run_status} - {error_message}")
                return None
            elif run_status in ['READY', 'RUNNING']:
                continue
        
        if run_status != 'SUCCEEDED':
            logger.warning(f"‚ö†Ô∏è E-commerce scraping timeout v…ô ya uƒüursuz ({max_wait}s), final status: {run_status}")
            return None
        
        # Get dataset ID
        final_status = requests.get(f"https://api.apify.com/v2/actor-runs/{run_id}?token={apify_api_key}", timeout=10).json()
        default_dataset_id = final_status['data'].get('defaultDatasetId')
        
        if not default_dataset_id:
            default_dataset_id = f"runs/{run_id}/dataset"
        
        # Get product data from dataset
        dataset_url = f"https://api.apify.com/v2/datasets/{default_dataset_id}/items?token={apify_api_key}"
        dataset_response = requests.get(dataset_url, timeout=10)
        dataset_response.raise_for_status()
        
        items = dataset_response.json()
        
        if not items or len(items) == 0:
            logger.warning("‚ö†Ô∏è M…ôhsul s…ôhif…ôsind…ôn he√ß bir m…ôlumat tapƒ±lmadƒ±")
            return None
        
        logger.info(f"‚úÖ E-commerce dataset-d…ôn {len(items)} item alƒ±ndƒ±")
        logger.info(f"üìã E-commerce dataset item struktur (FULL): {json.dumps(items[0], indent=2)}")
        
        product_data = items[0]
        
        # Parse product data - based on the JSON structure you provided
        # Structure: { url, name, offers: { price, priceCurrency }, brand: { slogan }, image, description, additionalProperties }
        product_name = product_data.get("name", "")
        description = product_data.get("description", "")
        image_url = product_data.get("image", "")
        
        # Brand - can be object with slogan or string
        brand_data = product_data.get("brand", "")
        if isinstance(brand_data, dict):
            brand = brand_data.get("slogan", "") or brand_data.get("name", "")
        else:
            brand = str(brand_data) if brand_data else ""
        
        # Offers/Price
        offers_data = product_data.get("offers", {})
        price = ""
        currency = ""
        
        if isinstance(offers_data, dict):
            price = str(offers_data.get("price", ""))
            currency = offers_data.get("priceCurrency", "")
        
        # Additional properties
        additional_properties = product_data.get("additionalProperties", {})
        
        scraped_data = {
            "name": product_name,
            "description": description,
            "image": image_url,
            "brand": brand,
            "price": price,
            "currency": currency,
            "url": product_data.get("url", product_url),
            "additional_properties": additional_properties,
            "raw_data": product_data  # Keep raw data for debugging
        }
        
        logger.info(f"üìä M…ôhsul scrape edildi: {scraped_data['name']}, Price: {scraped_data['price']} {scraped_data['currency']}")
        
        return scraped_data
        
    except requests.exceptions.RequestException as e:
        logger.error(f"‚ùå E-commerce Apify API x…ôtasƒ±: {str(e)}")
        return None
    except Exception as e:
        logger.error(f"‚ùå E-commerce scraping x…ôtasƒ±: {str(e)}", exc_info=True)
        return None


def scrape_facebook_posts_with_apify(profile_url, apify_api_key):
    """
    Scrape Facebook posts using Apify Facebook Posts Scraper
    Actor: apify/facebook-posts-scraper (ID: KoJrdxJCTtpon81KY)
    """
    try:
        logger.info(f"üîç Facebook Posts Scraper ba≈ülayƒ±r: {profile_url}")
        
        # Apify Facebook Posts Scraper actor ID
        # https://console.apify.com/actors/KoJrdxJCTtpon81KY
        apify_actor_id = "KoJrdxJCTtpon81KY"
        
        # Apify input format - startUrls array (required field)
        # Based on the actor UI: "Facebook URLs (required)" and "Results amount"
        # Try multiple parameter names to ensure we get more posts
        # Also add scroll parameters to ensure all posts are loaded
        run_input = {
            "startUrls": [{"url": profile_url}],  # startUrls with url objects
            "resultsLimit": 100,  # Try higher limit
            "resultsAmount": 100,  # Alternative parameter name
            "maxResults": 100,  # Another alternative
            "limit": 100,  # Another alternative
            "scrollDown": 50,  # Scroll down 50 times to load more posts (increased for "people" profiles)
            "maxScrolls": 50,  # Maximum scrolls (increased for "people" profiles)
            "scrollLimit": 50,  # Scroll limit (increased for "people" profiles)
            "maxPosts": 100,  # Max posts to scrape
            "maxItems": 100,  # Max items to scrape
            "scroll": True,  # Enable scrolling
            "enableScroll": True,  # Enable scrolling (alternative)
            "loadMore": True  # Load more posts
        }
        
        # Start Apify actor run
        run_url = f"https://api.apify.com/v2/acts/{apify_actor_id}/runs?token={apify_api_key}"
        
        logger.info(f"üì° Apify Facebook Posts API request: {run_url[:80]}...")
        logger.info(f"üì¶ Input data: {json.dumps(run_input, indent=2)}")
        logger.info(f"üì¶ Sending resultsLimit: {run_input.get('resultsLimit')} to get more posts")
        
        response = requests.post(
            run_url,
            json=run_input,
            headers={"Content-Type": "application/json"},
            timeout=30
        )
        
        if response.status_code not in [200, 201]:
            error_text = response.text
            logger.error(f"‚ùå Facebook Posts Scraper API error ({response.status_code}): {error_text[:500]}")
            return None
        
        run_data = response.json()
        run_id = run_data['data']['id']
        initial_status = run_data['data'].get('status', 'UNKNOWN')
        
        logger.info(f"‚úÖ Apify Facebook Posts actor ba≈üladƒ±: {run_id}, initial status: {initial_status}")
        
        max_wait = 120
        wait_interval = 5
        elapsed = 0
        run_status = initial_status
        
        while elapsed < max_wait:
            time.sleep(wait_interval)
            elapsed += wait_interval
            
            status_url = f"https://api.apify.com/v2/actor-runs/{run_id}?token={apify_api_key}"
            status_response = requests.get(status_url, timeout=10)
            status_data = status_response.json()
            
            run_status = status_data['data']['status']
            
            logger.info(f"‚è≥ Facebook Posts scraping status: {run_status} ({elapsed}s)")
            
            if run_status == 'SUCCEEDED':
                logger.info(f"‚úÖ Facebook Posts scraping tamamlandƒ± ({elapsed}s)")
                break
            elif run_status in ['FAILED', 'ABORTED', 'TIMED-OUT']:
                error_message = status_data['data'].get('statusMessage', '')
                logger.error(f"‚ùå Facebook Posts scraping uƒüursuz: {run_status} - {error_message}")
                return None
            elif run_status in ['READY', 'RUNNING']:
                continue
        
        if run_status != 'SUCCEEDED':
            logger.warning(f"‚ö†Ô∏è Facebook Posts scraping timeout v…ô ya uƒüursuz ({max_wait}s), final status: {run_status}")
            return None
        
        # Get dataset ID
        final_status = requests.get(f"https://api.apify.com/v2/actor-runs/{run_id}?token={apify_api_key}", timeout=10).json()
        default_dataset_id = final_status['data'].get('defaultDatasetId')
        
        if not default_dataset_id:
            default_dataset_id = f"runs/{run_id}/dataset"
        
        # Get posts from dataset
        dataset_url = f"https://api.apify.com/v2/datasets/{default_dataset_id}/items?token={apify_api_key}"
        dataset_response = requests.get(dataset_url, timeout=10)
        dataset_response.raise_for_status()
        
        posts = dataset_response.json()
        
        logger.info(f"‚úÖ Facebook Posts Scraper-d…ôn {len(posts)} post alƒ±ndƒ±")
        
        # Log all posts to see what we got
        if posts and len(posts) > 0:
            logger.info(f"üìã First post keys: {list(posts[0].keys())}")
            logger.info(f"üìã First post structure (ilk 1500 simvol): {json.dumps(posts[0], indent=2, default=str)[:1500]}")
            
            # Log all post IDs and URLs
            logger.info(f"üìã B√ºt√ºn post-larƒ±n siyahƒ±sƒ±:")
            for idx, post in enumerate(posts):
                post_id = post.get("postId", "") or post.get("id", "")
                post_url = post.get("url", "")
                post_text_preview = (post.get("text", "") or post.get("content", ""))[:50]
                logger.info(f"   Post {idx+1}: postId={post_id}, url={post_url[:60] if post_url else 'N/A'}..., text={post_text_preview}...")
        
        return posts
        
    except requests.exceptions.RequestException as e:
        logger.error(f"‚ùå Facebook Posts Scraper API x…ôtasƒ±: {str(e)}")
        return None
    except Exception as e:
        logger.error(f"‚ùå Facebook Posts scraping x…ôtasƒ±: {str(e)}", exc_info=True)
        return None


def scrape_facebook_with_apify(profile_url):
    """
    Scrape Facebook page/profile data using Apify
    Note: Facebook scraping works best with numeric page IDs
    """
    try:
        apify_api_key = getattr(settings, 'APIFY_API_KEY', None)
        
        logger.info(f"üîë APIFY_API_KEY: {apify_api_key[:20] if apify_api_key else 'YOX'}...")
        
        if not apify_api_key or apify_api_key == '':
            logger.warning("‚ö†Ô∏è APIFY_API_KEY yoxdur v…ô ya bo≈üdur, manual input istifad…ô edin")
            return None
        
        logger.info(f"üîç Facebook Apify scraping ba≈ülayƒ±r: {profile_url}")
        
        # Apify Facebook Pages Scraper actor ID
        # https://console.apify.com/actors/4Hv5RhChiaDk6iwad
        apify_actor_id = "4Hv5RhChiaDk6iwad"
        
        # ∆èvv…ôlc…ô actor-un input schema-sƒ±nƒ± alaq
        schema_url = f"https://api.apify.com/v2/acts/{apify_actor_id}/input-schema?token={apify_api_key}"
        schema_response = requests.get(schema_url, timeout=10)
        
        if schema_response.status_code == 200:
            schema = schema_response.json()
            logger.info(f"üìã Actor input schema alƒ±ndƒ±")
            logger.info(f"üìã Schema keys: {list(schema.get('properties', {}).keys())}")
            
            # Log startUrls schema if available
            if 'properties' in schema and 'startUrls' in schema['properties']:
                startUrls_schema = schema['properties']['startUrls']
                logger.info(f"üìã startUrls schema: {json.dumps(startUrls_schema, indent=2)[:500]}")
        else:
            logger.warning(f"‚ö†Ô∏è Schema alƒ±na bilm…ôdi ({schema_response.status_code}): {schema_response.text[:200]}")
        
        # Apify input format - startUrls array with object format
        # Based on Apify documentation, startUrls should be array of objects with "url" key
        # Note: This actor may not scrape posts even with maxPosts parameter
        # Posts will be scraped separately if needed
        run_input = {
            "startUrls": [{"url": profile_url}],
            "maxPosts": 0  # This actor doesn't scrape posts, only page info
        }
        
        # Start Apify actor run
        run_url = f"https://api.apify.com/v2/acts/{apify_actor_id}/runs?token={apify_api_key}"
        
        logger.info(f"üì° Apify Facebook API request: {run_url[:80]}...")
        logger.info(f"üì¶ Input data: {json.dumps(run_input, indent=2)}")
        
        response = requests.post(
            run_url,
            json=run_input,
            headers={"Content-Type": "application/json"},
            timeout=30
        )
        
        if response.status_code not in [200, 201]:
            error_text = response.text
            logger.error(f"‚ùå Facebook Apify API error ({response.status_code}): {error_text[:500]}")
            logger.error(f"‚ùå Full error response: {error_text}")
        return None
        
        run_data = response.json()
        run_id = run_data['data']['id']
        initial_status = run_data['data'].get('status', 'UNKNOWN')
        
        logger.info(f"‚úÖ Apify Facebook actor ba≈üladƒ±: {run_id}, initial status: {initial_status}")
        
        max_wait = 120
        wait_interval = 5
        elapsed = 0
        run_status = initial_status
        
        while elapsed < max_wait:
            time.sleep(wait_interval)
            elapsed += wait_interval
            
            status_url = f"https://api.apify.com/v2/actor-runs/{run_id}?token={apify_api_key}"
            status_response = requests.get(status_url, timeout=10)
            status_data = status_response.json()
            
            run_status = status_data['data']['status']
            
            logger.info(f"‚è≥ Facebook scraping status: {run_status} ({elapsed}s)")
            
            if run_status == 'SUCCEEDED':
                logger.info(f"‚úÖ Facebook scraping tamamlandƒ± ({elapsed}s)")
                break
            elif run_status in ['FAILED', 'ABORTED', 'TIMED-OUT']:
                error_message = status_data['data'].get('statusMessage', '')
                logger.error(f"‚ùå Facebook scraping uƒüursuz: {run_status} - {error_message}")
                return None
            elif run_status in ['READY', 'RUNNING']:
                # Run davam edir
                continue
        
        if run_status != 'SUCCEEDED':
            logger.warning(f"‚ö†Ô∏è Facebook scraping timeout v…ô ya uƒüursuz ({max_wait}s), final status: {run_status}")
            return None
        
        # Dataset ID-ni run-dan alaq
        final_status = requests.get(f"https://api.apify.com/v2/actor-runs/{run_id}?token={apify_api_key}", timeout=10).json()
        default_dataset_id = final_status['data'].get('defaultDatasetId')
        key_value_store_id = final_status['data'].get('defaultKeyValueStoreId')
        
        if not default_dataset_id:
            default_dataset_id = f"runs/{run_id}/dataset"
        
        logger.info(f"üì¶ Dataset ID: {default_dataset_id}, KV Store ID: {key_value_store_id}")
        
        # Try multiple methods to get data
        items = []
        
        # Method 1: Dataset items
        if default_dataset_id:
            dataset_url = f"https://api.apify.com/v2/datasets/{default_dataset_id}/items?token={apify_api_key}"
            logger.info(f"üîÑ Method 1 - Dataset URL: {dataset_url[:100]}...")
            dataset_response = requests.get(dataset_url, timeout=10)
            dataset_response.raise_for_status()
            
            items = dataset_response.json()
            logger.info(f"‚úÖ Method 1 uƒüurlu: {len(items)} item alƒ±ndƒ±")
        
        # Method 2: Key-Value Store OUTPUT key
        if (not items or len(items) == 0) and key_value_store_id:
            output_url = f"https://api.apify.com/v2/key-value-stores/{key_value_store_id}/records/OUTPUT?token={apify_api_key}"
            logger.info(f"üîÑ Method 2 - Key-Value Store OUTPUT key: {output_url[:100]}...")
            
            try:
                output_response = requests.get(output_url, timeout=10)
                
                if output_response.status_code == 200:
                    output_data = output_response.json()
                    logger.info(f"üìã OUTPUT key data type: {type(output_data)}")
                    
                    if isinstance(output_data, list):
                        items = output_data
                        logger.info(f"‚úÖ Method 2 - OUTPUT key-d…ôn {len(items)} item alƒ±ndƒ±")
                    elif isinstance(output_data, dict):
                        items = [output_data]
                        logger.info(f"‚úÖ Method 2 - OUTPUT key-d…ôn 1 page dict alƒ±ndƒ±")
                else:
                    logger.warning(f"‚ö†Ô∏è Method 2 uƒüursuz ({output_response.status_code}): {output_response.text[:200]}")
            except Exception as e:
                logger.error(f"‚ùå Method 2 error: {str(e)}")
        
        if not items or len(items) == 0:
            logger.warning("‚ö†Ô∏è Facebook √º√ß√ºn he√ß bir m…ôlumat tapƒ±lmadƒ±")
            return None
        
        logger.info(f"‚úÖ Facebook dataset-d…ôn {len(items)} item alƒ±ndƒ±")
        
        # Log all items to see structure
        for idx, item in enumerate(items):
            logger.info(f"üìã Item {idx} keys: {list(item.keys())}")
            # Check for any post-related fields
            post_related_keys = [k for k in item.keys() if 'post' in k.lower() or 'text' in k.lower() or 'time' in k.lower()]
            if post_related_keys:
                logger.info(f"üìã Item {idx} post-related keys: {post_related_keys}")
        
        # Separate page info from posts
        # First item is usually the page info, rest might be posts
        profile_data = None
        posts_array = []
        
        for item in items:
            # Check if this is a page info item (has title, categories, etc.)
            if 'title' in item or 'pageUrl' in item:
                if profile_data is None:
                    profile_data = item
                    logger.info(f"üìã Facebook page info keys: {list(item.keys())}")
                    logger.info(f"üìã Facebook page info struktur (ilk 2000 simvol): {json.dumps(item, indent=2, default=str)[:2000]}")
                    
                    # Check for posts in various possible fields
                    possible_post_fields = ['posts', 'pagePosts', 'recentPosts', 'feed', 'timeline', 'pageFeed']
                    for field in possible_post_fields:
                        if field in item:
                            field_value = item[field]
                            logger.info(f"üìã Found field '{field}': type={type(field_value)}, length={len(field_value) if isinstance(field_value, (list, dict)) else 'N/A'}")
                            if isinstance(field_value, list) and len(field_value) > 0:
                                posts_array.extend(field_value)
                                logger.info(f"‚úÖ Added {len(field_value)} posts from field '{field}'")
            # Check if this is a post (has postId, postUrl, text, etc.)
            elif 'postId' in item or 'postUrl' in item or ('text' in item and 'time' in item):
                posts_array.append(item)
                logger.info(f"‚úÖ Found post item: {item.get('postId', item.get('postUrl', 'unknown'))}")
        
        # If no page info found, use first item
        if profile_data is None and len(items) > 0:
            profile_data = items[0]
            logger.info(f"üìã Using first item as page info: {list(profile_data.keys())}")
        
        # Also check if posts are in a nested array in profile_data (final check)
        if profile_data:
            # Check all possible nested structures
            if 'posts' in profile_data:
                nested_posts = profile_data.get('posts', [])
                if isinstance(nested_posts, list) and len(nested_posts) > 0:
                    posts_array.extend(nested_posts)
                    logger.info(f"‚úÖ Added {len(nested_posts)} posts from nested 'posts' field")
            
            # Check if posts are in a dict structure
            if isinstance(profile_data.get('posts'), dict):
                posts_dict = profile_data.get('posts', {})
                logger.info(f"üìã Posts is a dict with keys: {list(posts_dict.keys())}")
                # Try to extract posts from dict
                for key, value in posts_dict.items():
                    if isinstance(value, list):
                        posts_array.extend(value)
                        logger.info(f"‚úÖ Added {len(value)} posts from dict key '{key}'")
        
        logger.info(f"üìä Found {len(posts_array)} posts total in dataset")
        
        # Facebook profile data parse et - Apify facebook-pages-scraper format
        title = profile_data.get("title", "") or profile_data.get("name", "")
        categories = profile_data.get("categories", [])
        likes_count = profile_data.get("likes", 0)
        followers_count = profile_data.get("followers", 0) or likes_count  # Use followers if available, fallback to likes
        info = profile_data.get("info", [])  # info is an array, not dict
        intro = profile_data.get("intro", "")  # About/intro text
        email = profile_data.get("email", "")
        phone = profile_data.get("phone", "")
        address = profile_data.get("address", "")
        website = profile_data.get("website", "")
        websites = profile_data.get("websites", [])  # websites is an array
        page_url = profile_data.get("pageUrl", "") or profile_url
        profile_pic_url = profile_data.get("profilePictureUrl", "") or profile_data.get("profilePhoto", "")
        cover_photo_url = profile_data.get("coverPhotoUrl", "")
        
        # Extract website from websites array if website is empty
        if not website and websites and len(websites) > 0:
            # Filter out Google Maps URLs and get actual website
            for ws in websites:
                if ws and "maps.google.com" not in ws and "facebook.com" not in ws:
                    website = ws
                    break
        
        # Extract about text from intro or info array
        about_text = intro
        if not about_text and info and isinstance(info, list) and len(info) > 0:
            # Join info array items, skip first item if it's just basic info
            about_text = " ".join([item for item in info if item and len(item) > 20])
        
        # Extract rating if available
        rating_info = profile_data.get("rating", "")
        if isinstance(rating_info, str) and "Reviews" in rating_info:
            # Parse "Not yet rated (0 Reviews)" or "4.5 (120 Reviews)"
            rating_text = rating_info
        else:
            rating_text = ""
        
        # Post count - use extracted posts_array from dataset items
        posts_count = len(posts_array) if posts_array else 0
        
        # If posts_count is 0, try to get from profile_data
        if posts_count == 0 and profile_data:
            nested_posts = profile_data.get("posts", [])
            if isinstance(nested_posts, list):
                posts_count = len(nested_posts)
                posts_array = nested_posts
        
        # If still 0, try to extract from info array (e.g., "11 talking about this" might indicate activity)
        # Note: This is just a fallback, actual post count may not be available
        if posts_count == 0 and info and isinstance(info, list):
            for info_item in info:
                if isinstance(info_item, str):
                    # Look for patterns like "X talking about this" or "X posts"
                    import re
                    # Try to find number in "X talking about this" or similar patterns
                    match = re.search(r'(\d+)\s+(?:talking|posts|post)', info_item.lower())
                    if match:
                        potential_count = int(match.group(1))
                        # "talking about this" is not exact post count, but indicates activity
                        # We'll use it as a rough estimate if no other data available
                        logger.info(f"üìä Found potential activity indicator: {info_item}, but not using as post count")
        
        # Always try to scrape posts using Facebook Posts Scraper (even if posts_count > 0)
        # Because the main scraper might not return all posts
        logger.info(f"üìù Facebook Posts Scraper il…ô post-larƒ± scrape edirik (current posts_count: {posts_count})...")
        posts_from_scraper = scrape_facebook_posts_with_apify(profile_url, apify_api_key)
        if posts_from_scraper:
            # Log all posts to see what we got
            logger.info(f"üìã Facebook Posts Scraper-d…ôn {len(posts_from_scraper)} post alƒ±ndƒ±")
            for idx, post in enumerate(posts_from_scraper):
                post_id = post.get("postId", "") or post.get("id", "")
                post_url = post.get("url", "")
                logger.info(f"üìã Post {idx+1}: postId={post_id}, url={post_url[:80] if post_url else 'N/A'}...")
            
            # Remove duplicates based on postId (more strict) - only remove if postId is exactly the same
            seen_post_ids = set()
            unique_posts = []
            for post in posts_from_scraper:
                post_id = post.get("postId", "") or post.get("id", "")
                # Only use postId for duplicate detection, not URL (URLs can be different for same post)
                if post_id:
                    post_id_str = str(post_id)
                    if post_id_str not in seen_post_ids:
                        seen_post_ids.add(post_id_str)
                        unique_posts.append(post)
                    else:
                        logger.warning(f"‚ö†Ô∏è Duplicate post skipped: postId={post_id}")
                else:
                    # If no postId, add anyway (might be different posts)
                    unique_posts.append(post)
            
            posts_array = unique_posts
            posts_count = len(posts_array)
            logger.info(f"‚úÖ Facebook Posts Scraper-d…ôn {len(posts_from_scraper)} post alƒ±ndƒ±, {posts_count} unikal post (duplicate detection: postId-based)")
        else:
            logger.warning(f"‚ö†Ô∏è Facebook Posts Scraper he√ß bir post qaytarmadƒ±")
        
        scraped_data = {
            "name": title,
            "full_name": title,
            "username": page_url.split("/")[-1] if page_url else "",
            "about": about_text or " | ".join(categories) if categories else "",
            "likes": likes_count,
            "followers": followers_count,
            "posts": posts_count,  # Post count
            "category": categories,
            "address": address,
            "phone": phone,
            "email": email,
            "website": website,
            "profile_pic_url": profile_pic_url,
            "cover_photo": cover_photo_url,
            "verified": False,  # Not provided by this scraper
            "profile_url": page_url,
            "rating": rating_text,
            "info": info,
            "posts_data": posts_array  # Full posts array if needed
        }
        
        logger.info(f"üìä Facebook scrape edildi: {scraped_data['name']}")
        logger.info(f"   Likes: {scraped_data['likes']}, Followers: {scraped_data['followers']}, Posts: {scraped_data['posts']}")
        logger.info(f"   Posts array length: {len(posts_array)}")
        logger.info(f"   Categories: {', '.join(categories) if categories else 'N/A'}")
        logger.info(f"   About: {about_text[:100] if about_text else 'N/A'}...")
        logger.info(f"   Email: {email or 'N/A'}")
        logger.info(f"   Phone: {phone or 'N/A'}")
        logger.info(f"   Website: {website or 'N/A'}")
        logger.info(f"   Address: {address or 'N/A'}")
        logger.info(f"‚úÖ Facebook Apify scraping uƒüurlu: {scraped_data['name']}")
        
        return scraped_data
        
    except requests.exceptions.RequestException as e:
        logger.error(f"‚ùå Facebook Apify API x…ôtasƒ±: {str(e)}")
        return None
    except Exception as e:
        logger.error(f"‚ùå Facebook scraping x…ôtasƒ±: {str(e)}", exc_info=True)
        return None


def extract_og_preview(url):
    """
    Extract OG preview metadata from URL
    """
    try:
        if not url.startswith(('http://', 'https://')):
            return {
                "error": "URL http:// v…ô ya https:// il…ô ba≈ülamalƒ±dƒ±r",
                "status_code": 400
            }
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        response = requests.get(url, headers=headers, timeout=15, allow_redirects=True)
        response.raise_for_status()
        
        final_url = response.url
        status_code = response.status_code
        
        logger.info(f"‚úÖ URL fetch edildi: {final_url} (status: {status_code})")
        
        if BeautifulSoup is None:
            return {
                "error": "BeautifulSoup qura≈üdƒ±rƒ±lmamƒ±≈üdƒ±r",
                "status_code": status_code,
                "final_url": final_url
            }
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        og_data = {
            "title": "",
            "description": "",
            "image": "",
            "site_name": "",
            "final_url": final_url,
            "status_code": status_code
        }
        
        title_tag = soup.find('meta', property='og:title') or soup.find('meta', attrs={'name': 'twitter:title'}) or soup.find('title')
        if title_tag:
            title_value = title_tag.get('content') if title_tag.get('content') else title_tag.get_text().strip()
            og_data["title"] = title_value
            logger.info(f"üìù Title: {title_value}")
        
        desc_tag = soup.find('meta', property='og:description') or soup.find('meta', attrs={'name': 'twitter:description'}) or soup.find('meta', attrs={'name': 'description'})
        if desc_tag:
            desc_value = desc_tag.get('content', '').strip()
            og_data["description"] = desc_value
            logger.info(f"üìù Description: {desc_value[:100]}...")
        
        image_tag = soup.find('meta', property='og:image') or soup.find('meta', attrs={'name': 'twitter:image'})
        if image_tag:
            image_url = image_tag.get('content', '').strip()
            if image_url:
                if image_url.startswith('//'):
                    image_url = 'https:' + image_url
                elif image_url.startswith('/'):
                    parsed = urlparse(final_url)
                    image_url = f"{parsed.scheme}://{parsed.netloc}{image_url}"
                elif not image_url.startswith('http'):
                    image_url = urljoin(final_url, image_url)
                og_data["image"] = image_url
                logger.info(f"üñºÔ∏è Image: {image_url}")
        
        site_tag = soup.find('meta', property='og:site_name')
        if site_tag:
            site_value = site_tag.get('content', '').strip()
            og_data["site_name"] = site_value
            logger.info(f"üåê Site: {site_value}")
        
        if not og_data["site_name"]:
            parsed = urlparse(final_url)
            if 'instagram.com' in parsed.netloc:
                og_data["site_name"] = "Instagram"
            elif 'facebook.com' in parsed.netloc or 'fb.com' in parsed.netloc:
                og_data["site_name"] = "Facebook"
            elif 'linkedin.com' in parsed.netloc:
                og_data["site_name"] = "LinkedIn"
            elif 'twitter.com' in parsed.netloc or 'x.com' in parsed.netloc:
                og_data["site_name"] = "Twitter/X"
        
        if not og_data["title"] and 'instagram.com' in final_url:
            username = final_url.rstrip('/').split('/')[-1]
            og_data["title"] = f"@{username}"
            og_data["description"] = "Instagram profili"
        
        return og_data
        
    except requests.exceptions.RequestException as e:
        logger.error(f"‚ùå URL request x…ôtasƒ±: {str(e)}")
        return {
            "error": f"URL-y…ô daxil olmaq m√ºmk√ºn olmadƒ±: {str(e)}",
            "status_code": 500
        }
    except Exception as e:
        logger.error(f"‚ùå OG preview extract x…ôtasƒ±: {str(e)}", exc_info=True)
        return {
            "error": f"X…ôta ba≈ü verdi: {str(e)}",
            "status_code": 500
        }


@api_view(['POST'])
@permission_classes([permissions.IsAuthenticated])
def analyze_profile_from_url(request):
    """
    Extract OG preview and analyze profile with AI
    Cache mechanism: Check cache first, if exists and fresh, return cached data
    """
    from .models import ProfileAnalysis
    
    try:
        url = request.data.get('url', '').strip()
        manual_data = request.data.get('manual_data', {})
        force_refresh = request.data.get('force_refresh', False)  # Force new analysis
        
        if not url:
            return Response({
                "error": "URL t…ôl…ôb olunur"
            }, status=status.HTTP_400_BAD_REQUEST)
        
        logger.info(f"üîç Profil analizi ba≈ülayƒ±r: {url}")
        
        # Check cache first (unless force_refresh is True)
        if not force_refresh:
            try:
                cached_analysis = ProfileAnalysis.objects.filter(profile_url=url).first()
                if cached_analysis:
                    # Cache is valid for 7 days
                    cache_age = timezone.now() - cached_analysis.updated_at
                    if cache_age < timedelta(days=7):
                        logger.info(f"‚úÖ Cache tapƒ±ldƒ±: {cached_analysis.profile_username or url} (age: {cache_age.days}d)")
                        cached_analysis.increment_access()
                        
                        return Response({
                            "preview": cached_analysis.preview_data,
                            "smm_analysis": cached_analysis.smm_analysis,
                            "cached": True,
                            "cache_age_days": cache_age.days
                        }, status=status.HTTP_200_OK)
                    else:
                        logger.info(f"‚ö†Ô∏è Cache k√∂hn…ôdir ({cache_age.days} g√ºn), yenil…ônir...")
                        # Delete old cache
                        cached_analysis.delete()
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Cache yoxlamasƒ± x…ôtasƒ±: {str(e)}")
                # Continue with fresh analysis
        
        parsed_url = urlparse(url)
        platform_domain = parsed_url.netloc.lower()
        username = parsed_url.path.rstrip('/').split('/')[-1] if parsed_url.path else ""
        
        logger.info(f"üåê Platform domain: {platform_domain}")
        
        scraped_data = None
        if 'instagram.com' in platform_domain:
            logger.info(f"üì∏ Instagram detect edildi, Apify scraping ba≈ülayƒ±r...")
            scraped_data = scrape_instagram_with_apify(url)
            if scraped_data:
                logger.info(f"‚úÖ Instagram Apify scraping uƒüurlu: @{scraped_data.get('username')}")
            else:
                logger.warning(f"‚ö†Ô∏è Instagram Apify scraping uƒüursuz v…ô ya deaktiv")
        elif 'linkedin.com' in platform_domain:
            logger.info(f"üíº LinkedIn detect edildi, Apify scraping ba≈ülayƒ±r...")
            # Check if it's a company page or personal profile
            if '/company/' in url.lower():
                logger.info(f"üè¢ LinkedIn ≈üirk…ôt s…ôhif…ôsi detect edildi")
                scraped_data = scrape_linkedin_company_with_apify(url)
                # ≈ûirk…ôt s…ôhif…ôl…ôri √º√ß√ºn personal profil scraping-i √ßaƒüƒ±rmƒ±rƒ±q
                if scraped_data:
                    logger.info(f"‚úÖ LinkedIn ≈üirk…ôt s…ôhif…ôsi Apify scraping uƒüurlu: {scraped_data.get('name')}")
                else:
                    logger.warning(f"‚ö†Ô∏è LinkedIn ≈üirk…ôt s…ôhif…ôsi Apify scraping uƒüursuz, OG preview istifad…ô edil…ôc…ôk")
            else:
                logger.info(f"üë§ LinkedIn personal profil detect edildi")
            scraped_data = scrape_linkedin_with_apify(url)
            if scraped_data:
                    logger.info(f"‚úÖ LinkedIn Apify scraping uƒüurlu: {scraped_data.get('full_name') or scraped_data.get('name')}")
            else:
                logger.warning(f"‚ö†Ô∏è LinkedIn Apify scraping uƒüursuz v…ô ya deaktiv")
        elif 'facebook.com' in platform_domain or 'fb.com' in platform_domain:
            logger.info(f"üìò Facebook detect edildi, Apify scraping ba≈ülayƒ±r...")
            scraped_data = scrape_facebook_with_apify(url)
            if scraped_data:
                logger.info(f"‚úÖ Facebook Apify scraping uƒüurlu: {scraped_data.get('name')}")
            else:
                logger.warning(f"‚ö†Ô∏è Facebook Apify scraping uƒüursuz v…ô ya deaktiv")
        
        if scraped_data:
            # Determine platform from scraped data structure
            if 'username' in scraped_data and 'biography' in scraped_data:
                # Instagram
                logger.info(f"‚úÖ Instagram Apify scraping uƒüurlu")
                platform = "Instagram"
                username = scraped_data.get('username', username)
                name = scraped_data.get('full_name', '')
                bio = scraped_data.get('biography', '')
                followers = str(scraped_data.get('followers', ''))
                following = str(scraped_data.get('following', ''))
                posts = str(scraped_data.get('posts', ''))
                category = scraped_data.get('category', '')
                profile_image = scraped_data.get('profile_pic_url', '')
                
                title = f"@{username}"
                description = bio
            elif 'headline' in scraped_data or 'public_identifier' in scraped_data or 'company_type' in scraped_data:
                # LinkedIn (personal profile or company page)
                logger.info(f"‚úÖ LinkedIn Apify scraping uƒüurlu")
                platform = "LinkedIn"
                
                # Check if it's a company page
                is_company = 'company_type' in scraped_data or 'industry' in scraped_data or '/company/' in url.lower()
                
                if is_company:
                    logger.info(f"üè¢ LinkedIn ≈üirk…ôt s…ôhif…ôsi parse edilir")
                    username = scraped_data.get('public_identifier', '') or username or scraped_data.get('profile_url', '').split('/')[-1] if scraped_data.get('profile_url') else ''
                    name = scraped_data.get('name', '') or scraped_data.get('full_name', '')
                    bio = scraped_data.get('about', '') or scraped_data.get('headline', '')
                    followers = str(scraped_data.get('followers', 0))
                    following = str(scraped_data.get('employees', 0))  # Company pages use employees instead of connections
                    posts = str(scraped_data.get('posts', 0))  # Posts count from scraped data
                    connections = str(scraped_data.get('connections', 0))  # Connections count
                    category = scraped_data.get('industry', '') or scraped_data.get('location', '')
                    profile_image = scraped_data.get('profile_pic_url', '')
                    
                    logger.info(f"üìä LinkedIn ≈üirk…ôt s…ôhif…ôsi parse edildi: name={name}, followers={followers}, employees={following}, posts={posts}, connections={connections}, bio length={len(bio)}")
                else:
                    logger.info(f"üë§ LinkedIn personal profil parse edilir")
                username = scraped_data.get('public_identifier', '') or username or scraped_data.get('profile_url', '').split('/')[-1] if scraped_data.get('profile_url') else ''
                name = scraped_data.get('full_name', '')
                bio = scraped_data.get('about', '') or scraped_data.get('headline', '')
                followers = str(scraped_data.get('followers', 0))
                following = str(scraped_data.get('connections', 0))  # LinkedIn uses connections
                posts = '0'  # LinkedIn doesn't provide posts count in profile
                category = scraped_data.get('location', '')
                profile_image = scraped_data.get('profile_pic_url', '')
                
                logger.info(f"üìä LinkedIn parse edildi: name={name}, followers={followers}, connections={following}, bio length={len(bio)}")
                
                title = name or username
                description = bio
            elif 'likes' in scraped_data or 'pageUrl' in scraped_data:
                # Facebook
                logger.info(f"‚úÖ Facebook Apify scraping uƒüurlu")
                platform = "Facebook"
                username = scraped_data.get('username', username)
                name = scraped_data.get('full_name', '') or scraped_data.get('name', '')
                bio = scraped_data.get('about', '')
                followers = str(scraped_data.get('followers', 0))
                following = str(scraped_data.get('likes', 0))  # Facebook uses "likes" instead of following
                posts = str(scraped_data.get('posts', 0))  # Posts count (number, not array)
                category = ', '.join(scraped_data.get('category', [])) if isinstance(scraped_data.get('category'), list) else scraped_data.get('category', '')
                profile_image = scraped_data.get('profile_pic_url', '')
                
                logger.info(f"üìä Facebook parse edildi: name={name}, followers={followers}, likes={following}, bio length={len(bio)}")
                
                title = name or f"@{username}"
                description = bio
        elif manual_data and manual_data.get('username'):
            # Manual input - platform detect from URL
            logger.info(f"üìù Manual m…ôlumatlar istifad…ô edilir")
            
            # Detect platform from URL
            if 'facebook.com' in platform_domain or 'fb.com' in platform_domain:
                platform = "Facebook"
            elif 'linkedin.com' in platform_domain:
                platform = "LinkedIn"
            else:
                platform = "Instagram"
            
            username = manual_data.get('username', username)
            name = manual_data.get('name', '')
            bio = manual_data.get('bio', '')
            followers = manual_data.get('followers', '')
            following = manual_data.get('following', '')
            posts = manual_data.get('posts', '')
            category = manual_data.get('category', '')
            
            title = f"@{username}" if username else name
            description = bio if bio else ""
            profile_image = ""
            
            logger.info(f"üìä Manual m…ôlumatlar ({platform}): posts={posts}, followers={followers}, category={category}")
            logger.info(f"üìù Platform detected: {platform}, username: {username}, name: {name}")
        else:
            og_preview = extract_og_preview(url)
            
            if "error" in og_preview:
                return Response({
                    "error": og_preview["error"],
                    "preview": og_preview
                }, status=status.HTTP_400_BAD_REQUEST)
            
            logger.info(f"‚úÖ OG preview alƒ±ndƒ±: {og_preview.get('title', 'N/A')}")
            
            platform = og_preview.get('site_name', 'Unknown')
            title = og_preview.get('title', '')
            description = og_preview.get('description', '')
            profile_image = og_preview.get('image', '')
            name = ''
            bio = ''
            followers = ''
            following = ''
            posts = ''
            category = ''
            
            if not title and username:
                title = f"@{username}"
            if not description and username:
                description = f"{platform} profili"
        
        client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
        
        profile_info = f"""PROFIL M∆èLUMATLARI:
- Platforma: {platform}
- Username: @{username}"""

        if name:
            profile_info += f"\n- Ad/Name: {name}"
        if posts:
            profile_info += f"\n- Posts: {posts}"
        if followers:
            profile_info += f"\n- Followers: {followers}"
        if following:
            profile_info += f"\n- Following: {following}"
        if category:
            profile_info += f"\n- Kateqoriya: {category}"
        if bio:
            profile_info += f"\n- Bio: {bio}"
        elif description:
            profile_info += f"\n- Description: {description}"
        
        profile_info += f"\n- URL: {url}"
        
        analysis_prompt = f"""Sosial media profil analizi (Az…ôrbaycan dilind…ô).

{profile_info}

Bu m…ôlumatlara …ôsas…ôn profil analizi edin v…ô JSON formatda qaytarƒ±n:
{{
    "account_type": "Personal/Business/Influencer/Brand",
    "niche": "Fashion/Tech/Food/...",
    "content_style": "Professional/Casual/Creative/..."
}}

B√ºt√ºn m…ôtnl…ôr Az…ôrbaycan dilind…ô olmalƒ±dƒ±r."""

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "Siz pe≈ü…ôkar Social Media Marketing (SMM) m…ôsl…ôh…ôt√ßisisiniz. Profil preview m…ôlumatlarƒ±na …ôsas…ôn analiz edib konkret t√∂vsiy…ôl…ôr verirsiniz. B√ºt√ºn cavablar Az…ôrbaycan dilind…ô olmalƒ±dƒ±r. YALNIZ JSON formatda cavab verin, he√ß bir …ôlav…ô m…ôtn yazmayƒ±n."
                },
                {
                    "role": "user",
                    "content": analysis_prompt
                }
            ],
            temperature=0.7,
            max_tokens=2000,
            response_format={"type": "json_object"}
        )
        
        analysis_text = response.choices[0].message.content
        
        if not analysis_text:
            logger.error("‚ùå OpenAI cavabƒ± bo≈üdur")
            return Response({
                "error": "AI cavab verm…ôdi",
                "preview": {
                    "title": og_preview.get("title", ""),
                    "description": og_preview.get("description", ""),
                    "image": og_preview.get("image", ""),
                    "site_name": og_preview.get("site_name", ""),
                    "final_url": og_preview.get("final_url", url)
                }
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        
        analysis_text = analysis_text.strip()
        logger.info(f"üìù AI cavabƒ± (ilk 200 simvol): {analysis_text[:200]}")
        
        if analysis_text.startswith('```'):
            analysis_text = re.sub(r'^```json?\s*', '', analysis_text)
            analysis_text = re.sub(r'\s*```$', '', analysis_text)
        
        smm_analysis = json.loads(analysis_text)
        
        logger.info(f"‚úÖ Analiz tamamlandƒ±")
        
        preview_data = {
            "title": title,
            "description": description,
            "image": profile_image,
            "site_name": platform,
            "final_url": url,
            "stats": {
                "followers": str(followers) if followers else "0",
                "following": str(following) if following else "0",
                "posts": str(posts) if posts else "0"
            }
        }
        
        # Apify-d…ôn g…ôl…ôn …ôlav…ô m…ôlumatlar
        if scraped_data:
            # Profil ≈ü…ôklini prioritet et (HD versiyasƒ± varsa)
            profile_pic = scraped_data.get("profile_pic_url", "")
            if profile_pic:
                preview_data["image"] = profile_pic
                logger.info(f"üñºÔ∏è Profil ≈ü…ôkli …ôlav…ô edildi: {profile_pic[:80]}...")
            
            # Latest posts parse et
            latest_posts = scraped_data.get("latest_posts", [])
            parsed_posts = []
            for post in latest_posts:
                parsed_posts.append({
                    "id": post.get("id", ""),
                    "type": post.get("type", ""),
                    "url": post.get("url", ""),
                    "caption": post.get("caption", ""),
                    "likes_count": post.get("likesCount", 0),
                    "comments_count": post.get("commentsCount", 0),
                    "timestamp": post.get("timestamp", ""),
                    "hashtags": post.get("hashtags", []),
                    "mentions": post.get("mentions", []),
                    "display_url": post.get("displayUrl", ""),
                    "video_url": post.get("videoUrl", ""),
                    "short_code": post.get("shortCode", "")
                })
            
            # Platforma g√∂r…ô f…ôrqli m…ôlumatlar
            if 'username' in scraped_data and 'biography' in scraped_data:
                # Instagram
                preview_data.update({
                    "username": scraped_data.get("username", ""),
                    "full_name": scraped_data.get("full_name", ""),
                    "biography": scraped_data.get("biography", ""),
                    "is_verified": scraped_data.get("is_verified", False),
                    "is_business": scraped_data.get("is_business", False),
                    "category": scraped_data.get("category", ""),
                    "is_private": scraped_data.get("is_private", False),
                    "highlight_reel_count": scraped_data.get("highlight_reel_count", 0),
                    "igtv_video_count": scraped_data.get("igtv_video_count", 0),
                    "latest_posts": parsed_posts
                })
                preview_data["stats"] = {
                    "followers": str(scraped_data.get("followers", 0)),
                    "following": str(scraped_data.get("following", 0)),
                    "posts": str(scraped_data.get("posts", 0))
                }
                
                logger.info(f"üì∏ {len(parsed_posts)} payla≈üƒ±m parse edildi")
            elif 'headline' in scraped_data or 'public_identifier' in scraped_data or 'company_type' in scraped_data:
                # LinkedIn (personal profile or company page)
                is_company = 'company_type' in scraped_data or 'industry' in scraped_data or '/company/' in url.lower()
                
                if is_company:
                    # LinkedIn Company Page
                    preview_data.update({
                        "full_name": scraped_data.get("name", "") or scraped_data.get("full_name", ""),
                        "biography": scraped_data.get("about", "") or scraped_data.get("headline", ""),
                        "headline": scraped_data.get("headline", ""),
                        "location": scraped_data.get("location", ""),
                        "company_type": scraped_data.get("company_type", ""),
                        "industry": scraped_data.get("industry", ""),
                        "website": scraped_data.get("website", ""),
                        "public_identifier": scraped_data.get("public_identifier", ""),
                        "verified": scraped_data.get("verified", False)
                    })
                    preview_data["stats"] = {
                        "followers": str(scraped_data.get("followers", 0)),
                        "employees": str(scraped_data.get("employees", 0)),
                        "posts": str(scraped_data.get("posts", 0)),  # Posts count from scraped data
                        "connections": str(scraped_data.get("connections", 0))  # Connections count
                    }
                    
                    logger.info(f"üè¢ LinkedIn ≈üirk…ôt s…ôhif…ôsi m…ôlumatlarƒ± …ôlav…ô edildi: name={preview_data.get('full_name')[:50] if preview_data.get('full_name') else 'N/A'}, industry={preview_data.get('industry')}, location={preview_data.get('location')}")
                    logger.info(f"üè¢ LinkedIn ≈üirk…ôt stats: {scraped_data.get('followers')} followers, {scraped_data.get('employees')} employees, {scraped_data.get('posts', 0)} posts, {scraped_data.get('connections', 0)} connections")
                else:
                    # LinkedIn Personal Profile
                    preview_data.update({
                        "full_name": scraped_data.get("full_name", ""),
                        "first_name": scraped_data.get("first_name", ""),
                        "last_name": scraped_data.get("last_name", ""),
                        "biography": scraped_data.get("about", "") or scraped_data.get("headline", ""),
                        "headline": scraped_data.get("headline", ""),
                        "location": scraped_data.get("location", ""),
                        "experience": scraped_data.get("experience", []),
                        "education": scraped_data.get("education", []),
                        "skills": scraped_data.get("skills", []),
                        "public_identifier": scraped_data.get("public_identifier", ""),
                        "verified": scraped_data.get("verified", False),
                        "open_to_work": scraped_data.get("open_to_work", False),
                        "premium": scraped_data.get("premium", False)
                    })
                preview_data["stats"] = {
                    "followers": str(scraped_data.get("followers", 0)),
                    "connections": str(scraped_data.get("connections", 0)),
                    "posts": "0"  # LinkedIn doesn't provide posts count
                }
                
                logger.info(f"üíº LinkedIn m…ôlumatlarƒ± …ôlav…ô edildi: headline={preview_data.get('headline')[:50] if preview_data.get('headline') else 'N/A'}, location={preview_data.get('location')}, experience={len(preview_data.get('experience', []))}, education={len(preview_data.get('education', []))}, skills={len(preview_data.get('skills', []))}")
                logger.info(f"üíº LinkedIn stats: {scraped_data.get('followers')} followers, {scraped_data.get('connections')} connections")
            elif 'likes' in scraped_data or 'pageUrl' in scraped_data:
                # Facebook
                # Parse Facebook posts if available (posts_data is the array, posts is the count)
                facebook_posts = scraped_data.get("posts_data", [])
                parsed_fb_posts = []
                seen_ids = set()  # Track seen IDs to ensure uniqueness
                
                for idx, post in enumerate(facebook_posts):
                    # Extract text/content - try multiple possible field names
                    post_text = (post.get("text", "") or 
                                post.get("content", "") or 
                                post.get("message", "") or
                                post.get("postText", "") or
                                post.get("description", ""))
                    
                    # Extract image - try multiple possible field names
                    post_image = (post.get("image", "") or 
                                 post.get("imageUrl", "") or
                                 post.get("photo", "") or
                                 post.get("photoUrl", "") or
                                 post.get("images", [""])[0] if isinstance(post.get("images"), list) and len(post.get("images", [])) > 0 else "")
                    
                    # Extract video - try multiple possible field names
                    post_video = (post.get("video", "") or 
                                 post.get("videoUrl", "") or
                                 post.get("videoUrlHd", "") or
                                 post.get("videos", [""])[0] if isinstance(post.get("videos"), list) and len(post.get("videos", [])) > 0 else "")
                    
                    # Extract from media field (Facebook Posts Scraper uses this)
                    media = post.get("media")
                    if media:
                        if isinstance(media, list) and len(media) > 0:
                            # media is an array of media objects
                            for media_item in media:
                                if isinstance(media_item, dict):
                                    # Check media type
                                    media_type = media_item.get("__typename", "") or media_item.get("__isMedia", "")
                                    
                                    # Extract image from photo_image.uri or thumbnail
                                    if not post_image:
                                        photo_image = media_item.get("photo_image", {})
                                        if isinstance(photo_image, dict):
                                            post_image = photo_image.get("uri", "")
                                        
                                        # Fallback to thumbnail
                                        if not post_image:
                                            post_image = media_item.get("thumbnail", "")
                                        
                                        # Fallback to other fields
                                        if not post_image:
                                            post_image = (media_item.get("url", "") or 
                                                         media_item.get("image", "") or
                                                         media_item.get("photo", "") or
                                                         media_item.get("src", ""))
                                    
                                    # Check for video (if media type is Video)
                                    if not post_video and ("Video" in media_type or "video" in str(media_type).lower()):
                                        post_video = (media_item.get("videoUrl", "") or 
                                                     media_item.get("video", "") or
                                                     media_item.get("videoUrlHd", "") or
                                                     media_item.get("source", ""))
                        elif isinstance(media, dict):
                            # media is a single object
                            media_type = media.get("__typename", "") or media.get("__isMedia", "")
                            
                            # Extract image from photo_image.uri or thumbnail
                            if not post_image:
                                photo_image = media.get("photo_image", {})
                                if isinstance(photo_image, dict):
                                    post_image = photo_image.get("uri", "")
                                
                                # Fallback to thumbnail
                                if not post_image:
                                    post_image = media.get("thumbnail", "")
                                
                                # Fallback to other fields
                                if not post_image:
                                    post_image = (media.get("url", "") or 
                                                 media.get("image", "") or
                                                 media.get("photo", "") or
                                                 media.get("src", ""))
                            
                            # Check for video
                            if not post_video and ("Video" in media_type or "video" in str(media_type).lower()):
                                post_video = (media.get("videoUrl", "") or 
                                             media.get("video", "") or
                                             media.get("videoUrlHd", "") or
                                             media.get("source", ""))
                    
                    # Extract URL - try multiple possible field names
                    post_url = (post.get("postUrl", "") or 
                               post.get("url", "") or
                               post.get("link", "") or
                               post.get("permalink", ""))
                    
                    # Extract timestamp - try multiple possible field names
                    post_timestamp = (post.get("time", "") or 
                                     post.get("timestamp", "") or
                                     post.get("createdTime", "") or
                                     post.get("date", "") or
                                     post.get("publishedAt", ""))
                    
                    # Format timestamp for display (if it's ISO format)
                    formatted_date = ""
                    if post_timestamp:
                        try:
                            from datetime import datetime
                            # Try to parse ISO format timestamp
                            if isinstance(post_timestamp, str) and "T" in post_timestamp:
                                dt = datetime.fromisoformat(post_timestamp.replace("Z", "+00:00"))
                                formatted_date = dt.strftime("%Y M%m %d")  # Format: "2025 M12 24"
                            elif isinstance(post_timestamp, (int, float)):
                                # Unix timestamp
                                dt = datetime.fromtimestamp(post_timestamp)
                                formatted_date = dt.strftime("%Y M%m %d")
                        except:
                            formatted_date = str(post_timestamp)
                    
                    # Generate unique ID - use postId/id if available, otherwise create unique ID
                    post_id = (post.get("postId", "") or 
                              post.get("id", "") or 
                              post.get("_id", ""))
                    
                    # Convert to string if it's a number
                    if post_id:
                        post_id = str(post_id)
                    
                    # If ID is empty or already seen, create unique ID
                    if not post_id or post_id in seen_ids:
                        # Create unique ID from URL, timestamp, or index
                        if post_url:
                            # Extract ID from URL if possible
                            import re
                            url_id_match = re.search(r'/(\d+)/', post_url)
                            if url_id_match:
                                post_id = f"{url_id_match.group(1)}_{idx}"
                            else:
                                # Use hash of URL + index
                                post_id = f"post_{hash(post_url) % 1000000}_{idx}"
                        elif post_timestamp:
                            # Use timestamp + index
                            post_id = f"post_{post_timestamp}_{idx}"
                        else:
                            # Use index as last resort
                            post_id = f"post_{idx}"
                    
                    # Ensure ID is unique by appending index if still duplicate
                    original_id = post_id
                    counter = 0
                    while post_id in seen_ids:
                        counter += 1
                        post_id = f"{original_id}_{counter}"
                    
                    seen_ids.add(post_id)
                    
                    # Extract all available data for frontend
                    parsed_post = {
                        "id": post_id,  # Always unique
                        "type": post.get("type", "") or post.get("postType", ""),
                        "url": post_url,
                        "text": post_text,
                        "caption": post_text,  # Frontend uses caption field
                        "likes_count": post.get("likes", 0) or post.get("likeCount", 0) or post.get("reactions", 0),
                        "comments_count": post.get("comments", 0) or post.get("commentCount", 0),
                        "shares_count": post.get("shares", 0) or post.get("shareCount", 0),
                        "timestamp": post_timestamp,
                        "date": formatted_date,  # Formatted date for display
                        "image": post_image,
                        "video": post_video,
                        "display_url": post_image,  # Frontend uses display_url for image display
                        # Additional fields from original post
                        "postId": post.get("postId", "") or post.get("id", ""),
                        "time": post.get("time", ""),
                        "facebookUrl": post.get("facebookUrl", ""),
                        "pageName": post.get("pageName", ""),
                        # Media information
                        "media": []
                    }
                    
                    # Extract all media items with full details
                    if media:
                        if isinstance(media, list):
                            for media_item in media:
                                if isinstance(media_item, dict):
                                    media_info = {
                                        "type": media_item.get("__typename", "") or media_item.get("__isMedia", ""),
                                        "id": media_item.get("id", ""),
                                        "url": media_item.get("url", ""),
                                        "thumbnail": media_item.get("thumbnail", ""),
                                    }
                                    # Extract photo_image details
                                    photo_image = media_item.get("photo_image", {})
                                    if isinstance(photo_image, dict):
                                        media_info["photo_image"] = {
                                            "uri": photo_image.get("uri", ""),
                                            "height": photo_image.get("height", 0),
                                            "width": photo_image.get("width", 0)
                                        }
                                    parsed_post["media"].append(media_info)
                        elif isinstance(media, dict):
                            media_info = {
                                "type": media.get("__typename", "") or media.get("__isMedia", ""),
                                "id": media.get("id", ""),
                                "url": media.get("url", ""),
                                "thumbnail": media.get("thumbnail", ""),
                            }
                            photo_image = media.get("photo_image", {})
                            if isinstance(photo_image, dict):
                                media_info["photo_image"] = {
                                    "uri": photo_image.get("uri", ""),
                                    "height": photo_image.get("height", 0),
                                    "width": photo_image.get("width", 0)
                                }
                            parsed_post["media"].append(media_info)
                    
                    parsed_fb_posts.append(parsed_post)
                
                # Get posts count (use posts field if available, otherwise count from array)
                posts_count = scraped_data.get("posts", 0) or len(parsed_fb_posts)
                
                preview_data.update({
                    "username": scraped_data.get("username", ""),
                    "full_name": scraped_data.get("name", "") or scraped_data.get("full_name", ""),
                    "biography": scraped_data.get("about", ""),
                    "category": ', '.join(scraped_data.get("category", [])) if isinstance(scraped_data.get("category"), list) else scraped_data.get("category", ""),
                    "address": scraped_data.get("address", ""),
                    "phone": scraped_data.get("phone", ""),
                    "email": scraped_data.get("email", ""),
                    "website": scraped_data.get("website", ""),
                    "verified": scraped_data.get("verified", False),
                    "profile_pic_url": scraped_data.get("profile_pic_url", ""),
                    "cover_photo": scraped_data.get("cover_photo", ""),
                    "latest_posts": parsed_fb_posts
                })
                preview_data["stats"] = {
                    "followers": str(scraped_data.get("followers", 0)),
                    "likes": str(scraped_data.get("likes", 0)),
                    "posts": str(posts_count)
                }
                
                logger.info(f"üìò Facebook m…ôlumatlarƒ± …ôlav…ô edildi: {len(parsed_fb_posts)} payla≈üƒ±m parse edildi")
                logger.info(f"üìò Orijinal post sayƒ±: {len(facebook_posts)}, Parse edilmi≈ü post sayƒ±: {len(parsed_fb_posts)}")
                
                # Log all post IDs to check for duplicates
                post_ids_list = [p.get('id', 'NO_ID') for p in parsed_fb_posts]
                unique_ids = set(post_ids_list)
                if len(post_ids_list) != len(unique_ids):
                    logger.warning(f"‚ö†Ô∏è Duplicate post IDs tapƒ±ldƒ±! Total: {len(post_ids_list)}, Unique: {len(unique_ids)}")
                    from collections import Counter
                    id_counts = Counter(post_ids_list)
                    duplicates = {k: v for k, v in id_counts.items() if v > 1}
                    logger.warning(f"‚ö†Ô∏è Duplicate IDs: {duplicates}")
                
                if parsed_fb_posts and len(parsed_fb_posts) > 0:
                    logger.info(f"üìò ƒ∞lk 3 post ID-l…ôri: {[p.get('id', 'NO_ID') for p in parsed_fb_posts[:3]]}")
                    first_post = parsed_fb_posts[0]
                    logger.info(f"üìò ƒ∞lk post n√ºmun…ôsi:")
                    logger.info(f"   ID: {first_post.get('id')}")
                    logger.info(f"   PostId: {first_post.get('postId')}")
                    logger.info(f"   URL: {first_post.get('url', '')[:80]}...")
                    logger.info(f"   Text length: {len(first_post.get('text', ''))}")
                    logger.info(f"   Text preview: {first_post.get('text', '')[:100]}...")
                    logger.info(f"   Image URL: {first_post.get('image', '')[:100] if first_post.get('image') else 'YOX'}...")
                    logger.info(f"   Display URL: {first_post.get('display_url', '')[:100] if first_post.get('display_url') else 'YOX'}...")
                    logger.info(f"   Date: {first_post.get('date', 'N/A')}")
                    logger.info(f"   Timestamp: {first_post.get('timestamp', 'N/A')}")
                    logger.info(f"   Likes: {first_post.get('likes_count', 0)}")
                    logger.info(f"   Comments: {first_post.get('comments_count', 0)}")
                    logger.info(f"   Shares: {first_post.get('shares_count', 0)}")
                    logger.info(f"   Media count: {len(first_post.get('media', []))}")
                logger.info(f"üìò Facebook stats: {scraped_data.get('followers')} followers, {scraped_data.get('likes')} likes, {posts_count} posts")
        
        # Save to cache
        try:
            with transaction.atomic():
                # Get or create cache entry
                cached_analysis, created = ProfileAnalysis.objects.update_or_create(
                    profile_url=url,
                    defaults={
                        'user': request.user if request.user.is_authenticated else None,
                        'profile_username': preview_data.get('username', ''),
                        'platform': preview_data.get('site_name', 'Unknown'),
                        'preview_data': preview_data,
                        'smm_analysis': smm_analysis,
                        'updated_at': timezone.now()
                    }
                )
                if created:
                    logger.info(f"üíæ Cache yaradƒ±ldƒ±: {url}")
                else:
                    logger.info(f"üíæ Cache yenil…ôndi: {url}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Cache yazma x…ôtasƒ±: {str(e)}")
            # Continue even if cache fails
        
        return Response({
            "preview": preview_data,
            "smm_analysis": smm_analysis,
            "cached": False
        }, status=status.HTTP_200_OK)
        
    except json.JSONDecodeError as e:
        logger.error(f"‚ùå JSON parse x…ôtasƒ±: {str(e)}")
        return Response({
            "error": "AI cavabƒ± parse edil…ô bilm…ôdi",
            "preview": og_preview if 'og_preview' in locals() else {}
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    except Exception as e:
        logger.error(f"‚ùå Profil analiz x…ôtasƒ±: {str(e)}", exc_info=True)
        return Response({
            "error": f"Analiz zamanƒ± x…ôta ba≈ü verdi: {str(e)}"
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


@api_view(['GET'])
@permission_classes([permissions.IsAuthenticated])
def get_saved_profiles(request):
    """
    Get all saved profile analyses for a specific platform
    Query param: ?platform=instagram|facebook|linkedin
    Returns cached profiles ordered by last_accessed
    """
    from .models import ProfileAnalysis
    
    try:
        platform_filter = request.GET.get('platform', 'instagram').lower()
        
        # Map platform names
        platform_map = {
            'instagram': 'instagram',
            'facebook': 'facebook',
            'linkedin': 'linkedin'
        }
        
        platform = platform_map.get(platform_filter, 'instagram')
        
        logger.info(f"üìã Saved profiles sorƒüusu: platform={platform}")
        
        # Get saved profiles for the platform
        profiles = ProfileAnalysis.objects.filter(
            platform__icontains=platform
        ).order_by('-last_accessed')[:50]  # Limit to 50 most recent
        
        profiles_data = []
        for profile in profiles:
            preview = profile.preview_data or {}
            stats = preview.get('stats', {})
            
            logger.info(f"üìã Profile: {profile.profile_username}, stats: {stats}")
            
            profiles_data.append({
                'id': str(profile.id),
                'profile_url': profile.profile_url,
                'username': profile.profile_username or preview.get('username', ''),
                'full_name': preview.get('full_name', ''),
                'image': preview.get('image', ''),
                'platform': profile.platform,
                'stats': stats,
                'preview_data': preview,  # Include full preview_data for debugging
                'smm_analysis': profile.smm_analysis or {},
                'last_accessed': profile.last_accessed.isoformat() if profile.last_accessed else None,
                'access_count': profile.access_count,
                'created_at': profile.created_at.isoformat() if profile.created_at else None
            })
        
        logger.info(f"üìã {len(profiles_data)} saved profile qaytarƒ±ldƒ±")
        
        return Response({
            'profiles': profiles_data,
            'count': len(profiles_data)
        }, status=status.HTTP_200_OK)
        
    except Exception as e:
        logger.error(f"‚ùå Saved profiles x…ôtasƒ±: {str(e)}", exc_info=True)
        return Response({
            "error": f"Saved profiles alƒ±nark…ôn x…ôta: {str(e)}"
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

